<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title></title>
                <meta name="robots" content="noindex" />
                

        <!-- Custom HTML head -->
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

                <link rel="icon" href="favicon.svg">
                        <link rel="shortcut icon" href="favicon.png">
                <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
                <link rel="stylesheet" href="css/print.css" media="print">
        
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
                <link rel="stylesheet" href="fonts/fonts.css">
        
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
            </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="1.1-front-page.html"><strong aria-hidden="true">1.</strong> Welcome</a></li><li class="chapter-item expanded "><a href="about.html"><strong aria-hidden="true">2.</strong> About LumoSQL</a></li><li class="chapter-item expanded "><a href="1.2-top-features.html"><strong aria-hidden="true">3.</strong> Top Features</a></li><li class="chapter-item expanded "><a href="1.4-install-LumoSQL.html"><strong aria-hidden="true">4.</strong> Install</a></li><li class="chapter-item expanded "><a href="quickstart.html"><strong aria-hidden="true">5.</strong> Quick Build and Benchmark</a></li><li class="chapter-item expanded affix "><li class="part-title">Features</li><li class="chapter-item expanded "><a href="3.4-not-forking-tool.html"><strong aria-hidden="true">6.</strong> Not-Forking Tool</a></li><li class="chapter-item expanded "><a href="lumo-build-benchmark.html"><strong aria-hidden="true">7.</strong> Build and Benchmark System (build.tcl)</a></li><li class="chapter-item expanded "><a href="backends.html"><strong aria-hidden="true">8.</strong> Available Backends - SQLite B-tree and LMDB</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="lumo-sqlite-bdb-backend.html"><strong aria-hidden="true">8.1.</strong> BDB 18.1.32 Backend</a></li><li class="chapter-item expanded "><a href="lumo-malbrain-backend.html"><strong aria-hidden="true">8.2.</strong> Karl Malbrain's C Btree</a></li></ol></li><li class="chapter-item expanded "><a href="lumo-corruption-detection-and-magic.html"><strong aria-hidden="true">9.</strong> Corruption Detection</a></li><li class="chapter-item expanded "><a href="3.3-benchmarking.html"><strong aria-hidden="true">10.</strong> Benchmarking</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="lumo-benchmark-filter.html"><strong aria-hidden="true">10.1.</strong> Displaying Benchmark Results (benchmark-filter.tcl)</a></li><li class="chapter-item expanded "><a href="statistical_analysis.html"><strong aria-hidden="true">10.2.</strong> Statistical Analysis</a></li></ol></li><li class="chapter-item expanded "><li class="part-title">New features</li><li class="chapter-item expanded "><a href="rbac-design.html"><strong aria-hidden="true">11.</strong> Design of Role-Based Access Control</a></li><li class="chapter-item expanded "><a href="encryption.html"><strong aria-hidden="true">12.</strong> Encryption</a></li><li class="chapter-item expanded "><a href="lumion_intro.html"><strong aria-hidden="true">13.</strong> Lumion</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="rfc.html"><strong aria-hidden="true">13.1.</strong> Lumion RFC</a></li></ol></li><li class="chapter-item expanded "><li class="part-title">Research</li><li class="chapter-item expanded "><a href="2.1-development-landscape.html"><strong aria-hidden="true">14.</strong> SQLite Development Landscape</a></li><li class="chapter-item expanded "><a href="3.7-relevant-codebases.html"><strong aria-hidden="true">15.</strong> Overview of Relevant Codebases</a></li><li class="chapter-item expanded "><a href="2.4-relevant-knowledgebase.html"><strong aria-hidden="true">16.</strong> Overview of the Relevant Knowledgebase</a></li><li class="chapter-item expanded "><a href="3.6-development-notes.html"><strong aria-hidden="true">17.</strong> LumoSQL 2019 Prototype</a></li><li class="chapter-item expanded affix "><li class="part-title">Design</li><li class="chapter-item expanded "><a href="api.html"><strong aria-hidden="true">18.</strong> SQLite API Interception Points</a></li><li class="chapter-item expanded "><a href="virtual-machine.html"><strong aria-hidden="true">19.</strong> SQLite Virtual Machine Layer</a></li><li class="chapter-item expanded "><a href="WALs.html"><strong aria-hidden="true">20.</strong> LMDB Alternative to WALs</a></li><li class="chapter-item expanded "><a href="what-are-savepoints.html"><strong aria-hidden="true">21.</strong> Savepoints in SQLite</a></li><li class="chapter-item expanded "><a href="online-database-servers.html"><strong aria-hidden="true">22.</strong> Online Database Servers</a></li><li class="chapter-item expanded affix "><li class="part-title">Other</li><li class="chapter-item expanded "><a href="LumoSQL-PhaseII-Announce.html"><strong aria-hidden="true">23.</strong> News - Phase II Announcement</a></li><li class="chapter-item expanded "><a href="release-announce-0.4.html"><strong aria-hidden="true">24.</strong> News - Release 0.4 Announcement</a></li><li class="chapter-item expanded "><a href="release-announce-0.3.html"><strong aria-hidden="true">25.</strong> News - Release 0.3 Announcement</a></li><li class="chapter-item expanded "><a href="lumo-gsoc-ideas.html"><strong aria-hidden="true">26.</strong> Google Summer of Code 2021</a></li><li class="chapter-item expanded "><a href="lumo-proposed-debug.html"><strong aria-hidden="true">27.</strong> SQLite Debug Proposal</a></li><li class="chapter-item expanded "><a href="CONTRIBUTING.html"><strong aria-hidden="true">28.</strong> Conributing</a></li><li class="chapter-item expanded "><a href="lumosql-meetbot.html"><strong aria-hidden="true">29.</strong> Contributing - IRC Meetbot</a></li><li class="chapter-item expanded "><a href="CODE-OF-CONDUCT.html"><strong aria-hidden="true">30.</strong> Code of Conduct</a></li><li class="chapter-item expanded "><a href="3.2-legal-aspects.html"><strong aria-hidden="true">31.</strong> Legal Aspects</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="licensing.html"><strong aria-hidden="true">31.1.</strong> Licensing</a></li><li class="chapter-item expanded "><a href="MIT.html"><strong aria-hidden="true">31.2.</strong> MIT Licence</a></li></ol></li></ol>            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                                                <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                                            </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                                                <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                                                                        
                    </div>
                </div>

                                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <!-- SPDX-License-Identifier: AGPL-3.0-only -->
<!-- SPDX-FileCopyrightText: 2020 The LumoSQL Authors, 2019 Oracle -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Dan Shearer, 2020 -->
<h1 id="welcome-to-lumosql"><a class="header" href="#welcome-to-lumosql">Welcome to LumoSQL</a></h1>
<p><img src="./images/lumo-logo-temp.svg" alt="" title="LumoSQL logo" /></p>
<p>LumoSQL is a project under active developement. Our goal is to build a reliable and secure database management system that is fully open-source and improves on the performance on classic SQLite. </p>
<ul>
<li>
<p>100% downstream and upstream compatibility with <a href="https://sqlite.org">SQLite</a>, with same command line interface.</p>
</li>
<li>
<p>Modular <a href="./backends.html">backends</a>.</p>
</li>
<li>
<p>Stability through <a href="./lumo-corruption-detection-and-magic.html">corruption detection</a> and <a href="./WALs.html">rollback journaling</a>.</p>
</li>
<li>
<p>Reliably tested and <a href="./3.3-benchmarking.html">benchmarked</a>. </p>
</li>
</ul>
<p>NEWS! - <a href="https://lumosql.org/src/lumosql/doc/trunk/doc/LumoSQL-PhaseII-Announce.md">LumoSQL Phase II announcement</a></p>
<h2 id="a-hrefhttpslumosqlorgsrclumosqldoctrunkdoclumosql-phaseii-announcemdphase-iia-ongoing"><a class="header" href="#a-hrefhttpslumosqlorgsrclumosqldoctrunkdoclumosql-phaseii-announcemdphase-iia-ongoing"><a href="https://lumosql.org/src/lumosql/doc/trunk/doc/LumoSQL-PhaseII-Announce.md">Phase II</a> (ongoing)</a></h2>
<ul>
<li><a href="https://lumosql.org/src/lumosql/file?name=doc/rbac-design.md"><strong>Role-based / attribute-based access control</strong></a></li>
<li><strong>Implementation of hidden colums and tables</strong></li>
<li><strong>Row level encryption</strong></li>
<li><a href="https://lumosql.org/src/lumosql/doc/trunk/doc/rfc/README.md"><strong>Reseach and design of Lumions</strong></a>
<ul>
<li><a href="../references/lumosql-abe.bib">Bibliography</a>(download .bib)</li>
</ul>
</li>
</ul>
<h2 id="phase-i-complete"><a class="header" href="#phase-i-complete">Phase I (complete)</a></h2>
<p>LumoSQL started as a combination of two embedded data storage C language libraries: <a href="https://sqlite.org">SQLite</a> and <a href="https://github.com/LMDB/lmdb">LMDB</a>. LumoSQL builds on Howard Chu's 2013 proof of concept <a href="https://github.com/LMDB/sqlightning">sqlightning</a> combining the two codebases. Howard's LMDB library has become a ubiquitous replacement for <a href="https://sleepycat.com/">bdb</a> on the basis of performance, reliability, and license so the 2013 claims of it greatly increasing the performance of SQLite seemed credible. D Richard Hipp's SQLite is used in thousands of software projects, and since three of them are Google's Android, Mozilla's Firefox and Apple's iOS, an improved version of SQLite will benefit billions of people.</p>
<ul>
<li>
<p><strong>Research</strong></p>
<ul>
<li><a href="./2.1-development-landscape.html">SQLite Development Landscape</a></li>
<li><a href="./3.7-relevant-codebases.html">What other software implements useful features?</a></li>
<li><a href="./2.4-relevant-knowledgebase.html">What research has been done on SQLite topics?</a></li>
<li><a href="./WALs.html">What is the best way to maintain journals?</a></li>
<li><a href="./online-database-servers.html">How database storage systems are scaled?</a></li>
<li><a href="./what-are-savepoints.html">What are savepoints in SQLite?</a></li>
<li><a href="./3.6-development-notes.html">Conclusions prior to development</a></li>
</ul>
</li>
<li>
<p><strong>Design</strong></p>
<ul>
<li><a href="./1.2-top-features.html">Feaures that LumoSQL will implement</a> </li>
<li><a href="./api.html">Identifying the API points of SQLite that LumoSQL will intercept</a></li>
<li><a href="./virtual-machine.html">Changes to be made to SQLite virtual machine layer</a></li>
</ul>
</li>
<li>
<p><strong>Implemented Features</strong></p>
<blockquote>
<ul>
<li><a href="https://lumosql.org/src/lumosql/doc/trunk/doc/lumo-build-benchmark.md"><strong>Build</strong></a></li>
</ul>
</blockquote>
<blockquote>
<p>LumoSQL build and testing system allows the user to choose any version of SQLite and any available backend version, as well as other options during build in order to build a database best suited for user's needs. The performance of LumoSQL database can be tested and benchmared using the same tool.</p>
</blockquote>
<blockquote>
<ul>
<li><a href="https://lumosql.org/src/not-forking/doc/trunk/README.md"><strong>Not-Forking tool</strong></a></li>
</ul>
</blockquote>
<blockquote>
<p>In order to make LumoSQL modular and compatible with a range of upstream versions, we have developed a tool that attempts to automate source code tracking. By tracking changes it avoids project level forking and therefore is called a not-forking tool.</p>
</blockquote>
<blockquote>
<ul>
<li><a href="./backends.html"><strong>LMDB and BDB backends</strong></a> </li>
</ul>
</blockquote>
<blockquote>
<p>LMDB provides a fast and reliable way to store key-value data and has been proven by <a href="https://github.com/LMDB/sqlightning">Howard Chu</a> to outperform the native SQLite b-tree in some situations.</p>
</blockquote>
<blockquote>
<ul>
<li><a href="./lumo-corruption-detection-and-magic.html"><strong>Row level checksums</strong></a></li>
</ul>
</blockquote>
<blockquote>
<p>Row level checksums lets us find out if the data has been corrupted and locate the precise row that has been affected, thus making it easier to fix corruption issues.</p>
</blockquote>
<blockquote>
<ul>
<li><a href="./3.3-benchmarking.html"><strong>Benchmarking tool</strong></a></li>
</ul>
</blockquote>
<blockquote>
<p>In order to test the performace of LumoSQL and prove or disprove its effectiveness we want to make sure that our benchmarking results are accurate and reproducible.</p>
</blockquote>
</li>
</ul>
<p>LumoSQL is currently under development. Contributions to <a href="https://lumosql.org/src/lumosql/file?name=CONTRIBUTING.md">code</a> and <a href="../README.html">documentation</a> are welcome. </p>
<p>LumoSQL was started in December 2019 by Dan Shearer, who did the original source tree archaeology, patching and test builds. Keith Maxwell joined shortly after and contributed version management to the Makefile and the benchmarking tools. </p>
<p>LumoSQL is supported by the <a href="https://nlnet.nl/project/LumoSQL/">NLnet Foundation</a>.</p>
<p>Published under <a href="./3.2-legal-aspects.html">MIT license</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><!-- Copyright 2020 The LumoSQL Authors, see LICENSES/MIT -->
<!-- SPDX-License-Identifier: MIT -->
<!-- SPDX-FileCopyrightText: 2020 The LumoSQL Authors -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Dan Shearer, December 2019 -->
<h1 id="lumosql"><a class="header" href="#lumosql">LumoSQL</a></h1>
<p><a href="lumosql.org">LumoSQL</a> is a modification (not a fork) of the
<a href="https://sqlite.org">SQLite</a> embedded data storage library, the <a href="https://sqlite.org/mostdeployed.html">most-deployed software</a>.
LumoSQL adds performance, security and privacy features, partly by adding
multiple backend storage systems.  If you are an SQLite user familiar with C
development wanting an easier way to benchmark and measure SQLite, or if you
are wanting features only available in other key-value storage engines, then
you may find LumoSQL interesting.</p>
<p>In <a href="./doc/LumoSQL-PhaseII-Announce.html">Phase II of LumoSQL</a> we are building on 
the existing optional per-row checksums to add per-row <a href="https://en.wikipedia.org/wiki/Attribute-based_encryption">Attribute-Based Encryption (ABE)</a> and much more.</p>
<p>In the existing LumoSQL 0.4 there are currently three LumoSQL backends:</p>
<ul>
<li>the default SQLite Btree storage system</li>
<li><a href="https://github.com/LMDB/lmdb">LMDB</a></li>
<li><a href="https://en.wikipedia.org/wiki/Berkeley_DB">the Berkley Database</a></li>
</ul>
<p>LumoSQL has a build and benchmarking tool for comparing vanilla SQLite versions
and configurations with each other, as well as comparing the performance of
different storage backends. LumoSQL is written in C, like SQLite. The
benchmarking and other tools are written in Tcl, like much of the tooling and
extensions for SQLite and Fossil. The build tool guarantees that options and
configurations are always selected in the same way, so that benchmark results are 
reliable.</p>
<p>LumoSQL is distributed under <a href="LICENCES/README.html">very liberal licence terms</a>.</p>
<p>LumoSQL is supported by the <a href="https://nlnet.nl">NLNet Foundation</a>.</p>
<p>Neither Windows nor Android are supported at present, despite being important
SQLite targets. We do plan to do so, and in addition contributors are most
welcome via the <a href="/">LumoSQL Fossil site</a>.</p>
<h1 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h1>
<ul>
<li><a href="about.html#design-not-forking-and-participating">Design, Not-Forking and Participating</a></li>
<li><a href="about.html#lumosql-and-sqlites-billions-of-users">LumoSQL, and SQLite's Billions of Users</a></li>
<li><a href="about.html#limitations-of-lumosql">Limitations of LumoSQL</a></li>
<li><a href="about.html#build-environment-and-dependencies">Build Environment and Dependencies</a></li>
<li><a href="about.html#using-the-build-and-benchmark-system">Using the Build and Benchmark System</a></li>
<li><a href="about.html#a-brief-history-of-lumosql">A Brief History of LumoSQL</a></li>
</ul>
<p><a name="design-not-forking-and-participating"></a></p>
<h2 id="design-not-forking-and-participating"><a class="header" href="#design-not-forking-and-participating">Design, Not-Forking and Participating</a></h2>
<p>If you are reading this on Github, then you are looking at a mirror. LumoSQL is
is maintained using <a href="/">the Fossil repository</a>. If you 
want to participate in LumoSQL there is a forum, and if you have code contributions
you can ask for access to the respository.</p>
<p>LumoSQL has multiple upstreams, but does not fork any of them despite needing modifications.
The novel <a href="https://lumosql.org/src/not-forking">Not-forking</a> tool semi-automatically 
tracks upstream changes and is a requirement for building LumoSQL. Between not-forking 
and the <a href="doc/lumo-build-benchmark.html">LumoSQL Build and Benchmark System</a>,
LumoSQL is as much about combining and configuring upstreams as it is about creating
original database software. By maintaining Not-forking outside LumoSQL, we hope
other projects will find it useful.</p>
<p>The LumoSQL and SQLite projects are cooperating, so any merge friction is
expected to become less over time, and key to that is the approach of not
forking.</p>
<p><a name="lumosql-and-sqlites-billions-of-users"></a></p>
<h2 id="lumosql-and-sqlites-billions-of-users"><a class="header" href="#lumosql-and-sqlites-billions-of-users">LumoSQL, and SQLite's Billions of Users</a></h2>
<p>LumoSQL exists to demonstrate changes to SQLite that might be useful, but which
SQLite probably cannot consider for many years because of SQLite's unique
position of being used by a majority of the world's population. </p>
<p>SQLite is used by thousands of software projects, just three being
Google's Android, Mozilla's Firefox and Apple's iOS which between them have
billions of users. That is a main reason why SQLite is so careful and conservative
with all changes.</p>
<p>On the other hand, many of these same users need SQLite to have new features
which do not fit with the SQLite project's cautious approach, and LumoSQL is a
demonstration of some of these improvements. </p>
<p>The LumoSQL documentation project reviews dozens of relevant codebases.  SQLite
has become ubiquitous over two decades, which means there is a great deal of
preparation needed when considering architectural changes.</p>
<p><a name="limitations-of-lumosql"></a></p>
<h2 id="limitations-of-lumosql"><a class="header" href="#limitations-of-lumosql">Limitations of LumoSQL</a></h2>
<p>As of LumoSQL 0.4, there are many obvious limitations, including:</p>
<ul>
<li>The tests used in benchmarking mostly come from an ancient version of SQLite's
speedtest.tcl modified many times, to which DATASIZE
and DEBUG have been added. Experts in SQLite and LMDB database testing 
should review the files in not-fork.d/sqlite3/benchmark/*test. There are 
<a href="https://sqlite.org/src/dir?ci=tip&amp;name=tool">9 tools named *speed*</a> 
in the SQLite source, and any/all of them should be added here.</li>
<li>Neither LMDB nor BDB backends ship with latest SQLite builds. Now all the LumoSQL infrastructure
exists, that is a smaller, more maintainable and repeatable task. But it is not done yet.
There are some generic problems to be solved in the process, such as the optimal way to
address keysize disparities between a KVP store provider and SQLite's internal large keysize.</li>
<li>If we import more of the speed tests from SQLite identified above, then we will 
have a problem with several LMDB and at least two BDB instances, where the SQLite
tests will fail. In most cases this is about the LMDB port needing to be more 
complete but in some it is about relevance, where some SQLite tests will not apply. In
addition some backends will always need
to have additional tests (for example, BDB has more extensive user management than 
SQLite).</li>
</ul>
<p><a name="a-brief-history-of-lumosql"></a></p>
<h2 id="a-brief-history-of-lumosql"><a class="header" href="#a-brief-history-of-lumosql">A Brief History of LumoSQL</a></h2>
<p>There have been several implementations of new storage backends to SQLite, all of them hard forks
and nearly all dead forks. A backend needs certain characteristics:</p>
<ul>
<li>btree-based key-value store</li>
<li>transactions, or fully ACID</li>
<li>full concurrency support, or fully MVCC</li>
</ul>
<p>There are not many candidate key-value stores. One of the most widely-used is
Howard Chu's LMDB. There was a lot of attention in 2013 when Howard released
his <a href="https://github.com/LMDB/sqlightning">proof of concept SQLite port</a>. LMDB
operates on a very different and more modern principle to all other widely-used
key/value stores, potentially bringing benefits to some users of SQLite. In
2013, the ported SQLite gave significant performance benefits.</p>
<p>The original 2013 code modified the SQLite <code>btree.c</code> from version SQLite
version 3.7.17 to use LMDB 0.9.9 . It took considerable work for LumoSQL to
excavate the ancient code and reproduce the results.</p>
<p>By January 2020 the LumoSQL project concluded:</p>
<ul>
<li>Howard's 2013 performance work is reproducible</li>
<li>SQLite's key-value store improved in performance since 2013, getting close to
parity with LMDB by some measures</li>
<li>SQLite can be readily modified to have multiple storage backends and still
pass 'make test'</li>
<li>SQLite doesn't expect there to be multiple backends, and this has many effects
including for example in error handling. An abstraction layer was needed.</li>
</ul>
<p>Since then, many new possibilities have emerged for LumoSQL, and new collaborations.</p>
<div style="break-before: page; page-break-before: always;"></div><!--- SPDX-License-Identifier: CC-BY-SA-4.0 --->
<!--- SPDX-FileCopyrightText: 2020 The LumoSQL Authors --->
<!--- SPDX-ArtifactOfProjectName: LumoSQL --->
<!--- SPDX-FileType: Documentation --->
<!--- SPDX-FileComment: Original by Dan Shearer, 2020 --->
<h1 id="table-of-contents-1"><a class="header" href="#table-of-contents-1">Table of Contents</a></h1>
<ul>
<li><a href="1.2-top-features.html#overall-objective-of-lumosql">Overall Objective of LumoSQL</a></li>
<li><a href="1.2-top-features.html#the-advantages-of-lumosql">The Advantages of LumoSQL</a></li>
<li><a href="1.2-top-features.html#developmenr-goals">Development Goals</a></li>
</ul>
<p><img src="./images/lumo-project-aims-intro.jpg" alt="" title="Mongolian horseback archery, rights request pending from https://www.toursmongolia.com/" /></p>
<h1 id="overall-objective-of-lumosql"><a class="header" href="#overall-objective-of-lumosql">Overall Objective of LumoSQL</a></h1>
<pre><code>To create Privacy-compliant Open Source Database Platform with Modern Design and Benchmarking,
usable either embedded or online.
</code></pre>
<p>This is the guide for every aspect of the project, which will ensure that
LumoSQL offers features that money can't buy, and drawing together an
SQLite-related ecosystem.</p>
<p>LumoSQL is based on SQLite. It aims to incorporate all of the <a href="https://www.sqlite.org/features.html">features of SQLite</a> and improve it many ways.</p>
<h1 id="development-goals"><a class="header" href="#development-goals">Development Goals</a></h1>
<ul>
<li>
<p>SQLite upstream promise: LumoSQL does not fork SQLite, and offers 100%
compatibility with SQLite by default, and will contribute to SQLite where possible.
This especially includes the SQLite user interface mechanisms of pragmas, 
library APIs, and commandline parameters.</p>
</li>
<li>
<p><a href="./3.2-legal-aspects.html">Legal promise</a>: LumoSQL does not come with legal terms less favourable than 
SQLite. LumoSQL will aim to improve the legal standing and safety worldwide
as compared to SQLite. </p>
</li>
<li>
<p>Developer contract: LumoSQL has <a href="./api.html">stable APIs</a> (<a href="https://en.wikipedia.org/wiki/Application_programming_interface#Libraries_and_frameworks">Application Programming Interfaces</a>) for features found in multiple unrelated SQLite downstream projects:
backends, frontends, encryption, networking and more. </p>
</li>
<li>
<p>Devops contract: LumoSQL reduces risk by making it possible to omit
compilation of unneeded features, and has stable ABIs (<a href="https://en.wikipedia.org/wiki/Application_binary_interface">Application Binary Interfaces</a>) so as to not break dynamically-linked applications.</p>
</li>
<li>
<p>Ecosystem creation: LumoSQL will offer consolidated contact, code curation, bug tracking,
licensing, and community communications across all these features from
other projects. Bringing together SQLite code contributions under one umbrella reduces 
technical risk in many ways, from inconsistent use of threads to tracking updated versions.</p>
</li>
</ul>
<h1 id="lumosql-design"><a class="header" href="#lumosql-design">LumoSQL Design</a></h1>
<ul>
<li>
<p>LumoSQL has three canonical and initial backends: btree (the existing
SQLite btree, ported to a new backend system); the LMDB backend; and the BDB
backend. Control over these interfaces is through the
same user interface mechanisms as the rest of LumoSQL, and SQLite.</p>
</li>
<li>
<p>LumoSQL improves SQLite quality and privacy compliance by introducing
optional on-disk checksums for storage backends including the original
SQLite btree format.  This allows real-time row-level <a href="./lumo-corruption-detection-and-magic.html">corruption detection</a>.</p>
</li>
<li>
<p>LumoSQL improves SQLite quality and privacy compliance by introducing
<a href="./backends.html">optional storage backends</a> that are more crash-resistant than SQLite btree (such as LMDB)
and more oriented towards complete recovery (such as BDB).</p>
</li>
<li>
<p>LumoSQL improves SQLite integrity in persistent storage by introducing
optional row-level checksums.</p>
</li>
<li>
<p>LumoSQL provides the benefits of Open Source by being an open project
and continuing to accept and review contributions in an open way, using
Fossil and having diverse <a href="../CONTRIBUTING.html">contributors</a>.</p>
</li>
<li>
<p>LumoSQL improves SQLite design by intercepting <a href="./api.html">APIs</a> at a very small
number of critical choke-points, and giving the user optional choices at
these choke points. The choices are for alternative storage backends,
front end parsers, encryption, networking and more, all without removing
the <a href="https://sqlite.org/zeroconf.html">zero-config</a> and embedded advantages of SQLite</p>
</li>
<li>
<p>LumoSQL provides a means of tracking upstream SQLite, by making
sure that anything other than the API chokepoints can be synched at each
release, or more often if need be.</p>
</li>
<li>
<p>LumoSQL provides updated public <a href="">testing tools</a>, with results published
and instructions for reproducing the test results. This also means
excluding parts of the LumoSQL test suite that don't apply to new backends</p>
</li>
<li>
<p>LumoSQL provides <a href="./3.3-benchmarking.html">benchmarking tools</a>, otherwise as per the testing
tools.</p>
</li>
<li>
<p>LumoSQL ensures that new code remains optional by means of <a href="./3.5-lumo-test-build.html">modularity</a> at
compiletime and also runtime. By illustration of modularity, at compiletime
nearly all 30 million lines of the Linux kernel can be excluded giving just 200k
lines. Runtime modularity is controlled through the same user interfaces 
as the rest of LumoSQL.</p>
</li>
<li>
<p>LumoSQL ensures that new code may be active at once, eg.
multiple backends or frontends for conversion between/upgrading from one
format or protocol to another. This is important to provide continuity and
supported upgrade paths for users, for example, users who want to become
privacy-compliant without disrupting their end users.</p>
</li>
<li>
<p>Over time, LumoSQL will carefully consider the potential benefits of dropping
some of the most ancient parts of SQLite when merging from upstream, provided
it does not conflict with any of the other goals in this document. Eliminating 
SQLite code can be done by a similar non-forking mechanism as used to keep in synch
with the SQLite upstream. Patches will be offered to sqlite.org</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><!-- SPDX-License-Identifier: AGPL-3.0-only -->
<!-- SPDX-FileCopyrightText: 2020 The LumoSQL Authors, 2019 Oracle -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Dan Shearer, 2020 -->
<h1 id="install-lumosql"><a class="header" href="#install-lumosql">Install LumoSQL</a></h1>
<p>Installation consists of obtaining all relevant 3rd party dependencies (tcl, tclx, perl with modules TextGlob and Git, file, gnumake, gzip, gnutar, fossil or git, wget or curl), then downloading and installing the not-forking tool as an executable command, and downloading LumoSQL build tools that work currently as <code>make</code> commands. </p>
<h3 id="containers"><a class="header" href="#containers">Containers</a></h3>
<p>The maintainers are test building LumoSQL on Debian, Fedora, Gentoo and Ubuntu.
Container images with the dependencies installed are available at
<a href="https://quay.io/repository/keith_maxwell/lumosql-build">https://quay.io/repository/keith_maxwell/lumosql-build</a> and the build steps are
in <a href="https://github.com/maxwell-k/containers">https://github.com/maxwell-k/containers</a>.</p>
<h3 id="installing-not-forking"><a class="header" href="#installing-not-forking">Installing Not-forking</a></h3>
<p>This step requires perl with Text::Glob and a few other core modules, not-forking installation will inform you if the relevant modules are missing on your system, more information on installing it on different systems can be found below, as well as with the <a href="https://lumosql.org/src/not-forking/doc/trunk/README.md">not-forking documentation</a>.</p>
<pre><code>wget -O- https://lumosql.org/src/not-forking/tarball/trunk/Not-forking-trunk.tar.gz | tar -zxf -
cd Not-forking-trunk
perl Makefile.PL
make
sudo make install      
</code></pre>
<h3 id="build-lumosql"><a class="header" href="#build-lumosql">Build LumoSQL</a></h3>
<p>Dependencies needed for this step include (tcl, tclx, file, gnumake, gzip, gnutar, fossil or git, wget or curl). Read below for more information on installing them on your distribution of choice.</p>
<pre><code>fossil clone https://lumosql.org/src/lumosql
cd lumosql
make what
</code></pre>
<p><code>fossil</code> command can be replaced with <code>git</code> or <code>wget</code></p>
<p><code>make what</code> will show which taregts will be built by typing <code>make</code></p>
<p>Now you can build different target databases and benchmark them. For make options read the <a href="./quickstart.html">quickstart</a> and the <a href="./lumo-build-benchmark.html">build and benchmark system</a> sections.</p>
<h2 id="installing-on-popular-distributions"><a class="header" href="#installing-on-popular-distributions">Installing on Popular Distributions</a></h2>
<h3 id="perl-modules"><a class="header" href="#perl-modules">Perl Modules</a></h3>
<p>The default Perl installation on Debian/Ubuntu is perl-base, and on Fedora/Red
Hat is perl-core. These have nearly all the Perl modules required except for Text::Glob.</p>
<p>For example, on a Debian or Ubuntu system, as root type:</p>
<pre><code># apt install libtext-glob-perl
</code></pre>
<p>Or on a Fedora/Red Hat system, as root type:</p>
<pre><code># dnf install perl-text-glob
</code></pre>
<p>Or on a Gentoo system, as root type:</p>
<pre><code>emerge --ask dev-perl/Text-Glob
</code></pre>
<p>On FreeBSD:</p>
<pre><code>pkg install perl5 p5-Text-Glob
# for the complete list of recommended programs to access source repositories:
pkg install fossil perl5 git p5-Git-Wrapper curl p5-Text-Glob patch
</code></pre>
<p>On minimal operating systems such as often used with <a href="https://docker.io">Docker</a> there is just
a basic Perl package present. You will need to add other modules including ExtUtils::MakeMaker,
Digest::SHA, Perl::Git, File::Path and Perl::FindBin .</p>
<p>Not-forking will inform you of any missing Perl modules.</p>
<h2 id="download-and-install-not-forking"><a class="header" href="#download-and-install-not-forking">Download and Install Not-Forking</a></h2>
<p>To download Not-forking, you can use <code>fossil clone</code> or <code>git clone</code>, or,  to download with wget:</p>
<pre><code>wget -O- https://lumosql.org/src/not-forking/tarball/trunk/Not-forking-trunk.tar.gz | tar -zxf -
cd Not-forking-trunk
</code></pre>
<p>Once you have downloaded the Not-forking source, you can install it using:</p>
<pre><code>perl Makefile.PL
make
sudo make install       # You need root for this step, via sudo or otherwise
</code></pre>
<p>If you are on a minimal operating system you may be missing some Perl modules
as decsribed above. The command <code>perl Makefile.PL</code> will fail with a helpful
message if you are missing modules needed to build Not-forking. Once you have
satisfied the Not-forking <em>build</em> dependencies, you can check that Not-forking
has everything it could possibly need by typing:</p>
<pre><code>not-fork --check-recommend
</code></pre>
<p>and fixing anything reported as missing, or which is too old in cases where that matters.</p>
<p>At which point the <code>not-fork</code> command is installed in the system and its
required modules are available where your perl installation expects to
find them.</p>
<h2 id="build-environment-and-dependencies-for-lumosql-build"><a class="header" href="#build-environment-and-dependencies-for-lumosql-build">Build Environment and Dependencies for LumoSQL build</a></h2>
<h4 id="debian-or-ubuntu-derived-operating-systems"><a class="header" href="#debian-or-ubuntu-derived-operating-systems">Debian or Ubuntu-derived Operating Systems</a></h4>
<p>Uncomment existing <code>deb-src</code> line in /etc/apt/sources.list, for example
for Ubuntu 20.04.2 a valid line is:
<b></p>
<pre><code>deb-src http://gb.archive.ubuntu.com/ubuntu focal main restricted
</code></pre>
</b>
<p>Then run
<b></p>
<pre><code>sudo apt update                              # this fetches the deb-src updates
sudo apt full-upgrade                        # this gets the latest OS updates
sudo apt install git build-essential tclx
sudo apt build-dep sqlite3
</code></pre>
</b>
<p>The <em>exact</em> commands above have been tested on a pristine install of Ubuntu
20.04.2 LTS, as installed from ISO or one of the operating systems shipped with
Windows Services for Linux.</p>
<h4 id="fedora-derived-operating-systems"><a class="header" href="#fedora-derived-operating-systems">Fedora-derived Operating Systems</a></h4>
<p>On any reasonably recent Fedora-derived Linux distribution, including Red Hat:</p>
<pre><code class="language-sh">&lt;b&gt;
sudo dnf install --assumeyes \
  git make gcc ncurses-devel readline-devel glibc-devel autoconf tcl-devel tclx-devel
</code></pre>
</b>
<h4 id="common-to-all-linux-operating-systems"><a class="header" href="#common-to-all-linux-operating-systems">Common to all Linux Operating Systems</a></h4>
<ul>
<li>
<p>Recommended: <a href="https://fossil-scm.org/">Fossil</a>. As described above, you don't necessarily need Fossil. But Fossil is very easy to install: if you can't get version 2.13 or later from your distrbution then it is easy to build from source. 
(<em>Note!</em> Ubuntu 20.04, Debian Buster and Gentoo do not include a sufficiently modern Fossil, while NetBSD
and Ubuntu 20.10 do.) Since you now have a development environment anyway you can 
<a href="https://fossil-scm.org/home/doc/trunk/www/build.wiki">build Fossil trunk according to the official instructions</a> or this simpler version (tested on Ubuntu 20.04 LTS):</p>
<ul>
<li>wget -O- https://fossil-scm.org/home/tarball/trunk/Fossil-trunk.tar.gz |  tar -zxf -</li>
<li>sudo apt install libssl-dev</li>
<li>cd Fossil-trunk ; ./configure ; make</li>
<li>sudo make install</li>
</ul>
</li>
<li>
<p>For completeness (although every modern Linux/Unix includes these), to build and benchmark any of the Oracle Berkeley DB targets, you need either &quot;curl&quot; or &quot;wget&quot;, and also &quot;file&quot;, &quot;gzip&quot; and GNU &quot;tar&quot;. Just about any version of these will be sufficient, even on Windows.</p>
</li>
<li>
<p>If you are running inside a fresh <a href="https://docker.io">Docker</a> or similar container system, Fossil may be confused about the user id. One solution is to add a user (eg &quot;adduser lumosql&quot; and answer the questions) and then &quot;export USER=lumosql&quot;.</p>
</li>
</ul>
<p>On <a href="https://www.debian.org/releases/buster/">Debian 10 &quot;Buster&quot; Stable Release</a>, the not-forking makefile
(&quot;perl Makefile.PL&quot;) will warn that git needs to be version 2.22 or higher.
Buster has version 2.20, however this is not a critical error. If you don't
like error messages scrolling past during a build, then install a more recent
git <a href="https://backports.debian.org/Instructions/">from Buster backports</a>.</p>
<p>Now you have the dependencies installed, clone the LumoSQL repository using
<code>fossil clone https://lumosql.org/src/lumosql</code> , which will create a new subdirectory called <code>lumosql</code> and
a file called <code>lumosql.fossil</code> in the current directory.</p>
<p>Try:
<b></p>
<pre><code>cd lumosql
make what
</code></pre>
</b>
<p>To see what the default sources and options are. The <code>what</code> target does not make any changes although it may generate a file <code>Makefile.options</code> to help <code>make</code> parse the command line.</p>
<div style="break-before: page; page-break-before: always;"></div><p><a name="build-environment-and-dependencies"></a></p>
<h2 id="build-environment-and-dependencies"><a class="header" href="#build-environment-and-dependencies">Build Environment and Dependencies</a></h2>
<p>Most developers already have the required minimum of git and core *ix
development tools. SQLite has very few dependencies (mostly Tcl), and
LumoSQL adds one Perl-based processing tool.</p>
<p>LumoSQL is mirrored to Github and application developers can use git
with Github in the usual way. LumoSQL developers working on the LumoSQL
library internals choose to use <a href="https://fossil-scm.org">Fossil source code
manager</a> instead of git, and if you're planning
to develop LumoSQL internals then you need Fossil.</p>
<p>There are many <a href="https://www.fossil-scm.org/home/doc/trunk/www/fossil-v-git.wiki">reasons why people choose
Fossil</a>.
For LumoSQL one of them is that SQLite and Fossil are symbiotic
projects, each written in the other.</p>
<h4 id="debian-or-ubuntu-derived-operating-systems-1"><a class="header" href="#debian-or-ubuntu-derived-operating-systems-1">Debian or Ubuntu-derived Operating Systems</a></h4>
<p>Uncomment existing <code>deb-src</code> line in /etc/apt/sources.list, for example
for Ubuntu 20.04.2 a valid line is:</p>
<pre><code>deb-src http://gb.archive.ubuntu.com/ubuntu focal main restricted
</code></pre>
</b>
<p>Then run</p>
<pre><code>sudo apt update                              # this fetches the deb-src updates
sudo apt full-upgrade                        # this gets the latest OS updates
sudo apt install git build-essential tclx
sudo apt build-dep sqlite3
</code></pre>
</b>
<p>The <em>exact</em> commands above have been tested on a pristine install of Ubuntu
20.04.2 LTS, as installed from ISO or one of the operating systems shipped with
Windows Services for Linux.</p>
<h4 id="fedora-derived-operating-systems-1"><a class="header" href="#fedora-derived-operating-systems-1">Fedora-derived Operating Systems</a></h4>
<p>On any reasonably recent Fedora-derived Linux distribution, including Red Hat:</p>
<pre><code class="language-sh">sudo dnf install --assumeyes \
  git make gcc ncurses-devel readline-devel glibc-devel autoconf tcl-devel tclx-devel
</code></pre>
</b>
<h4 id="common-to-all-linux-operating-systems-1"><a class="header" href="#common-to-all-linux-operating-systems-1">Common to all Linux Operating Systems</a></h4>
<p>Once you have done the setup specific to your operating system in the previous
steps, the following should work on reaonably recent Debian and Fedora-related
operating systems, and Gentoo. </p>
<p>Other required tools can be installed from your operating system's standard packages.
Here are the tool dependencies:</p>
<ul>
<li>Mandatory: <a href="https://lumosql.org/src/not-forking/">the not-forking tool</a>, 
which is a Perl script that needs to be downloaded and installed manually. The instructions for not-forking are on its website.</li>
<li>Recommended: <a href="https://fossil-scm.org/">Fossil</a>. As described above, you don't necessarily need Fossil. But Fossil is very easy to install: if you can't get version 2.13 or later from your distrbution then it is easy to build from source. 
(<em>Note!</em> Ubuntu 20.04, Debian Buster and Gentoo do not include a sufficiently modern Fossil, while NetBSD
and Ubuntu 20.10 do.) Since you now have a development environment anyway you can 
<a href="https://fossil-scm.org/home/doc/trunk/www/build.wiki">build Fossil trunk according to the official instructions</a> or this simpler version (tested on Ubuntu 20.04 LTS):
<ul>
<li>wget -O- https://fossil-scm.org/home/tarball/trunk/Fossil-trunk.tar.gz |  tar -zxf -</li>
<li>sudo apt install libssl-dev</li>
<li>cd Fossil-trunk ; ./configure ; make</li>
<li>sudo make install</li>
</ul>
</li>
<li>For completeness (although every modern Linux/Unix includes these), to build and benchmark any of the Oracle Berkeley DB targets, you need either &quot;curl&quot; or &quot;wget&quot;, and also &quot;file&quot;, &quot;gzip&quot; and GNU &quot;tar&quot;. Just about any version of these will be sufficient, even on Windows.</li>
<li>If you are running inside a fresh <a href="https://docker.io">Docker</a> or similar container system, Fossil may be confused about the user id. One solution is to add a user (eg &quot;adduser lumosql&quot; and answer the questions) and then &quot;export USER=lumosql&quot;.</li>
</ul>
<p>The not-forking tool will advise you with a message
if you need a tool or a version that is not installed. </p>
<p>On <a href="https://www.debian.org/releases/buster/">Debian 10 &quot;Buster&quot; Stable Release</a>, the not-forking makefile
(&quot;perl Makefile.PL&quot;) will warn that git needs to be version 2.22 or higher.
Buster has version 2.20, however this is not a critical error. If you don't
like error messages scrolling past during a build, then install a more recent
git <a href="https://backports.debian.org/Instructions/">from Buster backports</a>.</p>
<p><a name="using-the-build-and-benchmark-system"></a></p>
<h2 id="quickstart-using-the-build-and-benchmark-system"><a class="header" href="#quickstart-using-the-build-and-benchmark-system">Quickstart: Using the Build and Benchmark System</a></h2>
<p>This is a very brief quickstart, for full detail see the
<a href="./doc/lumo-build-benchmark.html">Build and Benchmark System documentation</a>. </p>
<p>Now you have the dependencies installed, clone the LumoSQL repository using
<code>fossil clone https://lumosql.org/src/lumosql</code> , which will create a new subdirectory called <code>lumosql</code> and
a file called <code>lumosql.fossil</code> in the current directory.</p>
<p>Try:</p>
<pre><code>cd lumosql
make what
</code></pre>
</b>
<p>To see what the default sources and options are. The <code>what</code> target does not make any changes although it may generate a file <code>Makefile.options</code> to help <code>make</code> parse the command line.</p>
<p>Benchmarking a single binary should take no longer than 4 minutes to complete depending
on hardware. The results are stored in an SQLite database stored in the LumoSQL 
top-level directory by default, that is, the directory you just created using <code>fossil clone</code>.</p>
<p>Start by building and benchmarking the official SQLite release version 3.35.5, which is the current
release at the time of writing this README.</p>
<p><code>make benchmark USE_LMDB=no USE_BDB=no SQLITE_VERSIONS='3.35.5'</code>
</b></p>
<p>All source files fetched are cached in ~/.cache/LumoSQL in a way that maximises reuse regardless of 
their origin (Fossil, git, wget etc) and which minimises errors. The LumoSQL build system is driving the
<code>not-fork</code> tool, which maintains the cache. Not-fork will download just the differences of a remote 
version if most of the code is already in cache.</p>
<p>The output from this make command will be lots of build messages followed by something like this:</p>
<pre><code>*** Running benchmark 3.35.5
    TITLE = sqlite 3.35.5
    SQLITE_ID = 1b256d97b553a9611efca188a3d995a2fff71275
    SQLITE_NAME = 3.35.5 2021-04-19 18:32:05 1b256d97b553a9611efca188a3d995a2fff712759044ba480f9a0c9e98faalt1
    DATASIZE = 1
    DEBUG = off
    LMDB_DEBUG = off
    LMDB_FIXED_ROWID = off
    LMDB_TRANSACTION = optimistic
    ROWSUM = off
    ROWSUM_ALGORITHM = sha3_256
    SQLITE3_JOURNAL = default
    RUN_ID = 70EA47101F68CDD6D3C0ED255962A2AA50F1540EE4FEBB46A03FAD888B49676C
          OK     0.003   1 Creating database and tables
          OK     0.019   2 1000 INSERTs
          OK     0.007   3 100 UPDATEs without an index, upgrading a read-only transaction
          OK     0.052   4 25000 INSERTs in a transaction
          OK     0.113   5 100 SELECTs without an index
          OK     0.243   6 100 SELECTs on a string comparison
          OK     0.012   7 Creating an index
          OK     0.046   8 5000 SELECTs with an index
          OK     0.036   9 1000 UPDATEs without an index
          OK     0.113  10 25000 UPDATEs with an index
          OK     0.093  11 25000 text UPDATEs with an index
          OK     0.032  12 INSERTs from a SELECT
          OK     0.020  13 DELETE without an index
          OK     0.028  14 DELETE with an index
          OK     0.027  15 A big INSERT after a big DELETE
          OK     0.010  16 A big DELETE followed by many small INSERTs
          OK     0.005  17 DROP TABLE
                 0.859 (total time)
</code></pre>
</b>
<p>A database with the default name of <code>benchmarks.sqlite</code> has been created with
two tables containing the results. This is one single test run, and the test
run data is kept in the table <code>test_data</code>. The table <code>run_data</code> contains data
relative to a set of runs (version numbers, time test started, etc). This is cumulative,
so another invocation of <code>make benchmark</code> will append to <code>benchmarks.sqlite</code>.</p>
<p>Every run is assigned a SHA3 hash, which helps in making results persistent over time and 
across the internet.</p>
<p>The tool <code>benchmark-filter.tcl</code> does some basic processing of these results:</p>
<pre><code>tool/benchmark-filter.tcl
RUN_ID                                                            TARGET  DATE        TIME         DURATION
70EA47101F68CDD6D3C0ED255962A2AA50F1540EE4FEBB46A03FAD888B49676C  3.35.5  2021-05-20  16:13:18        0.859
</code></pre>
</b>
<p>The option DATASIZE=<strong>parameter</strong> is a multiplication factor on the size of the chunks that is used for 
benchmarking. This is useful because it can affect the time it takes to run the tests by a very different
multiplication factor:</p>
<pre><code>make benchmark USE_LMDB=no USE_BDB=no DATASIZE=2 SQLITE_VERSIONS='3.35.5 3.33.0'
</code></pre>
</b>
<p>followed by:</p>
<pre><code>tool/benchmark-filter.tcl 
RUN_ID                                                            TARGET              DATE        TIME         DURATION
70EA47101F68CDD6D3C0ED255962A2AA50F1540EE4FEBB46A03FAD888B49676C  3.35.5              2021-05-20  16:13:18        0.859
65DD0759B133FF5DFBBD04C494F4631E013C64E475FC5AC06EC70F4E0333372F  3.35.5++datasize-2  2021-05-20  16:18:30        2.511
931B1489FC4477A41914A5E0AFDEF3927C306339FBB863B5FB4CF801C8F2F3D0  3.33.0++datasize-2  2021-05-20  16:18:51        2.572
</code></pre>
</b>
<p>Simplistically, these results suggest that SQLite version 3.35.5 is faster than
3.33.0 on larger data sizes, but that 3.35.5 is much faster with smaller data
sizes. After adding more versions and running the benchmarking tool again, we would
soon discover that SQLite 3.25.0 seems faster than 3.33.0, and other interesting things. 
Simplistic interpretations can be misleading :-)</p>
<p>This is a Quickstart, so for full detail you will need the 
<a href="doc/lumo-build-benchmark.html">Build/Benchmark documentation</a>. However as a teaser, and since LMDB
was the original inspiration for LumoSQL (see the 
[History section below]((#a-brief-history-of-lumosql) for more on that) here are some more things that
can be done with the LMDB target:</p>
<pre><code>$ make what LMDB_VERSIONS=all
tclsh tool/build.tcl what not-fork.d MAKE_COMMAND='make' LMDB_VERSIONS='all'
BENCHMARK_RUNS=1
COPY_DATABASES=
COPY_SQL=
MAKE_COMMAND=make
NOTFORK_COMMAND=not-fork
NOTFORK_ONLINE=0
NOTFORK_UPDATE=0
SQLITE_VERSIONS=3.35.5
USE_SQLITE=yes
USE_BDB=yes
SQLITE_FOR_BDB=
BDB_VERSIONS=
BDB_STANDALONE=18.1.32=3.18.2
USE_LMDB=yes
SQLITE_FOR_LMDB=3.35.5
LMDB_VERSIONS=all
LMDB_STANDALONE=
OPTION_DATASIZE=1
OPTION_DEBUG=off
OPTION_LMDB_DEBUG=off
OPTION_LMDB_FIXED_ROWID=off
OPTION_LMDB_TRANSACTION=optimistic
OPTION_ROWSUM=off
OPTION_ROWSUM_ALGORITHM=sha3_256
OPTION_SQLITE3_JOURNAL=default
BUILDS=
    3.35.5
    3.18.2
    +bdb-18.1.32
    3.35.5+lmdb-0.9.11
    3.35.5+lmdb-0.9.12
    3.35.5+lmdb-0.9.13
    3.35.5+lmdb-0.9.14
    3.35.5+lmdb-0.9.15
    3.35.5+lmdb-0.9.16
    3.35.5+lmdb-0.9.17
    3.35.5+lmdb-0.9.18
    3.35.5+lmdb-0.9.19
    3.35.5+lmdb-0.9.20
    3.35.5+lmdb-0.9.21
    3.35.5+lmdb-0.9.22
    3.35.5+lmdb-0.9.23
    3.35.5+lmdb-0.9.24
    3.35.5+lmdb-0.9.25
    3.35.5+lmdb-0.9.26
    3.35.5+lmdb-0.9.27
    3.35.5+lmdb-0.9.28
    3.35.5+lmdb-0.9.29
TARGETS=
    3.35.5
    3.18.2
    +bdb-18.1.32
    3.35.5+lmdb-0.9.11
    3.35.5+lmdb-0.9.12
    3.35.5+lmdb-0.9.13
    3.35.5+lmdb-0.9.14
    3.35.5+lmdb-0.9.15
    3.35.5+lmdb-0.9.16
    3.35.5+lmdb-0.9.17
    3.35.5+lmdb-0.9.18
    3.35.5+lmdb-0.9.19
    3.35.5+lmdb-0.9.20
    3.35.5+lmdb-0.9.21
    3.35.5+lmdb-0.9.22
    3.35.5+lmdb-0.9.23
    3.35.5+lmdb-0.9.24
    3.35.5+lmdb-0.9.25
    3.35.5+lmdb-0.9.26
    3.35.5+lmdb-0.9.27
    3.35.5+lmdb-0.9.28
    3.35.5+lmdb-0.9.29
</code></pre>
</b>
<p>After executing this build with <code>make benchmark</code> rather than <code>make what</code>, here are summary results using a 
a new parameter to <code>benchmark-filter.tcl</code>:</p>
<pre><code>$ tool/benchmark-filter.tcl -fields TARGET,DURATION
TARGET                 DURATION
3.35.5                    0.852
3.35.5+lmdb-0.9.11        1.201
3.35.5+lmdb-0.9.12        1.211
3.35.5+lmdb-0.9.13        1.212
3.35.5+lmdb-0.9.14        1.219
3.35.5+lmdb-0.9.15        1.193
3.35.5+lmdb-0.9.16        1.191
3.35.5+lmdb-0.9.17        1.213
3.35.5+lmdb-0.9.18        1.217
3.35.5+lmdb-0.9.19        1.209
3.35.5+lmdb-0.9.20        1.223
3.35.5+lmdb-0.9.21        1.229
3.35.5+lmdb-0.9.22        1.230
3.35.5+lmdb-0.9.23        1.215
3.35.5+lmdb-0.9.24        1.218
3.35.5+lmdb-0.9.25        1.219
3.35.5+lmdb-0.9.26        1.220
3.35.5+lmdb-0.9.27        1.220
3.35.5+lmdb-0.9.28        1.209
3.35.5+lmdb-0.9.29        1.209
</code></pre>
</b>
<p>Again, simplistic interpretations are insufficient, but the data here suggests that LMDB has decreased
in performance over time, to improve again with the most recent versions, and no version of LMDB is faster than native SQLite 3.35.5 . However, further
benchmark runs indicate that is not the final story, as LMDB run on slower hard disks improve in relative 
speed rapidly. And using the <code>DATASIZE</code> option also changes the picture.</p>
<p>The results for the Berkely DB backend are also most interesting.</p>
<div style="break-before: page; page-break-before: always;"></div><!-- SPDX-License-Identifier: CC-BY-SA-4.0 -->
<!-- SPDX-FileCopyrightText: 2020 The LumoSQL Authors -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Claudio Calvelli, March 2020 -->
<h1 id="not-forking-upstream-source-code-tracker"><a class="header" href="#not-forking-upstream-source-code-tracker">Not-Forking Upstream Source Code Tracker</a></h1>
<p>Not-forking tool is a code integration tool that offers unprecedented flexibility (patch/sed/diff/cp/mv) in making changes to codebases and tracking them over new releases of source software. </p>
<p>It was developed to facilitate LumoSQL, which combines SQLite and LMDB code for all available versions of SQLite and LMDB. </p>
<p>Not-Forking is released as a separate software tool and it's available for use in other proects. Documentation on Not-Forking is maintained separately. Read <a href="https://lumosql.org/src/not-forking/doc/trunk/README.md">about Not-Forking</a> and <a href="https://lumosql.org/src/not-forking/doc/trunk/doc/not-forking.md">full documentation</a> on using the tool.</p>
<div style="break-before: page; page-break-before: always;"></div><!-- Copyright 2020 The LumoSQL Authors, see LICENSES/MIT -->
<!-- SPDX-License-Identifier: MIT -->
<!-- SPDX-FileCopyrightText: 2022 The LumoSQL Authors -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Claudio Calvelli, December 2020 -->
<h1 id="lumosql-build-and-benchmark-system"><a class="header" href="#lumosql-build-and-benchmark-system">LumoSQL Build and Benchmark System</a></h1>
<p><a href="https://lumosql.org">LumoSQL</a> implements a meta-build system for SQLite, using
the <a href="https://lumosql.org/src/not-forking">Not-Forking tool</a> to handle many of
the complexities so we can build a matrix of combined codebases and versions
without having a mess of code specific to particular source trees.</p>
<p>But once a binary is built, how can we know if our changes to SQLite make a difference, and
what kind of difference? There was no standard way to compare software that
implements the SQLite APIs, so we designed one.</p>
<p>The LumoSQL Build and Benchmark System is relevant to all SQLite users wishing
to compare different configurations and versions of standard SQLite.  In
addition, the LumoSQL project includes code to combine any version of SQLite
with any version of third party storeage backends, and to run tests on the
combined code. This requires small modifications to SQLite itself, and some new
code to interface each backend.</p>
<h1 id="questions-the-build-and-benchmark-system-answers"><a class="header" href="#questions-the-build-and-benchmark-system-answers">Questions The Build and Benchmark System Answers</a></h1>
<p>A single command can now give universal, repeatable, definitive answers to the
following seemingly-simple questions:</p>
<ul>
<li>How can benchmarking runs be shared in a consistent manner between all users?
<em><strong>(hint: use a standardised SQLite database)</strong></em></li>
<li>Does SQLite get faster with each version? <em><strong>(hint: not always)</strong></em></li>
<li>Which compile options make a given version of SQLite faster?</li>
<li>How do different versions and compile options combine to change performance as
data size gets larger?</li>
<li>Does SQLITE_DEBUG really make
<a href="https://sqlite.org/compile.html">SQLite run approximately three times slower?</a></li>
<li>What happens when a given set of compile options, versions and data size are
tested on faster and slower disks?</li>
<li>Do I need to run hundreds of combinations to make decisions about SQLite versions/options/hardware?
<em><strong>(hint: no, because you now can compare benchmarking results databases)</strong></em></li>
</ul>
<p>Having addressed the above questions, the following seemingly more-difficult questions
now become very similar to the previous ones:</p>
<ul>
<li>What happens to performance when LMDB is swapped in as a storage backend for SQLite?
<em><strong>(hint: there is a strange performance curve with increasing LMDB versions)</strong></em></li>
<li>How does the Oracle-funded BDB backend compare with other backends, including the
SQLite Btree? <em><strong>(hint: Oracle seems to have thought longer runtimes are better :-)</strong></em></li>
<li>How do all of the above compare with each other with different build options,
versions and datasizes? <em><strong>(hint: now can share benchmarking results, we can take
advantage of thousands of CPU-hours from other people)</strong></em></li>
</ul>
<p>The rest of this document introduces and defines the benchmarking tool that 
makes answering these questions possible. </p>
<h2 id="build-and-benchmark-problem-statement"><a class="header" href="#build-and-benchmark-problem-statement">Build and benchmark problem statement</a></h2>
<p>Motivation: LumoSQL has established that there is currently no way of comparing
like-for-like SQLite-related databases. </p>
<p>Test matrix: LumoSQL consists of multiple source trees from multiple sources,
assembled with the assistance of the not-forking tool. These trees represent a
matrix with a very large number of dimensions.  The dimensions include among
other things: the combination of these source trees; their build process; their
invocation parameters; their input data; and the running environment.</p>
<p>Example instances of these dimensions are:</p>
<ul>
<li>SQLite version 1 combined with LMDB version 2, to make combined source object A</li>
<li>Combined source object A can be compiled with <code>-DSQLITE_DEBUG</code>, and also 
<code>-D MDB_MAXKEYSIZE</code> (which only applies to LMDB). That will give two build
objects to test, call them binary objects B and C.</li>
<li>Each of binary objects B and C can be tested with large data files, and
many small files. </li>
<li>Each of the above tests can be run in an environment with large amounts of
memory, or with deliberate memory constraints.</li>
<li>All of the above can then be repeated only with different versions of SQLite
and LMDB</li>
<li>... and then we move on to the different versions of pure SQLite, and SQLite
combined with Berkeley DB, etc.</li>
</ul>
<p><em><strong>Problem statement:</strong></em></p>
<blockquote>
<p>The LumoSQL Build and Benchmark system solves the problem of defining the
dimensions of the test matrix in a formal machine-friendly manner, and 
presenting them to the user in a human-friendly manner.
The user can then select some or all of these dimensions by human-readable
name, and then cause them to be actioned. Every selection by the user will have
multiple dependency actions.</p>
</blockquote>
<p>To ensure repeatability of tests, each test will include the following information:</p>
<ul>
<li>the version of the &quot;not-forking&quot; configuration used</li>
<li>the version of sqlite3 used (in one special case building third-party backend code
which provides its own patched version of sqlite3, this will be empty and the
backend name and version will contain information about the third-party code)</li>
<li>the name of the storage backend used: this is omitted if the test used an
unchanged version of sqlite3 with its own backend</li>
<li>the version of the storage backend used, also omitted for tests using
an unchanged version of sqlite3</li>
<li>any other options (currently only <code>datasize-N</code> to multiply the data size used
in some benchmarks by <code>N</code>)</li>
</ul>
<p>Where the user has requested average results, the tests may be run several times.</p>
<h1 id="build-and-benchmark-options"><a class="header" href="#build-and-benchmark-options">Build and benchmark options</a></h1>
<h2 id="commandline-parameters"><a class="header" href="#commandline-parameters">Commandline parameters</a></h2>
<p>(this section is included verbatim comments at the top of <code>build.tcl</code>, which is the master
version. It helps to have it here for context for the rest of the documentation.)</p>
<p>Executive summary:</p>
<pre><code>	# build.tcl OPERATION NOTFORK_CONFIG ARGUMENTS

	# OPERATION: options
	# ARGUMENTS: [OUTPUT_FILE]
	#            create a Makefile fragment so that &quot;make&quot; can accept
	#            command-line options corresponding to build options

	# OPERATION: database
	# ARGUMENTS: BUILD_DIR DATABASE_NAME
	#            create database

	# OPERATION: what
	# ARGUMENTS: BUILD_OPTIONS
	#            show what targets/options have been selected based on command-line

	# OPERATION: targets
	# ARGUMENTS: BUILD_OPTIONS
	#            same as &quot;what&quot;, but the list of targets are all in one line,
	#            for easier copy-paste when trying to run the exact same list
	#            in multiple places

	# OPERATION: build
	# ARGUMENTS: BUILD_DIR BUILD_OPTIONS
	#            build LumoSQL, if necessary

	# OPERATION: cleanup
	# ARGUMENTS: BUILD_DIR BUILD_OPTIONS
	#            check cached builds, and deletes anything which is no longer
	#            up-to-date (would be rebuilt anyway)

	# OPERATION: benchmark
	# ARGUMENTS: BUILD_DIR DATABASE_NAME BUILD_OPTIONS
	#            run benchmarks (run all tests marked for benchmarking and save timings)

	# OPERATION: test
	# ARGUMENTS: BUILD_DIR DATABASE_NAME BUILD_OPTIONS
	#            run tests (run all tests without saving timings)
</code></pre>
<h2 id="build-and-benchmark-configuration"><a class="header" href="#build-and-benchmark-configuration">Build and Benchmark configuration</a></h2>
<p>A special subdirectory <code>benchmark</code> in <code>not-fork.d/NAME</code> contain files to
control the build and benchmark process for backend <code>NAME</code> (<code>NAME</code> can be
<code>sqlite3</code> to control the process for all backends and/or for the unmodified
sqlite3). There must be at least one of the following files in each of those
directories:</p>
<ul>
<li>
<p><code>versions</code> - contains a list of versions of <code>NAME</code> to build/benchmark by
default, one version per line; if <code>NAME</code> is a backend (as opposed to <code>sqlite3</code>)
the version line can have two special formats: <code>=VERSION</code> specified which
version of sqlite3 to use with this backend; and <code>SQLITE_VERSION+BACKEND_VERSION</code>
specifies the two versions explicitely for a particular build; a line containing
just a version number will use the default specified with <code>=VERSION</code>; the
file <code>not-fork.d/lmdb/benchmarking/versions</code> contains some examples.
The special version <code>latest</code> corresponds to the latest version number
known to the not-forking tool (usually, the latest available).</p>
</li>
<li>
<p><code>standalone</code> - for backends which contain their own sqlite3 sources,
possibly modified, this file specifies to build/benchmark these rather
than build the backend and then link it to an &quot;official&quot; sqlite3.
Each line contains two version numbers, separated by space: the version
of the backend itself, and the version of sqlite3 that it includes these
are expected to include their own sqlite3, and they are built using that;
the file <code>not-fork.d/bdb/benchmarking/standalone</code> shows how to use this
for the BDB backend.</p>
</li>
</ul>
<p>The remaining files in these directories specify build/benchmark options and
code to run to produce benchmarks; the code will be documented in another
section.</p>
<p>File names matching the pattern <code>*.option</code> specify options which are
relevant to building and/or benchmarking <code>NAME</code>. Each file corresponds
to a single option (the file name with the <code>.option</code> suffix removed must
be the same as the option name). Each file contains lines of the form
<code>key=value</code> (or <code>key</code> only) with the following keys defined at present:</p>
<ul>
<li><code>build</code> - does the option affects the build process? (value <code>yes</code> or <code>no</code>,
default <code>no</code>)</li>
<li><code>default</code> - default value for the option, if not specified</li>
<li><code>equiv</code> - a list of values (separated by a comma) which are considered
equivalent; the first value is the one used to form internal target strings</li>
<li><code>requiv</code> - a regular expression and a string, separated by space; if
a value matches the regular expression, it will be treated as though it
were the replacement string instead</li>
<li><code>syntax</code> - regular expression defining the valid syntax for the option;
default is to accept any string of alphanumeric characters; note that the
expression is anchored, i.e. it must match the whole string</li>
<li><code>numeric</code> - abbreviation for <code>syntax=0|-?[1-9]\d*</code>, accept only (integer) numeric
values (without leading zeros); this key has no value</li>
<li><code>positive</code> - abbreviation for <code>syntax=[1-9]\d*</code>, accept only (integer) positive
numeric values (without leading zeros); this key has no value</li>
<li><code>boolean</code> - abbreviation for <code>syntax=on|off|true|false</code>, <code>equiv=on,true</code>
and <code>equiv=off,false</code>; this key has no value</li>
<li><code>enum</code> - followed by a comma-separated list of values, abbreviation for
<code>syntax=value1|value2|...|valuen</code> i.e. accept only values from the list</li>
</ul>
<p>For example, <code>not-fork.d/sqlite3/options/datasize.option</code> contains information
about the <code>datasize</code> benchmark option:</p>
<pre><code>build = no
syntax = [1-9]\d*(?:,[1-9]\d*)?
default = 1
requiv = (\d+),\1 \1

</code></pre>
<p>this means that the values are one or two positive numbers, and two identical
values are equivalent to just a single one (e.g. 2,2 is the same as 2) for
compatibility with previous versions of LumoSQL where the value was just
a single positive integer.</p>
<p>Options which affect the build must be known to the <code>lumo.build</code> script and/or
to the <code>lumo.mk</code> Makefile fragment to be effective; these files are installed
by the not-forking configuration and control the build process.</p>
<p>Options which affect the benchmark must be implemented by one or more of the
tests actually ran, for example by changing data sizes or using <code>PRAGMA</code>
statements; the <code>tool/build.tcl</code> tries to know as little as possible about
what is being done, to help using the framework for other systems.</p>
<p>Options which apply to all backends are usually found in
<code>not-fork.d/sqlite3/benchmark</code>; options which apply to a single backend
will be found in the corresponding directory <code>not-fork.d/BACKEND/benchmark</code>.
no matter which backend they use. Options which affect the build could be
in any directory; currently there is no mechanism to address the case of the
same option is present in multiple directories, and it is undefined which
one will take precedence.</p>
<h2 id="backends-as-of-lumosql-04"><a class="header" href="#backends-as-of-lumosql-04">Backends as of LumoSQL 0.4</a></h2>
<p>At present the only backend provided (in addition to sqlite's own <code>btree</code>)
is the <code>lmdb</code> backend; this was originally
derived from the sqlightning sources but has been rewritten to work with more
recent versions of lmdb and sqlite3;
however to add new backends see <a href="lumo-build-benchmark.html#adding-backends">Adding new backends</a> below.</p>
<p>A third backend, based on Oracle's Berkeley DB is in progress; a special target
of <code>+bdb-VERSION</code> (without a sqlite3 version) indicates to build the code provided
directy by Oracle, without using the LumoSQL build mechanism.</p>
<h2 id="specifying-buildbenchmark-options-to-make"><a class="header" href="#specifying-buildbenchmark-options-to-make">Specifying build/benchmark options to &quot;make&quot;</a></h2>
<p>The Makefile has a mechanism to recognise build/benchmark options as command-line
option with the form <code>OPTION=value</code> where <code>OPTION</code> is the name of an option
translated to be in all capital letters; the name can also be prefixes with
the string <code>OPTION_</code> in case there is a name clash with other existing Makefile
options: for example, if <code>datasize</code> is defined as the above example, the following
two commands are equivalent and will set this option to the value 2:</p>
<pre><code>make benchmark DATASIZE=2
make benchmark OPTION_DATASIZE=2
</code></pre>
<p>Options which affect the build may cause a rebuild of the objects; options which
only change the benchmark parameters can reuse an older build if available.</p>
<p>These options are in addition to the existing Makefile mechanism to generate
a list of targets, using the (previously documented) variables <code>USE_backend</code>,
<code>backend_VERSIONS</code>, etc:</p>
<ul>
<li><code>SQLITE_VERSIONS=list</code> - build and benchmark the specified versions instead
of the default; the first version in the list will also be used to update
the benchmark result database (if a backend is built, the corresponding
unmodified version of sqlite is also added to this list, so the benchmark
can compare them); see below for the exact syntax of the list</li>
<li><code>USE_SQLITE=yes</code> - build and benchmark an unmodified sqlite3: this is the
default</li>
<li><code>USE_SQLITE=no</code> - do not build/benchmark an unmodified sqlite3; however the
version which will be used to store the benchmark results in a database will
always be built if necessary</li>
<li><code>USE_backend=yes</code> - include <code>backend</code> in the build/benchmark; this is the default</li>
<li><code>USE_backend=no</code> - do not include <code>backend</code> in the build/benchmark</li>
<li><code>SQLITE_FOR_backend=list</code> - versions of sqlite3 to use when building a
backend if the backend version does not specify one; see below for the
exact syntax of the list</li>
<li><code>backend_VERSIONS=list</code> - replace the default list of versions to build; each
element of the list can be a version number as for other lists, and each element
of the resulting list will be combined with all versions specified by
<code>SQLITE_FOR_backend</code>; however a special element containing two version numbers
separated by a &quot;+&quot; is handled by expanding both versions separately; experimenting
with <code>make what</code> is probably the best way to figure out how this works for
anything but the simplest case (e.g. <code>latest+latest</code> will combine the latest
version of sqlite with the latest version of the backend)</li>
<li><code>backend_STANDALONE=list</code> - if a backend includes its own version of sqlite3,
then build that instead of linking against an official one; the list of versions
can be specified in the same way as other lists, to produce simple version
numbers; additionally an element can specify a single version of the backend
and a single version of sqlite3, in this order and separated by <code>=</code>: this
documents which version of sqlite3 is included in that version of the backend,
and will result in the unmodified sqlite3 being added to the benchmark for
comparison: the benchmark system will make no other use of this sqlite version
number, as the backend is expected to do what is necessary to build with it</li>
<li><code>DATABASE_NAME=filename</code> - where to store benchmark results, default is
<code>benchmarks.sqlite</code></li>
<li><code>EXTRA_BUILDS=targets</code> - makes sure that the program also builds the
targets specified; the targets are specified using the same syntax as
<code>TARGETS</code> described below, with run-time options ignored (build-time
options will be respected).  This is mainly used by some compatibility
tests to make sure a version with some special options is also built.</li>
<li><code>BENCHMARK_RUNS=number</code> - how many times to repeat each benchmark, default 1.</li>
</ul>
<p>Options which take a list of versions expect a space-separated list (this
will need to be quoted from the shell); each element can be one of the
following:</p>
<ul>
<li>version number (e.g. <code>3.37.2</code>): this number will be added as-is</li>
<li><code>all</code>: all known versions which aren't already in the list will be added</li>
<li><code>latest</code>: the latest known version</li>
<li><code>-version</code> (e.g. <code>-3.35.0</code>): remove this from the list if it's there</li>
<li><code>-latest</code>: remove the latest version from the list if it's there</li>
<li><code>version+</code> (e.g. <code>3.34.0+</code>): the specified version and all the ones
which follow it</li>
<li><code>version-</code> (e.g. <code>3.17.0-</code>): the specified version and all the ones
which precede it</li>
<li>commit-<code>id</code> (e.g. <code>commit-9c4e21abdca664d6b7bcf0043fe9ec05ef8b2949</code>):
the specified commit ID according to the version control system, which
does not need to be a formal release</li>
</ul>
<p>For example, the following sqlite version list:</p>
<pre><code>all -3.35.0- -latest
</code></pre>
<p>corresponds, at the time of writing to the list:</p>
<pre><code>3.35.1 3.35.2 3.35.3 3.35.4 3.35.5 3.36.0 3.37.0 3.37.1 3.37.2 3.38.0
</code></pre>
<p>that is, all versions except the ones until 3.35.0 included, and also
excluding the latest (3.38.1 at the time of writing); this could also
be specified equivalently as:</p>
<pre><code>3.35.1+ -latest
</code></pre>
<p>Instead of specifying <code>USE_backend=yes/no</code> and various lists of versions,
it's possible to specify an explicit list of targets to build or benchmark;
this list can be used, for example, to run the same set at different times,
when <code>all</code> and <code>latest</code> may have different meanings. This is done by using
the option <code>TARGETS</code> and is explained in the next section.</p>
<p>Some options are provided to control the use of the not-forking tool:</p>
<ul>
<li><code>NOTFORK_COMMAND=path</code> (default: look for <code>not-fork</code> in <code>$PATH</code>): the
name of the not-forking tool</li>
<li><code>NOTFORK_UPDATE=number</code> (default: 0): if nonzero, it will pass <code>--update</code>
the first time the not-forking tool is called with a particular repository;
this could be necessary if the defaults have been set to <code>--no-update</code>
and the cached copy of the repository is older than the version required.</li>
<li><code>NOTFORK_ONLINE=number</code> (default: 0): if nonzero, it will pass <code>--online</code>
to the not-forking tool; this could be necessary if the defaults have been
set to <code>--offline</code> and the operation cannot be completed with cached data.</li>
<li><code>CACHE_DIR=path</code> (default: <code>$HOME/.cache/LumoSQL/not-fork</code>), the directory
where not-forking will cache its downloads</li>
</ul>
<p>To help debugging, some options provide a mechanism to copy intermediate
files, as well as the SQL statement used:</p>
<ul>
<li><code>COPY_DATABASES=path</code> (default: empty, meaning the option is disabled).
The <code>path</code> must contain a <code>%s</code> which will be replaced with the target name,
and a <code>%d</code> which will be replaced with the test number, for example
<code>COPY_DATABASES=/tmp/testdbs/%s.%d</code>.  The database at the beginning of each
test will be copied to the resulting path, so the same test can be repeated
by calling the program on the copy of the database.</li>
<li><code>COPY_SQL=path</code> (defaul: empty, meaning the option is disabled). The
<code>path</code> must contain a <code>%s</code> and a <code>%d</code> like <code>COPY_DATABASES</code>: the complete
list of SQL statements executed by a test will be written to the file,
so it's possible to re-run them on the copy of the database.</li>
</ul>
<p>The <code>make</code> target <code>test</code> is similar to <code>benchmark</code>, however it produces
output in a different database (by default <code>tests.sqlite</code>) and can run some
extra tests which are not useful as benchmarks; also, some code which
helps produce precise timing is skipped in favour of speed of execution:
the aim here is to check that a backend works, not how long it takes.
The name of the <code>tests.sqlite</code> database can be changed using the option
<code>TEST_DATABASE_NAME=newname</code>.</p>
<h2 id="encoding-options-in-the-target-name"><a class="header" href="#encoding-options-in-the-target-name">Encoding options in the target name</a></h2>
<p>The target name is used internally by the benchmark system to determine if two
benchmarks are for similar things and can be compared; in general, two benchmarks
are comparable if they have the same build and benchmark options; to simplify
this decision, the options are encoded in the target name using the syntax:
<code>sqlite3version+[backendname-backendversion]+option-value[+option-value]...</code>
the options are always listed in lexycographic order, and default options are
omitted, so that if two string differ then the options differ.  This is an
internal representation, however it appears in the &quot;target&quot; field of the benchmark
database, in the output of <code>make what</code> and <code>make targets</code>, and can be specified
directly to make to repeat just a particular benchmark without specifying all
the options separately.</p>
<p>The syntax is:</p>
<pre><code>make build TARGETS='target1 target2 ...'
make benchmark TARGETS='target1 target2 ...'
make test TARGETS='target1 target2 ...'
</code></pre>
<p>As mentioned, the list of targets can be obtained in several ways; possibly
the easiest is <code>make targets</code> which will provide a single line for easy
copy and paste, for example:</p>
<pre><code>$ make targets USE_BDB=no USE_SQLITE=no LMDB_VERSIONS=0.9.28+ SQLITE_FOR_LMDB=3.37.1+
BENCHMARK_RUNS=1
COPY_DATABASES=
COPY_SQL=
CPU_COMMENT=
DB_DIR=
DISK_COMMENT=
MAKE_COMMAND=make
NOTFORK_COMMAND=not-fork
NOTFORK_ONLINE=0
NOTFORK_UPDATE=0
SQLITE_VERSIONS=latest 3.36.0
USE_SQLITE=no
USE_BDB=no
SQLITE_FOR_BDB=
BDB_VERSIONS=
BDB_STANDALONE=18.1.32=3.18.2
USE_LMDB=yes
SQLITE_FOR_LMDB=3.37.1+
LMDB_VERSIONS=0.9.28+
LMDB_STANDALONE=
OPTION_DATASIZE=1
OPTION_DEBUG=off
OPTION_LMDB_DEBUG=off
OPTION_LMDB_FIXED_ROWID=off
OPTION_LMDB_TRANSACTION=optimistic
OPTION_ROWSUM=off
OPTION_ROWSUM_ALGORITHM=sha3_256
OPTION_SQLITE3_JOURNAL=default
BUILDS=
    3.37.2 3.37.1 3.37.1+lmdb-0.9.28 3.37.2+lmdb-0.9.28 3.37.1+lmdb-0.9.29 3.37.2+lmdb-0.9.29
TARGETS=
    3.37.1 3.37.1+lmdb-0.9.28 3.37.2 3.37.2+lmdb-0.9.28 3.37.1+lmdb-0.9.29 3.37.2+lmdb-0.9.29
</code></pre>
<p>so to run exactly the same benchmark one can say:</p>
<pre><code>make benchmark TARGETS='3.37.1 3.37.1+lmdb-0.9.28 3.37.2 3.37.2+lmdb-0.9.28 3.37.1+lmdb-0.9.29 3.37.2+lmdb-0.9.29'
</code></pre>
<p>A subset of the normal syntax for lists of versions is recognised, with the
&quot;+&quot; and spaces escaped with a backslash, so for example one could run benchmarks
for all sqlite versions since 3.35.0 combined with all LMDB versions since
0.9.25, enabling LMDB debugging with:</p>
<pre><code>make benchmark TARGETS='3.35.0\++lmdb-0.9.25\++lmdb_debug-on'
</code></pre>
<p>The list of benchmarks generated by this syntax obviously depends on what
the current latest version is, however it can be converted to a fixed list
with:</p>
<pre><code>make targets TARGETS='3.35.0\++lmdb-0.9.25\++lmdb_debug-on'
</code></pre>
<h2 id="specifying-build-options-to-the-build-and-benchmark-tools"><a class="header" href="#specifying-build-options-to-the-build-and-benchmark-tools">Specifying build options to the build and benchmark tools</a></h2>
<p>The various tools provided by previous versions of LumoSQL have been merged
into a single tool, <code>tool/build.tcl</code>, which guarantees identical parsing of
configuration and options in all stages of the process; the Makefile arranges
to call this tool as appropriate, but it can be called manually using the
syntax:</p>
<pre><code>tclsh tool/build.tcl OPERATION NOTFORK_CONFIG ARGUMENTS
</code></pre>
<p>The <code>NOTFORK_CONFIG</code> is usually the not-fork.d directory provided with
LumoSQL; the <code>OPERATION</code> specifies what to do, and the <code>ARGUMENTS</code>
depend on the operation specified; the following <code>OPERATIONs</code> are
defined:</p>
<ul>
<li>
<p><code>options</code> - creates a Makefile fragment to instruct <code>make</code> to accept
the command-line options described elsewhere in this document; this
is normally generated automatically by <code>make</code> the first time it's needed
but may need to be regenerated if the configuration changes in a way
that <code>make</code> does not notice; <code>ARGUMENTS</code> contains just the name
of the file to write.</p>
</li>
<li>
<p><code>build</code> - builds all necessary binaries so that a benchmark can run;
the <code>ARGUMENTS</code> are the destination directory for the build followed
by build options of the form <code>-OPTION=VALUE</code> to set options specified
by an <code>*.options</code> file, or <code>OPTION=VALUE</code> for other options (such as
<code>USE_backend</code> and <code>TARGETS</code>); if the <code>VALUE</code> contains spaces or other
characters which may be special to the shell it will need to be quoted.</p>
</li>
<li>
<p><code>database</code> - creates a database to store benchmark results; this will
also be done automatically if required before running benchmarks;
<code>ARGUMENTS</code> contains just the database file name.</p>
</li>
<li>
<p><code>benchmark</code> - runs all benchmarks taking into account applicable options;
the <code>ARGUMENTS</code> are the destination directory for the build followed
by the name of the database where to store the results, followed
by build and runtime options in the same form as the <code>build</code> operation.</p>
</li>
<li>
<p><code>test</code> - runs all tests (a superset of the benchmarks) taking into
account applicable options; the <code>ARGUMENTS</code> are the same as for <code>benchmark</code>.</p>
</li>
<li>
<p><code>what</code> - outputs a description of what it would be built and benchmarked
as well as the values of any options; <code>ARGUMENTS</code> is just the build options,
like <code>build</code>, but without the destination directory; the output will show
the effect of applying these options without building or running anything.</p>
</li>
<li>
<p><code>targets</code> - similar to <code>what</code>, however the list of targets is all in
one line for easier copy and paste.</p>
</li>
</ul>
<p>Note that apart from the slightly different syntax, build/benchmark/test options
are specified in the same way as standard <code>Makefile</code> arguments.</p>
<p>For example, to build two versions of plain sqlite3, two versions of sqlite3+LMDB
and one version of BDB with its own sqlite3:</p>
<pre><code>tclsh tool/build.tcl build not-fork.d /tmp/objects \
      SQLITE_VERSIONS='3.14.15 3.33.0' \
      USE_LMDB=yes LMDB_VERSIONS='0.9.9 0.9.27' SQLITE_FOR_LMDB=3.8.0 \
      USE_BDB=yes BDB_STANDALONE='18.1.32'
</code></pre>
<p>To do the same build as above but specifying the target strings directly:</p>
<pre><code>tclsh tool/build.tcl build not-fork.d /tmp/objects \
      TARGETS='3.14.15 3.33.0 3.8.0+lmdb-0.9.9 3.8.0+lmdb-0.9.27 +bdb-18.1.32'
</code></pre>
<p>To add option <code>debug=on</code> to the build:</p>
<pre><code>tclsh tool/build.tcl build not-fork.d /tmp/objects myresults.sqlite \
      SQLITE_VERSIONS='3.14.15 3.33.0' \
      USE_LMDB=yes LMDB_VERSIONS='0.9.9 0.9.27' SQLITE_FOR_LMDB=3.8.0 \
      USE_BDB=yes BDB_STANDALONE='18.1.32' \
      -DEBUG=on
</code></pre>
<p>or, with an explicit list of targets:</p>
<pre><code>tclsh tool/build.tcl build not-fork.d /tmp/objects \
      TARGETS='3.14.15++debug-on 3.33.0++debug-on \
      3.8.0+lmdb-0.9.9+debug-on 3.8.0+lmdb-0.9.27+debug-on \
      +bdb-18.1.32+debug-on'
</code></pre>
<p>To run the benchmarks rather just building the targets, replace <code>build</code> with
<code>benchmark</code>, and add the name of the output database, for example:</p>
<pre><code>tclsh tool/build.tcl benchmark not-fork.d /tmp/objects myresults.sqlite \
      TARGETS='3.14.15++debug-on 3.33.0++debug-on \
      3.8.0+lmdb-0.9.9+debug-on 3.8.0+lmdb-0.9.27+debug-on \
      +bdb-18.1.32+debug-on'
</code></pre>
<p>The first version of sqlite3 provided (in this case 3.14.15) will be used to
update the benchmark results database.</p>
<h1 id="what-tests-will-run"><a class="header" href="#what-tests-will-run">What tests will run</a></h1>
<p>Each test is composed of three lists of SQL statements, the &quot;before&quot; list
prepares the environment for the test, then the test itself runs and the
time it takes is logged, finally the &quot;after&quot; list can do any necessary
cleanup.  Two special files in <code>not-fork.d/sqlite3/benchmark</code> can provide
common &quot;before&quot; and &quot;after&quot; code which will be included in every test;
these files must have names <code>before-test</code> and <code>after-test</code> respectively.</p>
<p>A backend can add some extra statements to these lists: the special file
<code>not-fork.d/BACKEND/benchmark/before</code>, if present, runs just after the
one in the <code>sqlite3</code> directory; and similarly the special file
<code>not-fork.d/BACKEND/benchmark/after</code>, if present, runs just before the
one in the <code>sqlite3</code> directory: the idea is that the backend's &quot;before&quot;
file executes some extra initialisation after the generic one, and
the backend's &quot;after&quot; file does some extra cleanup before the generic one.</p>
<p>Files matching the pattern <code>*.test</code> in directory <code>not-fork.d/sqlite3/benchmark</code>
contain the individual tests: the benchmark will read these files in lexycographic
order to decide which tests to run and in which order; for each test, the
contents of <code>before-test</code>, the test itself, and <code>after-test</code> are concatenated
and the result interpreted as TCL code; it is expected that this TCL code
sets the variable <code>name</code> to contain the name of the text, and also appends
SQL statements to three variables: <code>before_sql</code>, <code>sql</code> and <code>after_sql</code>:
these SQL statements will then be executed in the order listed, but only
the middle (<code>sql</code>) one is timed, so that setup and cleanup code does not
count towards the benchmarking.</p>
<p>If a backend defined a file with the same name as one in the directory
<code>not-fork.d/sqlite3/benchmark</code>, that file will be executed immediately
after the generic one and can modify the list of statement as appropriate;
for example in the current distribution the first test to run,
<code>not-fork.d/sqlite3/benchmark/0000.test</code>, creates a database; the LMDB
backend has <code>not-fork.d/lmdb/benchmark/0000.test</code> which adds a
<code>PRAGMA</code> to specify some backend-specific runtime options to the database.</p>
<p>This TCL code can access a number of variables from the <code>build.tcl</code> script,
in particular the array <code>options</code> contains the build and benchmark options;
test; each file is a fragment of TCL expected to set two variables: <code>name</code>
which is the name of the test, and <code>sql</code> which is the SQL to be executed;
the fragment can access the array <code>options</code> to determine the build and
benchmark options; examples are provided in the LumoSQL configuration to
specify the default set of tests, we show here an example from one of the tests:</p>
<pre><code>set d25000 [expr $options(DATASIZE) * 25000]
set name &quot;$d25000 INSERTs in a transaction&quot;

append sql &quot;BEGIN;\n&quot;
append sql &quot;CREATE TABLE t2(a INTEGER, b INTEGER, c VARCHAR(100));\n&quot;
for {set i 1} {$i&lt;=$d25000} {incr i} {
  set r [expr {int(rand()*500000)}]
  append sql &quot;INSERT INTO t2 VALUES($i,$r,'[number_name $r]');\n&quot;
}
append sql &quot;COMMIT;\n&quot;
</code></pre>
<p>This corresponds to the old &quot;25000 INSERTs in a transaction&quot; except that it now
multiplies the number of inserts by the <code>DATASIZE</code> option; so it first uses
<code>$options(DATASIZE)</code> to calculate the number of inserts, then sets the test
name accordinly and generates the SQL. (For simplicity of presentation, this
is an older version of the test; a recent version copes with the DATASIZE option
having two numbers, a read datasize and a write datasize; see the files actually
included in the distribution for the latest examples).</p>
<p>When running the benchmark, the program will measure just the time required to
run the appropriate version of sqlite3/backend on the sql generated by each
test.</p>
<p>The code fragment can optionally append to two more variables: <code>before_sql</code>
is executed at the start, but not included in the time measurement, and
<code>after_sql</code> is likewise executed at the end and not included in the time
measurement.</p>
<p>At present, tests must be specified in the <code>sqlite3</code> directory and not a backend
one: this is so that we run the same tests for unmodified sqlite3 as we do for
the one modified by a backend, to guarantee a meaningful comparison. If a
test appears in a backend directory, it is considered additional code to
add to the generic test, as described above.</p>
<p>Some of the test files do not produce meaningful timings, but are useful
to help checking correctness of backends: to inform the build system of
this fact, they can set variable <code>is_benchmark</code> to 0 (by default it has
value 1). These tests will then be skipped by <code>make benchmark</code> but still
included by <code>make test</code>.</p>
<h1 id="benchmark-run-comments-a-namebenchmark-commentsa"><a class="header" href="#benchmark-run-comments-a-namebenchmark-commentsa">Benchmark run comments <a name="benchmark-comments"></a></a></h1>
<p>When running benchmarks it's possible to add up to two free-form comments
which will be saved in the database but otherwise ignored by the program;
these are intended to contain information about the system, and are
specified using the command-line options <code>DISK_COMMENT</code> and
<code>CPU_COMMENT</code> with the obvious intended meaning, for example:</p>
<pre><code>make benchmark DISK_COMMENT='fast NVME' CPU_COMMENT='AMD Ryzen 3700x'
</code></pre>
<h1 id="adding-new-backends-a-nameadding-backendsa"><a class="header" href="#adding-new-backends-a-nameadding-backendsa">Adding new backends <a name="adding-backends"></a></a></h1>
<p>To add new backends, create a new directory inside <code>not-fork.d</code> (or inside the
appropriate not-forking configuration repository) with the same name as the
backend, and add information about how to obtain the sources etc. At a minimum
the directory will contain the following files:</p>
<ul>
<li><code>upstream.conf</code>: information about where to find the sources</li>
<li><code>lumo-new-files.mod</code>: a list of new files to be installed to link the
backend with sqlite3: see an existing backend for a quick example, or
read the more comprehensive documentation below</li>
<li><code>files/FILENAME</code>: every file mentioned in <code>lumo-new-files.mod</code> needs
to be provided in the <code>files/</code> directory</li>
<li>at least one of <code>benchmark/versions</code> or <code>benchmark/standalone</code>; the
former includes versions of the backend to build and link against a
&quot;standard&quot; sqlite, as well as specifying which versions of sqlite are
compatible with that; the latter specifies versions to build using an
included sqlite3; see the existing <code>versions</code> for LMDB and <code>standalone</code>
for BDB as examples</li>
</ul>
<p>The build process requires the backend to provide the following two files
(in directory <code>.lumosql</code>), which means that <code>lumo-new-files.mod</code> or some
other file in the not-forking configuration must install them:</p>
<ul>
<li><code>lumo.mk</code> is a Makefile fragment which will be inserted into the sqlite3
build process, for example to link against the backend</li>
<li><code>lumo.build</code> is a TCL script to build the backend; it has access to
various variables set by the build process; it needs to copy or move the
build result to <code>$lumo_dir/build</code></li>
</ul>
<p>The LumoSQL build system modifies sqlite3 to replace some of its own files
with a stub, which used the C preprocessor's <code>#include</code> directive to read
the original file. It also sets the include search path so that it looks
first in a subdirectory <code>.lumosql/backend</code> of the backend's sources, and
if not found there in the original sqlite3 sources. To avoid file name
collision, all such files will be prefixed with <code>lumo_</code></p>
<p>Therefore, to replace one of these sqlite3 files with a new one the backend
will need to have a line in <code>lumo-new-files.mod</code> to specify a new file with
the appropriate name in <code>.lumosql/backend</code>, and also add this file in the
<code>files</code> directory.</p>
<p>For example, to replace <code>btree.c</code> with a new one (probably something to call
the new backend using its own API rather than the original <code>btree.c</code> from
sqlite3), one would have the following:</p>
<p>File <code>lumo-new-files.mod</code>:</p>
<pre><code>method = replace
--
# files required by the LumoSQL build system
.lumosql/lumo.mk                 =  files/lumo.mk
.lumosql/lumo.build              =  files/lumo.build

# files we replace
.lumosql/backend/lumo_btree.c    =  files/btree.c
</code></pre>
<p>Then file <code>files/btree.c</code> would contain the new version, and file <code>files/lumo.mk</code>
would provide information on how to link the backend with sqlite3, for example:</p>
<pre><code>TCC += -I$(LUMO_SOURCES)/$(LUMO_BACKEND)/include
TLIBS += -L$(LUMO_BUILD)/$(LUMO_BACKEND)
TLIBS += -lmy_backend
</code></pre>
<p>would add the <code>include</code> subdirectory in the backend's sources to the search
path when building sqlite3 (probably because the replaced <code>btree.c</code> needs
to include something from there), and also add the <code>build</code> directory in the
backend's sources as library search path; finally it asks to link <code>libmy_backend.so</code>
or <code>libmy_backend.a</code> into the sqlite3 executable, probably finding it in
the build directory just added to the library search path.</p>
<p><code>files/lumo.build</code> could be something like:</p>
<pre><code>global backend_name
global backend_version
global build_options

puts &quot;Configuring $backend_name $backend_version&quot;
if {$build_options(DEBUG) eq &quot;on&quot;} {
    system ./configure --enable-debug
} else {
    system ./configure --disable-debug
}

puts &quot;Building $backend_name $backend_version&quot;
system make

# now move files of interest to lumo/build
global lumo_dir
set dest [file join $lumo_dir build]
if {! [file isdirectory $dest]} { file mkdir $dest }
file rename mybackend.h $dest
foreach fn [glob liblmybackend.*] {
    file rename $fn $dest
}
</code></pre>
<h1 id="sharing-the-buildbench-environment"><a class="header" href="#sharing-the-buildbench-environment">Sharing the Build/Bench Environment</a></h1>
<p>It is often useful to run multiple benchmarking sessions at once on a cluster.
Some but not all components of LumoSQL can be shared. The sharing status is as
follows:</p>
<ul>
<li>sharing cache directory is fine if locking works</li>
<li>sharing build directory is fine if locking works</li>
<li>sharing results directory is fine as long as each node uses an unique
file name when writing to it</li>
<li>sharing lumosql repository is fine as long as the results database has
a unique file name (and/or is moved somewhere else instead of using the
default location).</li>
<li>sharing directory where to run benchmarks is to be avoided at all costs;
in fact it is best if it is a local disk or a ramdisk, as network drives
would include random latency variation which will make the timing results
less useful</li>
</ul>
<p>So, assuming you've set up:</p>
<ul>
<li>a shared cache volume in /mnt/cache (5GB)</li>
<li>a shared results volume in /mnt/results (5GB)</li>
<li>a local, non-shared, volume to run the benchmarks in /mnt/benchmarks (5GB)</li>
<li>a shared volume for the builds in /mnt/build (25GB, possibly more depending
on how many targets will be built)</li>
<li>and maybe a shared volume containing the repository itself, for simplicity
of keeping things up to date on all nodes</li>
</ul>
<p>You can create a file Makefile.local in the repository directory (please
do not commit this file!) with:</p>
<pre><code>BUILD_DIR = /mnt/build
CACHE_DIR = /mnt/cache
DB_DIR = /mnt/benchmarks
DATABASE_NAME := /mnt/results/lumosql-$(shell hostname)-$(shell date +%Y-%m-%d).sqlite
</code></pre>
<p>Then these options will be automatically added to each run.  You may want to
change the <code>DATABASE_NAME</code> with a filename which makes sense, as long as it
is unique even when things are running at the same time.</p>
<p>It is also possible to use the <code>not-fork</code> command directly from a clone if
the fossil repository, rather than installing it; and that clone could be
in a shared volume; a simple shell script needs to call it with the right
options; if for example the repository is at <code>/mnt/repositories/not-forking</code>,
the script could contain:</p>
<pre><code>#!/bin/sh
repo=/mnt/repositories/not-forking
perl -I&quot;$repo/lib&quot; &quot;$repo/bin/not-fork&quot; &quot;$@&quot;
</code></pre>
<p>If the script is not in <code>$PATH</code>, or if it has a name other than <code>not-fork</code>,
add a line like the following to <code>Makefile.local</code>:</p>
<pre><code>NOTFORK_COMMAND = /path/to/not-fork-script
</code></pre>
<p>A specific example of a shared cluster is the <a href="../kbench/README.html">Kubernetes example files</a></p>
<div style="break-before: page; page-break-before: always;"></div><!-- SPDX-License-Identifier: CC-BY-SA-4.0 -->
<!-- SPDX-FileCopyrightText: 2020 The LumoSQL Authors -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Dan Shearer, 2019 -->
<h1 id="lumosql-backend-storage-engines"><a class="header" href="#lumosql-backend-storage-engines">LumoSQL Backend Storage Engines</a></h1>
<p>Development of <a href="https://github.com/LMDB/lmdb">LMDB</a> library by Howard Chu introduced a new way of data storage based on memory mapping, which offers new capailities and improved performance. Inspired by the 2013 prototype of <a href="https://github.com/LMDB/sqlightning">sqlightning</a>, LumoSQL dveloped tools to combine any version of SQLite with any version of LMDB. This was done to test the potential benefits of deploying LMDB as a substitute for currently very widely used SQLite b-tree. For comparison, BDB storage can also be used with SQLite version 3.18.2, see <a href="https://lumosql.org/src/lumosql/doc/tip/doc/lumo-sqlite-bdb-backend.md">discussion</a>.</p>
<p>At the moment LumoSQL supports:</p>
<ul>
<li>SQLite <a href="https://sqlite.org/src4/doc/trunk/www/bt.wiki">b-tree</a></li>
<li><a href="http://www.lmdb.tech/doc/">LMDB</a></li>
<li><a href="https://docs.oracle.com/database/bdb181/html/gsg/C/BerkeleyDB-Core-C-GSG.pdf#%5B%7B%22num%22%3A44%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C86.4%2C691.2%2Cnull%5D">BDB</a></li>
</ul>
<p>LumoSQL team is also considering the implementation of novel <a href="https://lumosql.org/src/lumosql/doc/tip/doc/lumo-malbrain-backend.md">C Btree</a> (created by Karl Malbrain).</p>
<div style="break-before: page; page-break-before: always;"></div><!-- Copyright 2020 The LumoSQL Authors, see LICENSES/MIT -->
<!-- SPDX-License-Identifier: MIT -->
<!-- SPDX-FileCopyrightText: 2020 The LumoSQL Authors -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Dan Shearer, September 2020 -->
<h1 id="notes-on-the-port-of-sqlite-to-bdb-18132"><a class="header" href="#notes-on-the-port-of-sqlite-to-bdb-18132">Notes on the Port of SQLite to BDB 18.1.32</a></h1>
<h1 id="table-of-contents-2"><a class="header" href="#table-of-contents-2">Table of Contents</a></h1>
<ul>
<li><a href="lumo-sqlite-bdb-backend.html#notes-on-the-port-of-sqlite-to-bdb-18132">Notes on the Port of SQLite to BDB 18.1.32</a></li>
<li><a href="lumo-sqlite-bdb-backend.html#motivation">Motivation</a></li>
<li><a href="lumo-sqlite-bdb-backend.html#versions-and-availability">Versions and Availability</a></li>
<li><a href="lumo-sqlite-bdb-backend.html#licensing-gotchas">Licensing Gotchas</a></li>
<li><a href="lumo-sqlite-bdb-backend.html#running-out-of-the-box">Running Out of the Box</a></li>
<li><a href="lumo-sqlite-bdb-backend.html#design-of-the-port">Design of the Port</a>
<ul>
<li><a href="lumo-sqlite-bdb-backend.html#files-and-directories">Files and Directories</a></li>
<li><a href="lumo-sqlite-bdb-backend.html#code-changes">Code Changes</a></li>
</ul>
</li>
</ul>
<h1 id="motivation"><a class="header" href="#motivation">Motivation</a></h1>
<p>The work described in this document is part of an ongoing project to test,
document and consolidate source code for numerous back-end storage systems that
have been ported to SQLite in various ways. There are 4 key-value stores used
at scale and widely-ported that have the property of being
<a href="https://en.wikipedia.org/wiki/Multiversion_concurrency_control">MVCC</a>. BDB is
one of them.</p>
<p>The original port of SQLite to BDB was funded by Oracle, who have ceased development
on it as of July 2020. LumoSQL plans to continue this, using the 
<a href="./lumo-not-forking.html">Not-Forking Upstream Source Code Tracker</a> . Once the BDB non-fork 
configuration is created and tested, and once the first forward-port of BDB 18.1.32 
has been completed, then:</p>
<ol>
<li>This document will be of historical interest only, and</li>
<li>LumoSQL will be where updated versions of SQLite+BDB can be found</li>
</ol>
<p>At that point, it will be possible to compare the performance and other
attributes of the BDB KV store to SQLite's default Btree store and to the LMDB
backend.</p>
<h1 id="versions-and-availability"><a class="header" href="#versions-and-availability">Versions and Availability</a></h1>
<p>BDB is freely available under the GNU AGPL license. It was formerly Sleepycat
BDB, before Sleepycat was purchased by Oracle. </p>
<p>BDB version 18.1.32 was the last to have a port of SQLite to BDB included. From
the 
<a href="https://download.oracle.com/otndocs/products/berkeleydb/html/changelog_18_1_40.html">release notes for BDB 18.1.40</a>:</p>
<blockquote>
<p><strong>Changes between version 18.1.32 and version 18.1.40</strong> [..]
The SQL API is no longer supported. 
If you require SQL support you must use Berkeley DB 18.1.32 or earlier. </p>
</blockquote>
<p>BDB 18.1.32 contains scripts that patch SQLite 3.18.2 to have BDB support. The scripts are 
intended to assist with forward-porting to later versions of SQLite.</p>
<p>A convenient place to get the source for BDB 18.1.32 is
<a href="https://lumosql.org/dist">the LumoSQL mirror</a>
.  It is also available at an Oracle site that requires a login. </p>
<p>This document will refer to the port as BDB-SQLite, because SQLite was modified
to fit BDB, not the other way around. This is the how nearly all of the many
projects that provide a different storage backend for SQLite have approached
the task.</p>
<h1 id="licensing-gotchas"><a class="header" href="#licensing-gotchas">Licensing Gotchas</a></h1>
<p>Oracle applied the AGPL to the BDB code so that it could no longer be used as
an embedded library without all code linked to it also becoming subject to the
AGPL. That is the viral nature of a 
<a href="https://en.wikipedia.org/wiki/Viral_license">Copyleft License</a>. The 
<a href="https://en.wikipedia.org/wiki/GNU_Lesser_General_Public_License">GNU Lesser GPL</a> would have 
avoided this, or indeed the original BDB license. Nevertheless, BDB is truly open source under GNU licensing without any tricks; it is just unhelpful for a library to be licensed as if it were an application.</p>
<p>This definitely means:</p>
<ul>
<li>All code linked to SQLite with a BDB backend is subject to the AGPL</li>
<li>Source code present in the same tree as the BDB backend is <em>not</em> subject to the AGPL, so there is nothing to worry about distributing BDB source code</li>
<li>LumoSQL contributions to BDB code are covered by the AGPL, as well as the LumoSQL license.</li>
</ul>
<p>Without offering legal advice in any way, this probably means:</p>
<ul>
<li>Users who have paid Oracle for a commercial licence to BDB code are probably exempt from the AGPL requirement, but do check with your open source lawyer</li>
</ul>
<h1 id="running-out-of-the-box"><a class="header" href="#running-out-of-the-box">Running Out of the Box</a></h1>
<p>Comments and documentation for BDB-SQLite suggest it should compile and run on
many platforms including some quite niche embedded operating systems.  However
we have only tested BDB-SQLite on common and current Linux distributions, where
the process is:</p>
<pre><code>tar zxvf berkeley-db-18.1.32.tar.gz
cd db-18.1.32/build_unix
../dist/configure --enable-sql_compat
make
</code></pre>
<p>This will create:</p>
<ol>
<li>libdb_sqlXX.{so|la} - A C API library, that exactly mirrors the SQLite
C API.</li>
<li>dbsql - A command line SQL interpreter, that exactly matches
the default sqlite3 command line interpreter.</li>
<li>libsqlite.{so|la} - A C API library, that exactly mirrors the SQLite C API,
and has the same name as the library generated by a SQLite build.</li>
<li>sqlite3 - A command line SQL interpreter, that exactly matches the 
semantics and name of the default sqlite3 command line interpreter.</li>
</ol>
<h1 id="design-of-the-port"><a class="header" href="#design-of-the-port">Design of the Port</a></h1>
<h2 id="files-and-directories"><a class="header" href="#files-and-directories">Files and Directories</a></h2>
<p>db-18.1.32/lang/sql/sqlite has the unmodified source for SQLite 3.18.2</p>
<p>db-18.1.32/lang/sql/adaptor has all of the replaced SQLite source files, 
such as btree.c</p>
<p>db-18.1.32/lang/sql/adaptor/sqlite-patches has 43 patches that modify the SQLite
sources. Most of them are very small patches, often just a line or two:</p>
<ul>
<li>inserting DB-specific files into the Makefile (eg the DB-specific pragmas)</li>
<li>branding (eg 07_shell_prompt.patch)</li>
<li>build fixes for less common platforms (Solaris, VS, Cygwin etc)</li>
<li>11 modifications to the test code that comes with SQLite</li>
</ul>
<p>db-18.1.32/dist/s_sql_upgrade is a script to help with upgrading the SQLite version</p>
<h2 id="code-changes"><a class="header" href="#code-changes">Code Changes</a></h2>
<p>In lang/sql/adaptor the significant files replaced are:</p>
<p>btree.c
backup.c
btreeInt.h
vacuum.c
userauth.c
userauth.h</p>
<p>There are also placeholder files, containing only function definitons and a
tiny amount of code. But most of these are not needed and a cleaner port would
probably eliminate them altogether:</p>
<p>backup.h
backup.c
pager.c
pager.h
btmutex.c
pcache.h  (stubs only, no code)
pcache.c  (stubs only, no code)
wal.h     (stubs only, no code)
pcache1.c (stubs only, no code)
wal.c     (stubs only, no code)</p>
<p>In addition there are files added to SQLite for entirely BDB-specific functionality. These
could probably be removed from the build without causing problems elsewhere:</p>
<p>db_encrypt.c
db_pragma.c
db_sequence.c
db_shell.c </p>
<div style="break-before: page; page-break-before: always;"></div><!-- Copyright 2021 The LumoSQL Authors, see LICENSES/MIT -->
<!-- SPDX-License-Identifier: MIT -->
<!-- SPDX-FileCopyrightText: 2021 The LumoSQL Authors -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Dan Shearer, February 2021 -->
<h1 id="notes-regarding-karl-malbrains-c-btree-code"><a class="header" href="#notes-regarding-karl-malbrains-c-btree-code">Notes Regarding Karl Malbrain's C Btree Code</a></h1>
<h1 id="context"><a class="header" href="#context">Context</a></h1>
<p>There are 4 key-value stores written in C that are used
at scale, are widely-ported and which have the property of being
<a href="https://en.wikipedia.org/wiki/Multiversion_concurrency_control">MVCC</a>. As documented in
<a href="https://lumosql.org/src/lumodoc/doc/trunk/doc/lumo-relevant-knowledgebase.md">the LumoDoc Knowledgebase</a>
they are:</p>
<ul>
<li>SQLite's built-in btree, which as the K-V store underneath the software which is most-deployed by at least two orders of magnitude, must therefore be the most-deployed K-V store.</li>
<li>Oracle BDB, once ubiquitous, effectively dead due to Oracle licensing changes. LumoSQL works with a relatively recent version of this out-of-date K-V store.</li>
<li><a href="http://www.lmdb.tech/doc/">LMDB</a>, which seems to have replaced BDB in most contexts, and which is modern, well-tested and uses mmap() instead of the older idea of Write Ahead Logs.</li>
<li><a href="https://github.com/bloomberg/comdb2">Comdb's BDB fork</a> (counting it as the spiritual successor to BDB, but <a href="https://bloomberg.com">Bloomberg</a> is in fact the only user. In another universe perhaps this could have become BDB-ng if LMDB didn't exist.</li>
</ul>
<h1 id="a-possible-new-contender-to-consider"><a class="header" href="#a-possible-new-contender-to-consider">A possible new contender to consider</a></h1>
<p><a href="mailto://malbrain@berkeley.edu">Karl Malbrain</a> has written a C Btree which is
not used anywhere, but which appears to have some novel features and be
intended to be efficient.  Since public C Btrees are relatively rare, this is
worth at least considering as a LumoSQL backend.</p>
<p>Karl says that his <a href="https://github.com/malbrain/database/tree/master/alpha">latest code in February 2021</a> 
is intended to go in his <a href="https://github.com/malbrain/Btree-source-code">Btree project</a> when it is more stable.
His <a href="">database project</a> has a wider scope than just the K-V store.</p>
<p>Features and experiments Karl mentions that may make this Btree interesting for making LumoSQL more scalable include:</p>
<ul>
<li>the multi-root-node subdirectory removes the locking load on the root node by creating a read-only copy of the latest updated root version. The root is updated out-of-band.</li>
<li>threads2 version: Multi-Threaded with latching implemented by a latch manager with test &amp; set latches in the first few btree pages.</li>
</ul>
<p>Karl's code is under the 2-Clause BSD.</p>
<div style="break-before: page; page-break-before: always;"></div><!-- SPDX-License-Identifier: CC-BY-SA-4.0 -->
<!-- SPDX-FileCopyrightText: 2020 The LumoSQL Authors -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Dan Shearer, 2020 -->
<h1 id="table-of-contents-3"><a class="header" href="#table-of-contents-3">Table of Contents</a></h1>
<ul>
<li><a href="lumo-corruption-detection-and-magic.html#summary-of-sql-database-corruption-detection">Summary of SQL Database Corruption Detection</a></li>
<li><a href="lumo-corruption-detection-and-magic.html#sqlite-and-integrity-checking">SQLite and Integrity Checking</a></li>
<li><a href="lumo-corruption-detection-and-magic.html#lumosql-checksums-and-the-sqlite-on-disk-file-format">LumoSQL Checksums and the SQLite On-disk File Format</a></li>
<li><a href="lumo-corruption-detection-and-magic.html#design-of-the-sqlite-checksum-vfs-loadable-extension">Design of the SQLite Checksum VFS Loadable Extension</a></li>
<li><a href="lumo-corruption-detection-and-magic.html#goals-for-corruption-detection">Goals for Corruption Detection</a></li>
<li><a href="lumo-corruption-detection-and-magic.html#design-for-corruption-detection">Design for Corruption Detection</a>
<ul>
<li><a href="lumo-corruption-detection-and-magic.html#for-non-row-data">For Non-row Data</a></li>
<li><a href="lumo-corruption-detection-and-magic.html#for-row-data">For Row Data</a></li>
</ul>
</li>
<li><a href="lumo-corruption-detection-and-magic.html#implementation-for-corruption-detection">Implementation for Corruption Detection</a></li>
</ul>
<p><img src="./images/lumo-corruption-detection-and-magic-intro.png" alt="" title="Not Yet Done" /></p>
<h1 id="summary-of-sql-database-corruption-detection"><a class="header" href="#summary-of-sql-database-corruption-detection">Summary of SQL Database Corruption Detection</a></h1>
<p>One of the short-term goals stated in the <a href="./lumo-project-aims.html">LumoSQL Project Aims</a> is:</p>
<blockquote>
<p>LumoSQL will improve SQLite quality and privacy compliance by introducing
optional on-disk checksums for storage backends including to the original
SQLite btree format. This will give real-time row-level corruption detection.</p>
</blockquote>
<p>This design and implementation discussion focusses on row-level corruption 
detection, which also gives the user a very rapid way of detecting changes.
The change detection aspect of row-level corruption detection is not dealt
with here, except that it is possible the speed benefits for detecting changes
might in many cases outweigh the costs of maintaining the checksum row.</p>
<p>It seems quite extraordinary that in 2020 none of the major online databases -
not Posgresql, Oracle, MariaDB, SQLServer or others - have the built-in ability
to check during a SELECT operation that the row being read from disk is exactly
the row that was previously written. There are many reasons why data can get
modified, deleted or overwritten outwith the control of the database, and the
ideal way to respond to this is to notify the database when a corrupt row is
accessed.  All that is needed is for a hash of the row to be stored with the
row when it is written.</p>
<p>All the major online databases have the capacity for an external process to
check disk files for database corruption, as does SQLite. This is very
different from real-time integrity checking, and cannot be done in real time.</p>
<p>Knowing that a corruption problem is limited to a row or an itemised
list of rows reduces a general &quot;database corruption problem&quot; down to a bounded
reconstruction task. Users can have confidence in the remainder of a database
even if there is corruption found in some rows.</p>
<p>This problem has been recognised and solved inefficiently at the SQL level by
various projects. Two of these are <a href="https://www.periscopedata.com/blog/hashing-tables-to-ensure-consistency-in-postgres-redshift-and-mysql">Periscope Data's Per-table Multi-database Solution</a>
and <a href="https://www.percona.com/blog/2018/10/12/track-postgresql-row-changes-using-public-private-key-signing/">Percona's Postgresql Public Key Row Tracking</a>.
By using SQL code rather than modifying the database internals there is a
performance hit. Both these companies specialise in performance optimisation
but choose not to apply it to this feature, suggesting they are not convinced
of high demand from users.</p>
<p>Interestingly, all the big online databases have row-level security, which has
many similarities to the problem of corruption detection. </p>
<p>For those databases that offer encryption, this is effectively page-level or
column-based hashes and therefore there is corruption detection by implication.
However this is not row-based checksumming, and it is not on by default in any
of the most common databases.</p>
<p>It is possible to introduce a checksum on database pages more easily than for
every row, and transparently to database users. However, knowing a database
page is corrupt isn't much help to the user, because there could be many rows
in a single page.</p>
<p>More on checksumming for SQL databses can be found referenced in <a href="./2.4-relevant-knowledebase/#list-of-relevant-sql-checksumming-related-knowledge">SQLite Relevant Knowledgebase</a>) </p>
<h1 id="sqlite-and-integrity-checking"><a class="header" href="#sqlite-and-integrity-checking">SQLite and Integrity Checking</a></h1>
<p>The SQLite developers go to great lengths to avoid database corruption, within
their project goals. Nevertheless, corrupted SQLite databases are an everyday
occurance for which there are recovery procedures and commercial tools.</p>
<p>SQLite does have checksums already in some places:</p>
<ul>
<li>for the journal transaction log (superceded by the Write Ahead Log system)</li>
<li>for each database page when using the closed-source SQLite Encryption Extension</li>
<li>for each page in a WAL file</li>
<li>for each page when using the Checksum VFS Extension, discussed below</li>
</ul>
<p>SQLite also has <a href="https://www.sqlite.org/pragma.html#pragma_integrity_check">PRAGMA integrity_check</a> and
<a href="https://www.sqlite.org/pragma.html#pragma_quick_check">PRAGMA quick_check</a>
which do partial checking, and which do not require exclusive access to the
database. These checks have to scan the database file sequentially and verify
the logic of its structure, because there are no checksums available to make it
work more quickly.</p>
<p>None of these are even close to end user benefits of row-level corruption
detection, at the potential cost of speed.</p>
<p>SQLite does have a file change counter in its database header, in 
<a href="https://www.sqlite.org/fileformat.html">offset 24 of the official file format</a>, however this
is not itself subject to integrity checks nor does it contain information about the rest of the file,
so it is a hint rather than a guarantee.</p>
<p>SQLite applications often need row-level integrity checking even more than the online databases because:</p>
<ul>
<li>SQLite embedded and IoT use cases often involve frequent power loss, which is the most likely time for corruption to occur.</li>
<li>an SQLite database is an ordinary filesystem disk file stored wherever the user decided, which can often be deleted or overwritten by any unprivileged process.</li>
<li>it is easy to backup an SQLite database partway through a transaction, meaning that the restore will be corrupted</li>
<li>SQLite does not have robust locking mechanisms available for access by multiple processes at once, since it relies on lockfiles and Posix advisory locking </li>
<li>SQLite provides the <a href="https://www.sqlite.org/vfs.html">VFS API Interface</a> which users can easily misuse to ignore locking via the sql3_*v2 APIs</li>
<li>the on-disk file format is seemingly often corrupted regardless of use case. Better evidence on this is needed but authors of SQLite data file recovery software (see listing in <a href="./2.4-relevant-knowledebase/#list-of-relevant-sql-checksumming-related-knowledge">SQLite Relevant Knowledgebase</a>) indicates high demand for their services. Informal shows of hands at conferences indicates that SQLite users expect corruption.</li>
</ul>
<p>sqlite.org has a much more detailed, but still incomplete, summary of <a href="https://www.sqlite.org/howtocorrupt.html">How to Corrupt an SQLite Database</a>.</p>
<h1 id="lumosql-checksums-and-the-sqlite-on-disk-file-format"><a class="header" href="#lumosql-checksums-and-the-sqlite-on-disk-file-format">LumoSQL Checksums and the SQLite On-disk File Format</a></h1>
<p>The SQLite database format is widely used as a defacto standard. LumoSQL ships
with the lumo-backend-mdb-traditional which is the unmodified SQLite on-disk
format, the same code generating the same data. There is no corruption
detection included in the file format for this backend.  However corruption
detection is available for the traditional backend, and other backends that do
not have scope for checksums in their headers. For all of these backends,
LumoSQL offers a separate metadata file containing integrity information.</p>
<p>The new backend lumo-backend-mdb-updated adds row-level checksums in the header
but is otherwise identical to the traditional SQLite MDB format. </p>
<p>There is an argument that any change at all is the same as having a completely
different format.  This is not a strong argument against adding checksums to
the traditional SQLite on-disk format because with encryption increasingly
becoming mandatory, the standard cannot apply. The sqlite.org closed-source SSE
solution is described as &quot;All database content, including the metadata, is
encrypted so that to an outside observer the database appears to be white
noise.&quot; Other solutions are possible involving metadata that is not encrypted
(but definitely checksummed), but in any case, there is no on-disk standard for
SQLite databases with encryption.</p>
<h1 id="design-of-the-sqlite-checksum-vfs-loadable-extension"><a class="header" href="#design-of-the-sqlite-checksum-vfs-loadable-extension">Design of the SQLite Checksum VFS Loadable Extension</a></h1>
<p>In April 2020 the <a href="https://sqlite.org/cksumvfs.html">SQLite Checksum VFS</a> was 
committed to the <a href="https://sqlite.org/src/file/ext/misc/cksumvfs.c">ext/ source tree</a>.
The design goals were:</p>
<blockquote>
<p>The checksum VFS extension is a VFS shim that adds an 8-byte checksum to the
end of every page in an SQLite database. The checksum is added as each page
is written and verified as each page is read. The checksum is intended to
help detect database corruption caused by random bit-flips in the mass
storage device. </p>
</blockquote>
<p>It is important to note that this VFS is among the very first, if not the first,
of mainstream databases to recognise that all read operations should be subject to
validation.</p>
<p>The VFS overloads the low-level Read() function like this:</p>
<pre><code>/* Verify the checksum if
 **    (1) the size indicates that we are dealing with a complete
 **        database page
 **    (2) checksum verification is enabled
 **    (3) we are not in the middle of checkpoint
*/
</code></pre>
<p>This means that if a page-level corruption is detected during a read operation
then SQLITE_IOERR_DATA is returned. This implementation has some major problems, including:</p>
<ul>
<li>No information about the logical location of this error, eg what row(s) it
affects. The application knows nothing about how rows map to pages.</li>
<li>No facility for isolation or recovery of data</li>
<li>Brittle implementation due to requirements of the file format. The
&quot;bytes of reserved space on each page&quot;
value at offset 20 the SQLite database header must be exactly 8.</li>
</ul>
<p>Good points to learn from this VFS include:</p>
<ul>
<li>the various PRAGMAs implememnted for control and ad hoc verification</li>
<li>the new data error</li>
<li>the fact that the status of verification is made visible via a SELECT</li>
<li>page level detection protects all parts of the database, not just rows</li>
</ul>
<h1 id="goals-for-corruption-detection"><a class="header" href="#goals-for-corruption-detection">Goals for Corruption Detection</a></h1>
<ul>
<li>Similar control interface to the Checksum VFS</li>
<li>Row-oriented detection</li>
<li>Detection available from SQL, with recovery an option</li>
<li>Special column, just like RowID</li>
<li>Complete abort also an option</li>
<li>Optionally include page level as well, however, not necessarily</li>
</ul>
<h1 id="design-for-corruption-detection"><a class="header" href="#design-for-corruption-detection">Design for Corruption Detection</a></h1>
<p>Row-level checksum data will be stored as an extra column. Non-row data will
be stored according to the same mechanism needed for encryption and other 
LumoSQL features. Per-row checksums are a valid choice without checksums 
for the other data including indexes and metadata.</p>
<p>It isn't yet clear whether there is merit in adding table-level corruption
detection, given that page-level checksumming is possible for all the 
initally-expected btree-type backends for SQLite. This can be added at a 
later stage, and would be included in the category of non-row data.</p>
<h2 id="for-non-row-data"><a class="header" href="#for-non-row-data">For Non-row Data</a></h2>
<p>Non-row data means all metadata associated with a LumoSQL database, which may
be considerably more than is with a traditional SQLite database depending on
the encryption or other options that are selected. We already know from the
Checksum VFS implementation that there is very little scope for adding checksum
metadata to a traditional SQLite file. </p>
<p>All LumoSQL backends can have corruption detection enabled, with the metadata
stored either directly in the backend database files, or in a separate file.
When a user switches on checksums for a database, metadata needs to be stored.</p>
<p>This depends on two new functions needed in any case for labelling LumoSQL
databases provided by backend-magic.c: lumosql_set_magic() and
lumosql_get_magic(). These functions add and read a unique metadata signature
to a LumoSQL database.</p>
<ol>
<li>
<p>if possible magic is inserted into the existing header</p>
</li>
<li>
<p>if not a separate &quot;metadata&quot; b-tree is created which contains a key &quot;magic&quot;
and the appropriate value. get_magic() will look for the special metadata
b-tree and the &quot;magic&quot; key</p>
</li>
</ol>
<h2 id="for-row-data"><a class="header" href="#for-row-data">For Row Data</a></h2>
<p>High-level design for row-level checksums is:</p>
<ol>
<li>an internally maintained row hash updated with every change to a row</li>
<li>If a corruption is detected on read, LumoSQL should make maximum relevant
fuss. At minimum, <a href="https://www.sqlite.org/rescode.html#corrupt">error code 11 is SQLITE_CORRUPT</a> but there is also
SQLITE_IOERR_DATA (not SQLITE_IOERR_DATA is missing from the official SQLite 
list of error codes, but this seems to be an error.)</li>
<li>This hash is kept in a special column so that user-level logic can do not
only corruption detection, but also change detection.</li>
</ol>
<p>At a later stage a column checksum can be added giving change detection on a
table, or corruption detection for read-only tables.</p>
<p>In the case where there is a separate metadata file, a function pair in
lumo-backend-magic.c reads and writes a whole-of-file checksum for the
database. This can't be done for where metadata is stored in the main database
file because it is a recursive problem. This is like a fail-early case of 
all other corruption detection, perhaps to warn the application to run 
integrity checks.</p>
<h1 id="implementation-for-corruption-detection"><a class="header" href="#implementation-for-corruption-detection">Implementation for Corruption Detection</a></h1>
<p>There is already precedent for having a column with metadata for every row, as 
explained in <a href="https://sqlite.org/c3ref/last_insert_rowid.html">the Last Insert Rowid documentation</a>:</p>
<blockquote>
<p>Each entry in most SQLite tables (except for WITHOUT ROWID tables) has a unique
64-bit signed integer key called the &quot;rowid&quot;. The rowid is always available as
an undeclared column named ROWID, OID, or <em>ROWID</em> as long as those names are
not also used by explicitly declared columns.</p>
</blockquote>
<p>The implementation for corruption detection is to perform similar operations to
maintain a similar implicit column. In every non-index btree, the 
btree.c/sqlite3BtreeInsert() is called before every write of a row. At this point
the full row data is known, and so (just before invalidateIncrblobCursors() is
called in that function) we can add a hash of that data to the ROWCSUM column.</p>
<p>For reading, the function sqlite3_step() sees every row read from a table, and 
can check that the hash matches the data about to be returned to the user.</p>
<p>The user can execute:</p>
<pre><code>    SELECT ROWCSUM from table;
</code></pre>
<p>and treat the results like any other column return (which is how change detection can 
be managed, by storing the checksum results in another column.)</p>
<p><a href="https://sqlite.org/withoutrowid.html">WITHOUT ROWID</a> tables are intended for
corner cases requiring marginally greater speed. Without some specific addition
reason not thought of yet, it seems incorrect to add checksums to a WITHOUT
ROWID table because that will reduce the one advantage they provide.</p>
<p>main.c/sqlite3_table_column_metadata() will need to know about ROWCSUM (and
<em>ROWCSUM</em>), however this does not seem to be vital functionality, used only in
shell.c and as a feature for extensions to export. </p>
<p>The control mechanism will be the same as for the Checksum VFS Extension, 
with the addition of the term &quot;<em>row</em>&quot; to make it clear this is on a per-row
rather than per-page basis:</p>
<pre><code>PRAGMA checksum_row_verification;          -- query status
PRAGMA checksum_row_verification=OFF;      -- disable verification
PRAGMA checksum_row_verification=ON;       -- re-enable verification

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><!-- SPDX-License-Identifier: CC-BY-SA-4.0 -->
<!-- SPDX-FileCopyrightText: 2020 The LumoSQL Authors -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Dan Shearer, 2020 -->
<h1 id="table-of-contents-4"><a class="header" href="#table-of-contents-4">Table of Contents</a></h1>
<ul>
<li><a href="3.3-benchmarking.html#about-benchmarking">About Benchmarking</a></li>
<li><a href="3.3-benchmarking.html#all-sqlite-performance-papers-are-nonsense">All SQLite Performance Papers are Nonsense</a></li>
<li><a href="3.3-benchmarking.html#limiting-the-problem-space">Limiting the Problem Space</a></li>
<li><a href="3.3-benchmarking.html#what-questions-will-benchmarking-answer">What Questions Will Benchmarking Answer?</a></li>
<li><a href="3.3-benchmarking.html#details-of-benchmarking-code">Details of Benchmarking Code</a>
<ul>
<li><a href="3.3-benchmarking.html#metrics">Metrics</a></li>
<li><a href="3.3-benchmarking.html#sql-in-benchmarktcl">SQL in benchmark.tcl</a></li>
<li><a href="3.3-benchmarking.html#sql-logic-test">SQL Logic Test</a></li>
<li><a href="3.3-benchmarking.html#c-speed-tests-with-sqlite-c-api">C speed tests with SQLite C API</a></li>
</ul>
</li>
<li><a href="3.3-benchmarking.html#computer-architectures-and-operating-systems">Computer architectures and operating systems</a>
<ul>
<li><a href="3.3-benchmarking.html#c-speed-tests-with-the-sqlitelumosql-kv-api">C speed tests with the SQLite/LumoSQL KV API</a></li>
</ul>
</li>
<li><a href="3.3-benchmarking.html#list-of-relevant-benchmarking-and-test-knowledge">List of Relevant Benchmarking and Test Knowledge</a></li>
</ul>
<h1 id="about-benchmarkings"><a class="header" href="#about-benchmarkings">About Benchmarkings</a></h1>
<p>Having a reliable benchmarking system has always been one of the LumoSQL objectives. LumoSQL is a modification of SQLite and benchmarking is used to measure and compare the performance of different builds on different machines.</p>
<p>The results are stored in an SQLite database which is available to download at  <a href="https://lumosql.org/dist/benchmarks-to-date">https://lumosql.org/dist/benchmarks-to-date</a>. It is being actively updated and accepting data from volunteers. </p>
<blockquote>
<p><a href="https://lumosql.org/dist/benchmarks-to-date/all-lumosql-benchmark-data-combined.sqlite">Direct Download Link</a></p>
</blockquote>
<p>The source code for benchmarking tools can be found in the <a href="https://lumosql.org/src/lumosql/dir?name=tool">lumosql repo</a>. <em>benchmark-filter.tcl</em>  is a useful tool for viewing the data, see <a href="https://lumosql.org/src/lumosql/file?name=doc/lumo-benchmark-filter.md">documentation on how to use it</a>. </p>
<p>Alternatively, plotted data is presented with an <a href="http://r.lumosql.org:3838/contrastexample.html">interactive web UI</a>.</p>
<p>Once LumoSQL is installed the user can perform benchmarks using <code>make benchmark [OPTIONS]</code>. Follow an <a href="https://lumosql.org/src/lumosql/doc/tip/README.md#using-the-build-and-benchmark-system">example of running a benchmark</a> and read the full documantation on <a href="https://lumosql.org/src/lumosql/doc/tip/doc/lumo-build-benchmark.md">benchmark options</a>.</p>
<h1 id="discussion"><a class="header" href="#discussion">Discussion</a></h1>
<p>The strange thing is that benchmarking between SQL databases is almost non-existent, as well as difficult.
We focus on the practical recommendations of the 2018
paper <a href="https://mytherin.github.io/papers/2018-dbtest.pdf">Fair Benchmarking Considered Difficult:Common Pitfalls In Database Performance Testing</a>. 
We store the results in an SQLite database, and we make the method and the 
<a href="https://lumosql.org/dist/benchmarks-to-date/">results</a> available publicly.</p>
<p>The LumoSQL benchmarking problem is less difficult than comparing 
unrelated databases, which is perhaps why the <a href="https://tpc.org">Transaction Processing Performance Council</a> has not published news since 2004.
There are testing tools released with SQLite, Postgresql, MariaDB etc, but 
there simply is no way to compare. Benchmarking and testing overlap.</p>
<p>The well-described <a href="https://sqlite.org/testing.html">testing of SQLite</a>
involves some open code, some closed code, and many ad hoc processes. Clearly
the SQLite team have an internal culture of testing that has benefited the
world. However that is very different to testing that is reproducible by
anyone, which is in turn very different to reproducible reproducible by anyone,
and that is even without considering whether the benchmarking is a reasonable
approximation of actual use cases.</p>
<h2 id="all-sqlite-performance-papers-are-nonsense"><a class="header" href="#all-sqlite-performance-papers-are-nonsense">All SQLite Performance Papers are Nonsense</a></h2>
<p>In 2017 a helpful paper was published by <a href="https://www.cs.utexas.edu/%7Evijay/papers/apsys17-sqlite.pdf">Purohith, Mohan and Chidambaram</a> on the
topic of &quot;The Dangers and Complexities of SQLite Benchmarking&quot;. Since the first
potential problem is that this paper itself is in error, LumoSQL repeated
the literature research component in the paper. We agree with the authors in stating:</p>
<blockquote>
<p>When we investigated 16 papers from the 2015-2017
whose evaluation included SQLite, we find that none report
all the parameters required to meaningfully compare
results: ten papers do not report any parameters [17–26],
five do not report the sync mode [27–31], while only
one paper reports all parameters except write-ahead log
size [32]. Without reporting how SQLite was configured,
it is meaningless to compare SQLite results.</p>
</blockquote>
<p>LumoSQL found three additional papers published in 2017-2019, with similar flaws.
In brief:</p>
<blockquote>
<p><strong>All published papers on SQLite's performance are nonsense</strong></p>
</blockquote>
<p>And this is for SQLite alone, something that has relatively few parameters
compared to the popular online SQL databases. The field of SQL databases in
general is even more poorly benchmarked.</p>
<p>Benchmarking between SQL databases hardly exists at all. </p>
<h1 id="limiting-the-problem-space"><a class="header" href="#limiting-the-problem-space">Limiting the Problem Space</a></h1>
<p>LumoSQL has some advantages that reduce the problem space for benchmarking:</p>
<ul>
<li>The test harness is effectively the entire SQLite stack above the btree layer
(or lumo-backend.c). It is true that SQLite benchmarking is difficult because
there are so many pragmas and compile options, but most of these apply to all
backends. The <em>effect</em> of a given pragma or compile option may differ by
backend, but this will be a second-order effect and hopefully not as severe as
first-order effects.</li>
<li>The backends we have today differ in a relatively small number of dimensions,
usually to do with their speciality. The SQLite Btree backend has options for
WAL files, journals and cache sizes; the LMDB backend uses the OS buffer cache
and so there are OS-level defaults to be aware of; the BDB backend has tuning
options relating to cache and locking. That relatively small number of
differences still potentially gives a large benchmarking matrix, so we have to
control for that (or regard computation as free, which is close to accurate at
the scale of the LumoSQL project.)</li>
<li>No networking, clustering or client interoperability involved. This
eliminates many classes of complexity.</li>
</ul>
<p>To further reduce the problem space we will not be testing across multiple
platforms. This can be addressed later.</p>
<h1 id="what-questions-will-benchmarking-answer"><a class="header" href="#what-questions-will-benchmarking-answer">What Questions Will Benchmarking Answer?</a></h1>
<p>Questions by LumoSQL/SQLite internals developers:</p>
<ul>
<li>I am considering a change to the main code path to integrate a new feature,
will the performance of LumoSQL suffer?</li>
<li>I have identified a potential optimisation, is the performance benefit worth
the additional complexity?</li>
<li>I have implemented a new backend, should we make it the default?</li>
</ul>
<p>Questions by LumoSQL/SQLite application developers:</p>
<ul>
<li>Is LumoSQL any different from SQLite when configured to use the SQLite backend?</li>
<li>I have these requirements for a system, which LumoSQL backend should I choose?</li>
</ul>
<h1 id="checklist-from-the-considered-difficult-paper"><a class="header" href="#checklist-from-the-considered-difficult-paper">Checklist from the &quot;Considered Difficult&quot; Paper</a></h1>
<p>We have considered the checklist from the <a href="https://mytherin.github.io/papers/2018-dbtest.pdf">Fair Benchmarking Considered Difficult:Common Pitfalls In Database Performance Testing paper</a> as a guidline for good benckmarking practice.</p>
<ul>
<li>Choosing your Benchmarks.
<ul>
<li>Benchmark covers whole evaluation space</li>
<li>Justify picking benchmark subset</li>
<li>Benchmark stresses functionality in the evaluation space</li>
</ul>
</li>
<li>Reproduciblity.
<ul>
<li>Hardware configuration</li>
<li>DBMS parameters and version</li>
<li>Source code or binary files</li>
<li>Data, schema &amp; queries</li>
</ul>
</li>
<li>Optimization.
<ul>
<li>Compilation flags</li>
<li>System parameters</li>
</ul>
</li>
<li>Apples vs Apples
<ul>
<li>Similar functionality</li>
<li>Equivalent workload</li>
</ul>
</li>
<li>Comparable tuning
<ul>
<li>Different data</li>
<li>Various workloads</li>
</ul>
</li>
<li>Cold/warm/hot runs.
<ul>
<li>Differentiate between cold and hot runs</li>
<li>Cold runs: Flush OS and CPU caches</li>
<li>Hot runs: Ignore initial runs</li>
</ul>
</li>
<li>Preprocessing.
<ul>
<li>Ensure preprocessing is the same between systems</li>
<li>Be aware of automatic index creation</li>
</ul>
</li>
<li>Ensure correctness.
<ul>
<li>Verify results</li>
<li>Test different data sets</li>
<li>Corner cases work</li>
</ul>
</li>
<li>Collecting Results.
<ul>
<li>Do several runs to reduce interference</li>
<li>Check standard deviation for multiple runs</li>
<li>Report robust metrics (e.g., median and confidence inter-vals)</li>
</ul>
</li>
</ul>
<h2 id="reproducibility-and-tests"><a class="header" href="#reproducibility-and-tests">Reproducibility and Tests</a></h2>
<p>We aim to record a complete set of parameters that define the outcome of the benchmark run. First, to define the environment in which the runs are performed we record:</p>
<ul>
<li>os-type</li>
<li>os-version</li>
<li>cpu-type</li>
<li>cpu-comment</li>
<li>disk-comment</li>
<li>byte-order</li>
<li>word-size</li>
</ul>
<p>Secondly, the exact versions of software involved in the build process:</p>
<ul>
<li>backend</li>
<li>backend-id</li>
<li>backend-name</li>
<li>backend-version</li>
<li>backend-date</li>
<li>sqlite-id</li>
<li>sqlite-name</li>
<li>sqlite-version</li>
<li>sqlite-date</li>
<li>notforking-id</li>
<li>notforking-date</li>
</ul>
<p>And lastly, the software specific parameters:</p>
<ul>
<li>option-debug</li>
<li>option-lmdb_debug</li>
<li>option-lmdb_fixed_rowid</li>
<li>option-lmdb_transaction</li>
<li>option-rowsum</li>
<li>option-rowsum_algorithm</li>
<li>option-sqlite3_journal</li>
</ul>
<p>Each run performs 17 types of queries that reflect the average user experience (user-cpu-time,  system-cpu-time, and real-time is recorded for each test) :</p>
<ul>
<li>Creating database and tables</li>
<li>1000 INSERTs</li>
<li>100 UPDATEs without an index, upgrading a read-only transaction</li>
<li>25000 INSERTs in a transaction</li>
<li>100 SELECTs without an index</li>
<li>100 SELECTs on a string comparison</li>
<li>Creating an index</li>
<li>5000 SELECTs with an index</li>
<li>1000 UPDATEs without an index</li>
<li>25000 UPDATEs with an index</li>
<li>25000 text UPDATEs with an index</li>
<li>INSERTs from a SELECT</li>
<li>DELETE without an index</li>
<li>DELETE with an index</li>
<li>A big INSERT after a big DELETE</li>
<li>A big DELETE followed by many small INSERTs</li>
<li>DROP TABLE</li>
</ul>
<p>Runs are performed on different scales by multiplying the number of queries by some factor. That factor is recorded as:</p>
<ul>
<li>option-datasize</li>
</ul>
<h1 id="details-of-benchmarking-code"><a class="header" href="#details-of-benchmarking-code">Details of Benchmarking Code</a></h1>
<h2 id="metrics"><a class="header" href="#metrics">Metrics</a></h2>
<p>Benchmarking will take place via SQL, with these items being measured at least:</p>
<ul>
<li>
<p>Elapsed time for a series of SQL statements</p>
<p>The TCL script benchmark.tcl is a forked version of speedtest.tcl, which 
writes results to an SQLite database as well a producing HTML output.
The SQL statements are discussed further down in this section. Each of the 
timed tests will also have VDBE ops and IOPS recorded as per the next 
two sections.</p>
</li>
<li>
<p>VDBE Operations per second </p>
<p>benchmark.tcl can collect VDBE ops, but only with some help from LumoSQL.</p>
<p>A timer is started in sqlite3_prepare(), VDBE opcodes are counted in
sqlite3VdbeExec(), and the timer is stopped in sqlite3_finalize(). This
then allows us to calculate how long the sql3_stmt took to execute per
instruction. The number of instructions will be the same for all backends.</p>
</li>
<li>
<p>Disk Operations per second</p>
<p>benchmark.tcl can do this by comparing per-pid IOPS using the algorithm
here: https://github.com/true/aspersa-mirror/blob/master/iodump . 
We look up the IOPS at the beginning and end of the test and store the 
difference. </p>
<p>This is not portable to other operating systems, however, that will
hopefully be a relatively small variable compared to the the 
variable of one backend vs another. </p>
</li>
</ul>
<h2 id="sql-in-benchmarktcl"><a class="header" href="#sql-in-benchmarktcl">SQL in benchmark.tcl</a></h2>
<p>To start with we are modifying speedtest.tcl as described. We are adding a BLOB
test with large generated blobs, but it is basically the same. In the future we
need to have more realistic SQL statements. And that varies by use case:</p>
<ol>
<li>embedded style SQL statements, typically developing for heavily resource
constrained deployments, who are likely to use SQL to simply store and
retrieve values and be more interested in tradeoffs and settings that
reduce latency. Tightly coupled with the SQLite library. Short transactions.</li>
<li>online style SQL statements, used for transaction processing. Concurrency
matters. Same SQL might be used with another database. Some long transactions.</li>
<li>online style SQL statements, used for analytics processing. Much more 
batch oriented. Same SQL might be used with another database. Some long transactions.</li>
</ol>
<h2 id="sql-logic-test"><a class="header" href="#sql-logic-test">SQL Logic Test</a></h2>
<p>It isn't clear that the SQL logic test is suitable for benchmarking. We are 
working on this, but our hope is that it will be readily adaptable.</p>
<p>This works by ODBC - noting that SQLite has an ODBC driver.</p>
<h2 id="c-speed-tests-with-sqlite-c-api"><a class="header" href="#c-speed-tests-with-sqlite-c-api">C speed tests with SQLite C API</a></h2>
<p>We have only done basic testing to make sure the code runs.  Our objective in
running these tests will be to quantify performance. These tests use the C API.</p>
<p><code>speedtest1.c</code> appears to be very actively maintained by <a href="https://sqlite.org">https://sqlite.org</a>,
the file has a number of different contributors and has frequent commits.</p>
<p><code>mptest.c</code> and <code>threadtest3.c</code> look promising for testing async access. See the 
notes previously about the unsophisticated concurrency handling we have already
demonstrated in SQLite. </p>
<h1 id="computer-architectures-and-operating-systems"><a class="header" href="#computer-architectures-and-operating-systems">Computer architectures and operating systems</a></h1>
<p>We are not going to get ambitious with platforms and variations. For the present,
benchmarking on 64 bit x86 Linux will be sufficient.</p>
<p>We will impose memory pressure in benchmarking runs by limiting the memory
available to the LumoSQL process. However we can do this effectively with the
cgroups API rather than having small VMs.</p>
<p>Other obvious variations for the future include Windows, and 32-bit hardware.
We are ignoring these for now.</p>
<h2 id="c-speed-tests-with-the-sqlitelumosql-kv-api"><a class="header" href="#c-speed-tests-with-the-sqlitelumosql-kv-api">C speed tests with the SQLite/LumoSQL KV API</a></h2>
<p>This is a lesser priority.</p>
<p>It is important to also benchmark at the LumoSQL KV API level, ie
lumo-backend.c .  This is so that we can observe if the performance of each
backend remains roughly the same (especially, <em>relatively</em> the same compared to
the others) whether accessed via the SQLite API or directly via the common KV
API. It is possible that the SQLite stack will have some unexpected interaction
with a particular backend - to pick a pathological corner case, a magic string.</p>
<h1 id="list-of-relevant-benchmarking-and-test-knowledge"><a class="header" href="#list-of-relevant-benchmarking-and-test-knowledge">List of Relevant Benchmarking and Test Knowledge</a></h1>
<p>References articles and papers discussing benchmarking can be found in the <a href="./2.4-relevant-knowledgebase.html#list-of-relevant-benchmarking-and-test-knowledge">Full Knowledgebase Relevant to LumoSQL</a> section.</p>
<p>References to other benchmarking tools are linked in the <a href="./3.7-relevant-codebases.html">Relevant Codebases</a> section.</p>
<h1 id="benchmarking-lumosql"><a class="header" href="#benchmarking-lumosql">Benchmarking LumoSQL</a></h1>
<h1 id="nature-of-the-benchmarking"><a class="header" href="#nature-of-the-benchmarking">Nature of the Benchmarking</a></h1>
<p>The SQL benchmarking is to seek answers about throughput:</p>
<ul>
<li>total time elapsed for tests</li>
<li>bytes per second (in cases where we are reading or writing a known quantity of data)</li>
<li>operations per second (with the ops measured either by <em>vdbe</em>c, or os_*c, or both.) </li>
</ul>
<p>LumoSQL benchmarking will mostly be using blackbox testing approaches
(high-level functionality) with some highly-targetted whitebox testing for
known tricky differences between backends (eg locking). </p>
<p>Being a low-level library, functional benchmarking often gets close to
internals testing. That's ok, but we need to be aware of this. We don't want to
be doing internals testing. That is for make test.</p>
<p>At a later date we can add benchmarking at LumoSQL KV API level, ie
lumo-backend.c .  This is so that we can observe if the performance of each
backend remains roughly the same (especially, <em>relatively</em> the same compared to
the others) whether accessed via the SQLite API or directly via the common KV
API. It is possible that the SQLite stack will have some unexpected interaction
with a particular backend - to pick a pathological corner case, a magic string.</p>
<h1 id="possible-benchmarking-dimensions"><a class="header" href="#possible-benchmarking-dimensions">Possible Benchmarking Dimensions</a></h1>
<p>[TBD - notes only]</p>
<ul>
<li>Dataset can fit entirely in memory (or not)</li>
<li>Caching (durability on/off)</li>
<li>Updates vs Reads vs -Only</li>
<li>Ops per second at disk</li>
<li>VDBE ops per second</li>
<li>Latency at C API</li>
<li>Blobs</li>
</ul>
<h1 id="sql-dimensions"><a class="header" href="#sql-dimensions">SQL Dimensions</a></h1>
<ul>
<li>Single filtering plus offset</li>
<li>join with grouping and ordering</li>
<li>multiple indexing</li>
</ul>
<h1 id="concurrency-dimensions"><a class="header" href="#concurrency-dimensions">Concurrency Dimensions</a></h1>
<p>This is going to be very important, especially since concurrency is one of SQLite known weak points.
<a href="https://github.com/sqlite/sqlite/blob/master/test/async2.test">The async SQLite tests</a> don't seem to be stressing concurrency really.
<a href="https://github.com/sqlite/sqlite/tree/master/mptest">mptest</a> seems to be more along those lines but I haven't done sample runs of it (mptest hasn't changed in 7 years, and all the references to it seem to be in the context of Windows, if that means anything.)</p>
<h1 id="workload-simulation"><a class="header" href="#workload-simulation">Workload Simulation</a></h1>
<ul>
<li>Analytics-type workload patterns</li>
<li>Human thread-following app simulation - nearly all read operations</li>
<li>50/50 read/write - classical ecommerce-type application</li>
<li>Mixture of all of above on different threads to be really mean</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><!-- Copyright 2020 The LumoSQL Authors, see LICENSES/MIT -->
<!-- SPDX-License-Identifier: MIT -->
<!-- SPDX-FileCopyrightText: 2020 The LumoSQL Authors -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Claudio Calvelli, November 2020 -->
<h1 id="table-of-contents-5"><a class="header" href="#table-of-contents-5">Table of Contents</a></h1>
<ul>
<li><a href="lumo-benchmark-filter.html#displayingprocessing-benchmark-results">Displaying/processing benchmark results</a></li>
<li><a href="lumo-benchmark-filter.html#full-set-of-options">Full set of options</a></li>
</ul>
<h1 id="displayingprocessing-benchmark-results"><a class="header" href="#displayingprocessing-benchmark-results">Displaying/processing benchmark results</a></h1>
<p>The LumoSQL project runs a number of benchmarks on different versions of SQLite
optionally combined with third party storage backends; the results of these
benchmarks are kept in a SQLite database, by default <code>benchmarks.sqlite</code>;
the <code>benchmark-filter</code> tool is a simple TCL script which displays the results
in different ways; it can optionally also update the database to add extra
information to the benchmarks (for example something to identify who ran the
benchmarks, or the system where they ran) and export the benchmarks to a text
file for sending to other people without sending the sqlite database itself.</p>
<p>After running some benchmarks, call the tool with:</p>
<pre><code>tclsh tool/benchmark-filter.tcl OPTIONS
</code></pre>
<p>If sqlite3 is installed on the system, or it is available from the LumoSQL
build directory, the tool can run without any options; otherwise it needs
to be given a path to a working version of sqlite; for the examples here
we assume that sqlite3 is available and the tool can run without any options.</p>
<p>Without any options it will show a summary of the most recent 20 benchmarks
with one line per benchmark starting with a &quot;run ID&quot; which is a unique
identifier which can be used to refer to the benchmark.  For example:</p>
<pre><code>tclsh tool/benchmark-filter.tcl
RUN_ID                                                            TARGET                         DATE        TIME         DURATION
6D5E57885E9AE39E44CFF21ECAF40C24A8C4734A2829E1BD045B998090E1777E  3.37.0                         2022-02-13  18:58:48        4.679
81AC418B168B405615E46E916E714C96E6E3CA973A4CD4E4121D3865A178547E  3.37.0+lmdb-0.9.28             2022-02-13  18:59:14        3.280
4D0479B0DDA565B181B3EF17CAFB9DB97AAB6030B89C6189A46417D18C079509  3.37.1                         2022-02-13  18:59:39        4.728
C8FB7CC1C6486BB13F88ACD14C6C99572CEBEACA237645D3892F82BA1C25023A  3.37.1+lmdb-0.9.28             2022-02-13  19:00:05        3.024
0D18BDC37964B1D01150C160168DF5AB8A7514B20CF4E12170D9935B6682FCE3  3.37.2                         2022-02-13  19:00:29        4.207
670890DBE1E7AC4D61502E29FFAA5F950F59CD43D836BE3F34C3987FECD3C5DE  3.37.2+lmdb-0.9.28             2022-02-13  19:00:55        3.235
59074F6911449A752D9B842B5463869FE4283189E00FE3412A6F61A41CA2C63C  3.37.0+lmdb-0.9.29             2022-02-13  19:01:20        3.332
380FAC937E5B3088B0C246B4E0BE8FBB518085D6AD02956B53B90F071EC75BB4  3.37.1+lmdb-0.9.29             2022-02-13  19:01:44        3.188
F94BAB67A863BBA40CDE5D0B6C341FB9894563C008E44D682895C6723B4CD6AB  3.37.2+lmdb-0.9.29             2022-02-13  19:02:09        3.201
2CE9245515AFCBF387E94276309CCA4E9B70C920648F3509A95F3468F0F190A6  3.18.2++datasize-2             2022-02-13  19:02:41        8.829
A7342AB2CFF20FDF0A50B7EE88ED3907BDCCA9FA565D507C15A6537440C42930  +bdb-18.1.32+datasize-2        2022-02-13  19:03:11       10.879
D04DBC3C4BCF95331F209D01A58CF336ACBE3A85C2728837F5C0FF23E19494FC  3.37.0++datasize-2             2022-02-13  19:03:44        9.460
5851F2F616898546DECF1FA40BC8DB4B30B804F54C8B843A642B238F5DE06B94  3.37.0+lmdb-0.9.28+datasize-2  2022-02-13  19:04:14       34.170
C30E8AA797DCE8AB613C59A4D595E2FC530A4443B20FFAA17F9139D7285B94A9  3.37.1++datasize-2             2022-02-13  19:05:10        9.140
5BA1D49295BE671078A8162735B999918283FD8D719289204019A6E3C1D28D40  3.37.1+lmdb-0.9.28+datasize-2  2022-02-13  19:05:40       32.710
5790AB5B8F27AFFFE663A1A69A183B46F0261E40A6BBC8CA3AD541521325C308  3.37.2++datasize-2             2022-02-13  19:06:35        9.315
4E9B32A4DEBA03F09F44961726EACD7DF98161ABE94BD0F5A26965AEE80EAFC0  3.37.2+lmdb-0.9.28+datasize-2  2022-02-13  19:07:05       32.959
8DDF79AA141A5E6FB86BE0669088C5BA4DBD7D11D0BC2CF28CFF197F94E61682  3.37.0+lmdb-0.9.29+datasize-2  2022-02-13  19:08:00       33.064
11FE5A7259AA58C536260D4CC69D2CCA8E58700367B13BA809914EF11259BA51  3.37.1+lmdb-0.9.29+datasize-2  2022-02-13  19:08:54       32.555
3831757C412F45E78E3CBD7620C70ECB8A59AAEDA37F35E15078A70987332456  3.37.2+lmdb-0.9.29+datasize-2  2022-02-13  19:09:48       32.042
FIlter returned more than 20 runs, list has been truncated
Use: -limit NUMBER to change the limit, or: -limit 0 to show all
</code></pre>
<p>To display one or more results, add the run IDs to the command, for example:</p>
<pre><code>tclsh tool/benchmark-filter.tcl 0D18BDC37964B1D01150C160168DF5AB8A7514B20CF4E12170D9935B6682FCE3
    Benchmark: sqlite 3.37.2
       Target: 3.37.2
              (3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1)
       Ran at: 2022-02-13 19:00:29
     Duration: 4.207
    Disk time: read: 0.522; write: 2.177

       TIME TEST NAME
      0.007    1 Creating database and tables
      2.789    2 1000 INSERTs
      0.013    3 100 UPDATEs without an index, upgrading a read-only transaction
      0.085    4 25000 INSERTs in a transaction
      0.117    5 100 SELECTs without an index
      0.436    6 100 SELECTs on a string comparison
      0.032    7 Creating an index
      0.066    8 5000 SELECTs with an index
      0.061    9 1000 UPDATEs without an index
      0.155   10 25000 UPDATEs with an index
      0.136   11 25000 text UPDATEs with an index
      0.082   12 INSERTs from a SELECT
      0.076   13 DELETE without an index
      0.060   14 DELETE with an index
      0.059   15 A big INSERT after a big DELETE
      0.022   16 A big DELETE followed by many small INSERTs
      0.013   17 DROP TABLE
------------
      4.207 (total benchmark run time)
</code></pre>
<p>or:</p>
<pre><code>tclsh tool/benchmark-filter.tcl 0D18BDC37964B1D01150C160168DF5AB8A7514B20CF4E12170D9935B6682FCE3 670890DBE1E7AC4D61502E29FFAA5F950F59CD43D836BE3F34C3987FECD3C5DE F94BAB67A863BBA40CDE5D0B6C341FB9894563C008E44D682895C6723B4CD6AB
Column 1
    Benchmark: sqlite 3.37.2
       Target: 3.37.2
              (3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1)
       Ran at: 2022-02-13 19:00:29
     Duration: 4.207
    Disk time: read: 0.522; write: 2.177

Column 2
    Benchmark: sqlite 3.37.2 with lmdb 0.9.28
       Target: 3.37.2+lmdb-0.9.28
              (3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1lmdb 0.9.28)
       Ran at: 2022-02-13 19:00:55
     Duration: 3.235
    Disk time: read: 0.522; write: 2.176

Column 3
    Benchmark: sqlite 3.37.2 with lmdb 0.9.29
       Target: 3.37.2+lmdb-0.9.29
              (3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1lmdb 0.9.29)
       Ran at: 2022-02-13 19:02:09
     Duration: 3.201
    Disk time: read: 0.523; write: 2.192

--------------- TIME --------------
          1           2           3 TEST NAME
      0.007       0.010       0.010    1 Creating database and tables
      2.789       1.484       1.429    2 1000 INSERTs
      0.013       0.020       0.016    3 100 UPDATEs without an index, upgrading a read-only transaction
      0.085       0.094       0.096    4 25000 INSERTs in a transaction
      0.117       0.240       0.238    5 100 SELECTs without an index
      0.436       0.533       0.527    6 100 SELECTs on a string comparison
      0.032       0.037       0.044    7 Creating an index
      0.066       0.062       0.061    8 5000 SELECTs with an index
      0.061       0.099       0.103    9 1000 UPDATEs without an index
      0.155       0.180       0.183   10 25000 UPDATEs with an index
      0.136       0.144       0.148   11 25000 text UPDATEs with an index
      0.082       0.080       0.092   12 INSERTs from a SELECT
      0.076       0.082       0.077   13 DELETE without an index
      0.060       0.070       0.074   14 DELETE with an index
      0.059       0.071       0.069   15 A big INSERT after a big DELETE
      0.022       0.021       0.023   16 A big DELETE followed by many small INSERTs
      0.013       0.007       0.010   17 DROP TABLE
------------------------------------
      4.207       3.235       3.201 (total benchmark run time)
</code></pre>
<p>This result can also be obtained by selecting runs by their properties, in this
case they all had the SQLite version (3.37.2) and datasize (1) in common, so:</p>
<pre><code>tclsh tool/benchmark-filter.tcl -version 3.37.2 -datasize 1
Column 1
    Benchmark: sqlite 3.37.2
       Target: 3.37.2
              (3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1)
       Ran at: 2022-02-13 19:00:29
     Duration: 4.207
    Disk time: read: 0.522; write: 2.177

... (same output as previous example)
</code></pre>
<p>Or to compare all LMDB results with datasize 2:</p>
<pre><code>tclsh tool/benchmark-filter.tcl -backend lmdb -datasize 2
Column 1
    Benchmark: sqlite 3.37.0 with lmdb 0.9.28
       Target: 3.37.0+lmdb-0.9.28+datasize-2
              (3.37.0 2021-11-27 14:13:22 bd41822c7424d393a30e92ff6cb254d25c26769889c1499a18a0b9339f5dalt1lmdb 0.9.28)
       Ran at: 2022-02-13 19:04:14
     Duration: 34.170
    Disk time: read: 0.403; write: 1.938

Column 2
    Benchmark: sqlite 3.37.1 with lmdb 0.9.28
       Target: 3.37.1+lmdb-0.9.28+datasize-2
              (3.37.1 2021-12-30 15:30:28 378629bf2ea546f73eee84063c5358439a12f7300e433f18c9e1bddd948dalt1lmdb 0.9.28)
       Ran at: 2022-02-13 19:05:40
     Duration: 32.710
    Disk time: read: 0.405; write: 1.978

Column 3
    Benchmark: sqlite 3.37.2 with lmdb 0.9.28
       Target: 3.37.2+lmdb-0.9.28+datasize-2
              (3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1lmdb 0.9.28)
       Ran at: 2022-02-13 19:07:05
     Duration: 32.959
    Disk time: read: 0.404; write: 1.948

Column 4
    Benchmark: sqlite 3.37.0 with lmdb 0.9.29
       Target: 3.37.0+lmdb-0.9.29+datasize-2
              (3.37.0 2021-11-27 14:13:22 bd41822c7424d393a30e92ff6cb254d25c26769889c1499a18a0b9339f5dalt1lmdb 0.9.29)
       Ran at: 2022-02-13 19:08:00
     Duration: 33.064
    Disk time: read: 0.404; write: 1.941

Column 5
    Benchmark: sqlite 3.37.1 with lmdb 0.9.29
       Target: 3.37.1+lmdb-0.9.29+datasize-2
              (3.37.1 2021-12-30 15:30:28 378629bf2ea546f73eee84063c5358439a12f7300e433f18c9e1bddd948dalt1lmdb 0.9.29)
       Ran at: 2022-02-13 19:08:54
     Duration: 32.555
    Disk time: read: 0.404; write: 1.939

Column 6
    Benchmark: sqlite 3.37.2 with lmdb 0.9.29
       Target: 3.37.2+lmdb-0.9.29+datasize-2
              (3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1lmdb 0.9.29)
       Ran at: 2022-02-13 19:09:48
     Duration: 32.042
    Disk time: read: 0.404; write: 1.950

--------------------------------- TIME --------------------------------
          1           2           3           4           5           6 TEST NAME
      0.010       0.010       0.010       0.009       0.014       0.014    1 Creating database and tables
      2.973       3.113       2.621       2.599       2.842       2.511    2 2000 INSERTs
      0.053       0.060       0.057       0.053       0.037       0.056    3 200 UPDATEs without an index, upgrading a read-only transaction
      0.158       0.162       0.160       0.157       0.168       0.163    4 50000 INSERTs in a transaction
      0.828       0.881       0.874       0.812       0.900       0.862    5 200 SELECTs without an index
      2.056       2.019       2.059       2.039       1.989       2.035    6 200 SELECTs on a string comparison
      0.068       0.063       0.068       0.059       0.074       0.079    7 Creating an index
     26.623      24.999      25.713      25.925      25.114      24.914    8 10000 SELECTs with an index
      0.289       0.308       0.307       0.305       0.317       0.309    9 2000 UPDATEs without an index
      0.347       0.343       0.339       0.343       0.340       0.340   10 50000 UPDATEs with an index
      0.266       0.261       0.266       0.266       0.268       0.269   11 50000 text UPDATEs with an index
      0.132       0.137       0.116       0.146       0.119       0.142   12 INSERTs from a SELECT
      0.126       0.127       0.126       0.127       0.131       0.125   13 DELETE without an index
      0.091       0.083       0.082       0.085       0.094       0.080   14 DELETE with an index
      0.114       0.109       0.127       0.112       0.114       0.116   15 A big INSERT after a big DELETE
      0.026       0.024       0.023       0.017       0.025       0.020   16 A big DELETE followed by many small INSERTs
      0.010       0.011       0.012       0.011       0.010       0.007   17 DROP TABLE
------------------------------------------------------------------------
     34.170      32.710      32.959      33.064      32.555      32.042 (total benchmark run time)

</code></pre>
<p>When there are a lot of runs selected, this output can get unreadable as
it will be wider than any terminal.  This default (show one target per
column) can be changed by adding <code>-column benchmark</code> meaning &quot;show one
benchmark per column&quot; (here we omit most columns for illustration):</p>
<pre><code>tclsh tool/benchmark-filter.tcl -backend lmdb -datasize 2 -column benchmark
Column 1: Creating database and tables
Column 2: 2000 INSERTs
Column 3: 200 UPDATEs without an index, upgrading a read-only transaction
Column 4: 50000 INSERTs in a transaction
Column 5: 200 SELECTs without an index
Column 6: 200 SELECTs on a string comparison
Column 7: Creating an index
Column 8: 10000 SELECTs with an index
Column 9: 2000 UPDATEs without an index
Column 10: 50000 UPDATEs with an index
Column 11: 50000 text UPDATEs with an index
Column 12: INSERTs from a SELECT
Column 13: DELETE without an index
Column 14: DELETE with an index
Column 15: A big INSERT after a big DELETE
Column 16: A big DELETE followed by many small INSERTs
Column 17: DROP TABLE
Column 18: Total run duration

    1     2     3     4     5     6     7      8     9    10    11    12    13    14    15    16    17     18 Target
0.010 2.973 0.053 0.158 0.828 2.056 0.068 26.623 0.289 0.347 0.266 0.132 0.126 0.091 0.114 0.026 0.010 34.170 3.37.0+lmdb-0.9.28+datasize-2
0.010 3.113 0.060 0.162 0.881 2.019 0.063 24.999 0.308 0.343 0.261 0.137 0.127 0.083 0.109 0.024 0.011 32.710 3.37.1+lmdb-0.9.28+datasize-2
0.010 2.621 0.057 0.160 0.874 2.059 0.068 25.713 0.307 0.339 0.266 0.116 0.126 0.082 0.127 0.023 0.012 32.959 3.37.2+lmdb-0.9.28+datasize-2
0.009 2.599 0.053 0.157 0.812 2.039 0.059 25.925 0.305 0.343 0.266 0.146 0.127 0.085 0.112 0.017 0.011 33.064 3.37.0+lmdb-0.9.29+datasize-2
0.014 2.842 0.037 0.168 0.900 1.989 0.074 25.114 0.317 0.340 0.268 0.119 0.131 0.094 0.114 0.025 0.010 32.555 3.37.1+lmdb-0.9.29+datasize-2
0.014 2.511 0.056 0.163 0.862 2.035 0.079 24.914 0.309 0.340 0.269 0.142 0.125 0.080 0.116 0.020 0.007 32.042 3.37.2+lmdb-0.9.29+datasize-2
</code></pre>
<p>While this output is still quite wide, it will fit most displays, and does
not grow with the number of runs selected; also, the <code>-benchmarks</code> option
can help by selecting only the columns of interest, for example:</p>
<pre><code>tclsh tool/benchmark-filter.tcl -backend lmdb -datasize 2 -column benchmark -benchmarks 2,8,total
Column 2: 2000 INSERTs
Column 8: 10000 SELECTs with an index
Column 18: Total run duration

    2      8     18 Target
2.973 26.623 34.170 3.37.0+lmdb-0.9.28+datasize-2
3.113 24.999 32.710 3.37.1+lmdb-0.9.28+datasize-2
2.621 25.713 32.959 3.37.2+lmdb-0.9.28+datasize-2
2.599 25.925 33.064 3.37.0+lmdb-0.9.29+datasize-2
2.842 25.114 32.555 3.37.1+lmdb-0.9.29+datasize-2
2.511 24.914 32.042 3.37.2+lmdb-0.9.29+datasize-2
</code></pre>
<p>Note that only &quot;like-for-like&quot; can be compared, the tests with the &quot;datasize 1&quot; option
differ from the tests with &quot;datasize 2&quot; and the tool will not show these side by side.
However, the option <code>-ignore-numbers</code> instructs the tool to ignore numbers in
the test names, so that they can be compared:</p>
<pre><code>tclsh tool/benchmark-filter.tcl 0D18BDC37964B1D01150C160168DF5AB8A7514B20CF4E12170D9935B6682FCE3 5790AB5B8F27AFFFE663A1A69A183B46F0261E40A6BBC8CA3AD541521325C308
Runs 5790AB5B8F27AFFFE663A1A69A183B46F0261E40A6BBC8CA3AD541521325C308 and 0D18BDC37964B1D01150C160168DF5AB8A7514B20CF4E12170D9935B6682FCE3 have different tests

tclsh tool/benchmark-filter.tcl -ignore-numbers 0D18BDC37964B1D01150C160168DF5AB8A7514B20CF4E12170D9935B6682FCE3 5790AB5B8F27AFFFE663A1A69A183B46F0261E40A6BBC8CA3AD541521325C308
Column 1
    Benchmark: sqlite 3.37.2
       Target: 3.37.2
              (3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1)
       Ran at: 2022-02-13 19:00:29
     Duration: 4.207
    Disk time: read: 0.522; write: 2.177

Column 2
    Benchmark: sqlite 3.37.2
       Target: 3.37.2++datasize-2
              (3.37.2 2022-01-06 13:25:41 872ba256cbf61d9290b571c0e6d82a20c224ca3ad82971edc46b29818d5dalt1)
       Ran at: 2022-02-13 19:06:35
     Duration: 9.315
    Disk time: read: 0.405; write: 1.945

--------- TIME --------
          1           2 TEST NAME
      0.007       0.011    1 Creating database and tables
      2.789       5.574    2 # INSERTs
      0.013       0.032    3 # UPDATEs without an index, upgrading a read-only transaction
      0.085       0.147    4 # INSERTs in a transaction
      0.117       0.455    5 # SELECTs without an index
      0.436       1.730    6 # SELECTs on a string comparison
      0.032       0.050    7 Creating an index
      0.066       0.099    8 # SELECTs with an index
      0.061       0.149    9 # UPDATEs without an index
      0.155       0.305   10 # UPDATEs with an index
      0.136       0.235   11 # text UPDATEs with an index
      0.082       0.126   12 INSERTs from a SELECT
      0.076       0.139   13 DELETE without an index
      0.060       0.093   14 DELETE with an index
      0.059       0.120   15 A big INSERT after a big DELETE
      0.022       0.028   16 A big DELETE followed by many small INSERTs
      0.013       0.021   17 DROP TABLE
------------------------
      4.207       9.315 (total benchmark run time)
</code></pre>
<h1 id="full-set-of-options-a-namefull-set-of-optionsa"><a class="header" href="#full-set-of-options-a-namefull-set-of-optionsa">Full set of options <a name="full-set-of-options"></a></a></h1>
<p>The tool accepts a large set of options:</p>
<h2 id="environment"><a class="header" href="#environment">environment</a></h2>
<ul>
<li><code>-database</code> <code>PATH_TO_DATABASE</code>  - the database to read, default is the last database updated by <code>make benchmark</code>, normally <code>benchmarks.sqlite</code></li>
<li><code>-sqlite</code> <code>PATH_TO_SQLITE</code>  - the sqlite3 executable; by default the tool tries to find it either in the LumoSQL build directory or installed on the system</li>
<li><code>-limit</code> <code>N</code>  - limit the output to the most recent <code>N</code> runs which match other criteria; the default is 20</li>
<li><code>-import</code> <code>FILE</code> <code>[FILE]...</code> - instead of using runs in the database, read each <code>FILE</code> (which must have been created using the <code>-export</code> option) into a temporary database, then process the data as normal; if it is desired to import the runs into a permanent database, see the <code>-copy</code> option below; multiple files can be specified, for example <code>-import FILE1 FILE2</code> or <code>-import downloads/data.*</code>; the file names must not start with a <code>-</code> which would be interpreted as the next option; such files can be specified using <code>./-NAME</code></li>
</ul>
<h2 id="selecting-runs"><a class="header" href="#selecting-runs">selecting runs</a></h2>
<p>If more than one selection option is provided, the tool will select runs which match all the criteria; however if the same option is repeated, it selects any which match: so for example <code>-version N</code> <code>-version X</code> <code>-backend B</code> selects all runs with backend <code>B</code> which also used saqlite version <code>N</code> or <code>X</code>.</p>
<ul>
<li><code>RUN_ID</code>  - specifying a run ID (which appears as a long hexadecimal string) means that only that run will be processed; if this option is repeated, it select all the runs listed; the option can be
abbreviated to fewer digits, and the program will look up the full ID</li>
<li><code>-option</code> <code>NAME-VALUE</code> - select runs which used the named option and value in the target</li>
<li><code>-missing</code> <code>NAME</code> - select runs which do not have option <code>NAME</code> recorded in the database as a target option</li>
<li><code>-datasize</code> <code>N</code> - select runs which used the <code>datasize</code> option with value <code>N</code>; this is an abbreviation for <code>option</code> <code>datasize-N</code>; like the <code>datasize</code> option, <code>N</code> could also be two numbers separated by comma</li>
<li><code>-target</code> <code>T</code> - select runs which used the specified target (same syntax as each element of the <code>TARGETS</code> make option)</li>
<li><code>-version</code> <code>V</code> - select runs which used the specified version of sqlite3; this differ from <code>-target</code> as the <code>-version</code> option can select any backend, while <code>-target</code> selects on the full specification of version of sqlite3, backend, options</li>
<li><code>-backend</code> <code>B</code> - select runs which used the specified backend (any version)</li>
<li><code>-backend</code> <code>B-V</code> - select runs which used version <code>V</code> of backend <code>B</code></li>
<li><code>-no-backend</code> - select runs which used an unmodified sqlite (any version, unless <code>-version</code> is also secified).
The <code>-backend</code> and <code>-no-backend</code> options can be combined, and they include anything which matches, so <code>-backend</code> <code>lmdb</code> <code>-no-backend</code> means &quot;select anything with an unmodified sqlite OR the LMDB backend&quot; (but not for example the BDB backend)</li>
<li><code>-failed</code> - select runs which have failed tests</li>
<li><code>-interrupted</code> - select runs in which some tests were interrupted by a signal</li>
<li><code>-completed</code> - select runs in which all tests completed successfully and the run itself recorded an end time</li>
<li><code>-crashed</code> - select runs which have a start time but not an end time: this usually means that the runs have crashed; however a currently running benchmark will also be selected becauase it does not have an end time yet</li>
<li><code>-empty</code> - selects runs with no tests; usually combined with <code>-delete</code> (see below) to clean up the database</li>
<li><code>-invalid</code> - select runs which are invalid for some reason, for example they have test data but not information about the run itself, or the sums don't add up; usually combined with <code>-delete</code> or <code>-add</code> (see below) to clean up the data</li>
<li><code>-cpu-comment</code> <code>PATTERN</code> - select runs whose &quot;cpu comment&quot; matches the
pattern, for example <code>-cpu-comment</code> <code>%amd%</code> would select all benchmarks
running on an AMD processor, assuming the cpu comment was set appropriately
(or left at the default, and the tool could detect the CPU type); if this
option is repeated, select runs which match any of the patterns</li>
<li><code>-disk-comment</code> <code>PATTERN</code> - select runs whose &quot;disk comment&quot; matches the
pattern, for example <code>-disk-comment</code> <code>%nvme%</code> would select runs which stored
the databases on an NVME SSD, assuming the disk comment was set appropriately
(or left at the default, and the benchmark could detect the device); if this
option is repeated, select runs which match any of the patterns</li>
</ul>
<h2 id="output-format"><a class="header" href="#output-format">output format</a></h2>
<p>More than one output format option can appear in the command line, and they all apply,
unless specified otherwise</p>
<ul>
<li><code>-average</code>  - instead of displaying run details, calculates an average of runs with the same properties and displays that instead (currently unimplemented)</li>
<li><code>-list</code>  - list the selected runs, one per line, with no information about the single tests; this is the default if there are no selection options</li>
<li><code>-fields</code> <code>FIELD[,FIELD]...</code> - change the fields printed by <code>-list</code>, default is
<code>RUN_ID,TARGET,DATE,TIME,DURATION</code>; see below for the possible values</li>
<li><code>-summary</code>  - display a summary of each test in each selected run; this only works if the selected runs have the same tests; cannot be combined with <code>-details</code>; this is the default if there are some selection options</li>
<li><code>-quick</code> - similar to summary, but omits the initial test description and just shows the columns of timings: the column headers show the sqlite/backend combination</li>
<li><code>-count</code> - only shows the number of results matching filters</li>
<li><code>-details</code>  - display full details for each test in each selected run including all the information in the database; cannot be combined with <code>-summary</code></li>
<li><code>-column</code> <code>WHAT</code> - what to show in columns, where <code>WHAT</code> is one of:
<code>test</code>, <code>benchmark</code> or <code>target</code> (<code>test</code> and <code>benchmark</code> are considered
equivalent for this option); applies only to <code>-summary</code> and <code>-quick</code></li>
<li><code>-tests</code> <code>LIST</code> (or equivalently <code>-benchmarks</code> <code>LIST</code>) - in the <code>-summary</code>
and <code>-quick</code> output formats, selects which tests are included, by default
without this option they are all shown; the LIST is a comma-separate list
of test/benchmark numbers, as shown in the normal output, or anything starting
with the letter &quot;t&quot; to include the total run duration; this option is
normally used to make the output narrower when using <code>-column</code> <code>test</code>
(or the equivalent <code>-column</code> <code>benchmark</code>)</li>
<li><code>-ignore-numbers</code> - replace all numbers in test names with &quot;#&quot;; this
allows the <code>-summary</code> and <code>-quick</code> output to compare tests which differ
only by numbers, for example because they have different data sizes.</li>
<li><code>-normalise</code> - replaces timings in list, summary and details outputs with a normalised
number calculated as 1000 multiplied by the test time and divided by the total run
time for all tests; this could be used to compare results obtained from similar systems 
but which differ in the base speed by eliminating the variation due to the speed</li>
<li><code>-export</code> <code>FILE</code>  - write the selected runs to <code>FILE</code> in a text format, useful for example to send the run information by email</li>
<li><code>-copy</code> <code>DATABASE</code>  - copies all information about the selected runs to <code>DATABASE</code>;
if the database already exists, it must have the same schema and must not already
contain the selected runs (the database will be created if it does not exist)</li>
</ul>
<p>If no output format options (other than <code>-average</code>) are specified, the default is <code>-list</code> if there are no specific run selection criteria, <code>-summary</code> if there are any criteria.</p>
<h3 id="list-columns"><a class="header" href="#list-columns">list columns</a></h3>
<p>The following entries are valid values for the <code>-field</code> option, selecting which
columns are displayed:</p>
<ul>
<li><code>RUN_ID</code> or <code>ID</code>: the run identifier, a long hexadecimal string which identifies
the run uniquely</li>
<li><code>RUN_ID:abbrev</code> or <code>ID:abbrev</code>: same as <code>RUN_ID</code>, but abbreviated to <code>abbrev</code>
hexadecimal digits (minimum 8)</li>
<li><code>TARGET</code>: the encoded target, for example <code>3.36.0</code> or <code>3.36.0+lmdb-0.9.29</code>;
using this with the build tool allows to repeat the benchmark with exactly the
same options</li>
<li><code>TITLE</code>: a human-readable version of <code>TARGET</code>, for example &quot;sqlite 3.36.0 with
lmdb 0.9.29&quot;</li>
<li><code>SQLITE_NAME</code>: the output of <code>sqlite -version</code></li>
<li><code>DATE</code> and <code>TIME</code>: a representation of the date or time the run started</li>
<li><code>END_DATE</code> and <code>END_TIME</code>: a representation of the date or time the run
completed, or <code>-</code> for runs which did not complete</li>
<li><code>DONE</code>: &quot;YES&quot; or &quot;NO&quot;, depending on whether the run completed or not</li>
<li><code>OK</code>, <code>INTR</code> or <code>FAIL</code>: the count of tests which succeeded, were interrupted
or failed with some error, respectively</li>
<li><code>CPU_TYPE</code> or <code>ARCH</code>: the CPU architecture used to run the tests, for example
<code>x86_64</code>, <code>arm</code> or <code>s390x</code></li>
<li><code>OS_TYPE</code> or <code>OS</code>: the operating system used to run the tests, for example
<code>Linux</code> or <code>NetBSD</code></li>
<li><code>CPU_COMMENT</code> or <code>CPU</code>: a user-provided comment intended to describe the
system used for the benchmark; if not provided, it shows as &quot;-&quot;; note that
the benchmark system will try to detect the CPU if the user did not
provide a comment</li>
<li><code>DISK_COMMENT</code> or <code>DISK</code>: a user-provided comment intended to describe the
storage medium used for the databases during the benchmark, or &quot;-&quot; if not provided;
note that the benchmark system will try to detect the storage device
if the user did not provide a ciomment; this doesn't always succeed, so
the column can still show as &quot;-&quot;</li>
<li><code>DISK_TIME</code> (or <code>DISK_WRITE_TIME</code>): the time taken to write a fixed amount of
data (256 KB) to disk, which may help determining how fast the storage medium was;
older benchmarks don't have this information and it will show as &quot;-&quot;.</li>
<li><code>SQLITE</code> or <code>SQLITE_VERSION</code>: the version of sqlite used in the benchmark;
the special case of BDB, which used its own include sqlite, shows this as an
empty field</li>
<li><code>BACKEND_NAME</code> (or <code>BE_NAME</code>: the name of the backend used (an empty field if this was
unmodified sqlite)</li>
<li><code>BACKEND_VERSION</code> (or <code>BE_VERSION</code>: the version of the backend used (an empty field
if this was unmodified sqlite)</li>
<li><code>N_TESTS</code>: the number of tests in the run, currently 17 for benchmarks;
however future versions of LumoSQL may define more tests</li>
</ul>
<h2 id="extra-actions"><a class="header" href="#extra-actions">extra actions</a></h2>
<ul>
<li><code>-add</code> <code>NAME=VALUE</code> - adds some run information, for example to find older benchmark results which did not include the default value for <code>datasize</code> and to update them to have it, one could specify: &quot;<code>-missing datasize -add option-datasize=1</code>&quot;</li>
<li><code>-delete</code> - delete the selected runs from the database; it is recommended to run the tool with <code>-list</code> instead of <code>-delete</code> first, and/or make a copy of the database before running <code>-delete</code></li>
<li><code>-delete-from</code> <code>DB</code> - delete the selected runs from <code>DB</code>, as opposed to the database used to extract the data from;
for example, if runs were added using <code>-copy</code> <code>DB</code>, then running the same filter with <code>-delete-from</code> <code>DB</code> will
delete the runs again</li>
</ul>
<h2 id="checking-test-results"><a class="header" href="#checking-test-results">checking test results</a></h2>
<p>When running tests (as opposed to benchmarks) the build tool will store the results in a different database and the useful data could be different; one can get a summary of test results with:</p>
<pre><code>tool/benchmark-filter.tcl -database tests.sqlite -list -fields TARGET,DONE,OK,INTR,FAIL
</code></pre>
<p>or have complete information about targets with failed tests:</p>
<pre><code>tool/benchmark-filter.tcl -database tests.sqlite -details -failed
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="table-of-contents-6"><a class="header" href="#table-of-contents-6">Table of Contents</a></h1>
<ol>
<li><a href="statistical_analysis.html#orgc3e34f0">Research questions</a></li>
<li><a href="statistical_analysis.html#orge3f663a">Data</a></li>
<li><a href="statistical_analysis.html#org1fa9c9c">Design of experiment</a>
<ol>
<li><a href="statistical_analysis.html#org4c9f39a">Concepts</a></li>
<li><a href="statistical_analysis.html#orga4c3602">LumoSQL versus SQLite</a></li>
<li><a href="statistical_analysis.html#org4ce47ab">Benchmark data</a></li>
</ol>
</li>
<li><a href="statistical_analysis.html#org5e20a6a">Methods</a>
<ol>
<li><a href="statistical_analysis.html#org9602d2a">General considerations</a></li>
<li><a href="statistical_analysis.html#org3d7cf37">Model matrix</a></li>
</ol>
</li>
<li><a href="statistical_analysis.html#org47694e5">References</a></li>
</ol>
<p><a id="orgc3e34f0"></a></p>
<h1 id="research-questions"><a class="header" href="#research-questions">Research questions</a></h1>
<p>Amassing data is not enough. We need to define the comparisons that
we want to make to design the type of runs suitable for the analysis.</p>
<p>We have <em>many</em> run time measurements produced under varying conditions:</p>
<ul>
<li>Data size</li>
<li>Backend (unmodified vs. lmdb)</li>
<li>SQLite version</li>
<li>lmdb version</li>
</ul>
<p>We have <em>some</em> run time measurements produced under varying
conditions of storage, cpu, and OS version. If we want to address
the impact of these on run times, we'd need to produce new runs.</p>
<p>We have <em>no</em> data to study the effects of these:</p>
<ol>
<li><code>byte-order</code></li>
<li><code>option-debug</code></li>
<li><code>option-lmdb_debug</code></li>
<li><code>option-lmdb_fixed_rowid</code></li>
<li><code>option-lmdb_transaction</code></li>
<li><code>option-rowsum</code></li>
<li><code>option-rowsum_algorithm</code></li>
<li><code>option-sqlite3_journal</code></li>
<li><code>os-type</code></li>
</ol>
<p>These are questions pulled from [1] and or #lumosql at libera.chat</p>
<ul>
<li>[Q1] What happens to performance when LMDB is swapped in as a storage
backend for SQLite?</li>
<li>[Q2] Does SQLite get faster with each version?</li>
<li>[Q3] Does LMDB get faster with each version?</li>
<li>[Q4] Which compile options make a given version of SQLite faster?</li>
<li>[Q5] How do different versions combine to change performance as data size gets large, separately for read and write?</li>
<li>[Q6] Do compile options affect performance differently as data size changes?</li>
<li>[Q7] Does SQLITE_DEBUG really make SQLite run approximately three
times slower, as claimed on sqlite.org?</li>
<li>[Q8] What happens when a given set of compile options, versions and
data size are tested on faster and slower disks?</li>
<li>[Q9] What is the effect of testing reads and writes on different disks,
given that the relative speeds for read and write may differ greatly?</li>
<li>[Q10] meta-benchmarking question: does the &quot;discard output&quot; option to
benchmarking make no significant timing difference, as expected?
(it may prevent benchmarking crashing on giant sizes).</li>
</ul>
<p>Please, list below your questions in the right category. Leave your
name after each question so we can follow up. Consider framing your
questions in terms of <em>how much</em> rather than binary terms (<em>how
much does A differ from B?</em> versus <em>is A higher than B?</em>)</p>
<ul>
<li>Main questions: that should be up front the focus of the immediate
runs and analysis
<ul>
<li>Dan - Q1, Q2, Q3, Q5, Q9</li>
<li>Add question and name here</li>
</ul>
</li>
<li>Peripheric questions: that you would like answer as a side effect
with little to no additional effort involved
<ul>
<li>Dan - Q4, Q6, Q8</li>
<li>Add question and name here</li>
</ul>
</li>
<li>Dream questions: that you would like to answer in the future but
it's out of reach right now
<ul>
<li>What is the performance impact of row checksums?  This will
need to wait until we've reworked the way we store it.
(Uilebheist 20220325)</li>
<li>What is the impact of using a different strategy to implement
sqlite transactions using LMDB transactions?  This is the
<code>option-lmdb_transaction</code> but its effect is only visible
when lots of processes try to run concurrent transaction, and
would require a different experiment from what we've been running
(Uilebheist 20220325)</li>
<li>What difference does it make to use the pre-computed checksum
columns for operations such as selecting the rows which have changed
compared with a traditional method such as a column called &quot;last updated&quot;,
and also a straight SELECT.
(Dan 20220405)</li>
</ul>
</li>
</ul>
<p><a id="orge3f663a"></a></p>
<h1 id="data"><a class="header" href="#data">Data</a></h1>
<ul>
<li>Download from <a href="https://lumosql.org/dist/benchmarks-to-date/">/dist/benchmarks-to-date</a></li>
<li>Start with the <code>all-lumosql-benchmark-data-combined.sqlite</code> file</li>
<li>For records with empty <code>sqliteVersion</code> and <code>backendName = dbd</code>, set
<code>sqliteVersion</code> to the version number in the <code>sqliteTitle</code> string</li>
<li>For records with empty <code>backendVersion</code>, set <code>backendName</code> and
<code>backendVersion</code> to <code>unmodified</code></li>
<li>Subset records with <code>backendVersion</code> matching <code>unmodified|lmdb</code></li>
<li>Subset records with <code>sqliteVersion</code>  matching <code>3\.3[4-8]</code></li>
</ul>
<p><a id="org1fa9c9c"></a></p>
<h1 id="design-of-experiment"><a class="header" href="#design-of-experiment">Design of experiment</a></h1>
<p><a id="org4c9f39a"></a></p>
<h2 id="concepts"><a class="header" href="#concepts">Concepts</a></h2>
<ul>
<li><strong>Control variables:</strong> Experimenter-selected treatment variables where knowledge of
their effect is the primary objective</li>
<li><strong>Environmental variables:</strong> Describe the operating conditions of an experimental
subject/unit/process
<ul>
<li>A <em>blocking factor</em> is a qualitative environmental variables
identifying identical groups of experimental material</li>
<li>A <em>confounding variable</em> is unrecognized by the experimental
but actively affect the mean output of the physical
system. These can mask or exaggerate the effect of a treatment
variable. E.g., active confounding variable that is correlated
with the treatment variable.</li>
</ul>
</li>
<li><strong>Model variables:</strong> Not present in this experiment</li>
<li><strong>Reproducibility variables:</strong> data recorded for reproducibility that is expected to have no
effect, or whose effect is not of interest, on the response</li>
</ul>
<p><a id="orga4c3602"></a></p>
<h2 id="lumosql-versus-sqlite"><a class="header" href="#lumosql-versus-sqlite">LumoSQL versus SQLite</a></h2>
<p>LumoSQL runs SQLite with LMDB for key-value storing (<a href="https://lumosql.org/src/lumosql/file?name=new-doc/web/lumosql.org/releases-downloads.html&amp;ci=tip">flavor 3</a>). We
want to compare runtimes for LumoSQL versus unmodified SQLite,
where the latter acts as a baseline. This analysis focuses on two
configuration sets:</p>
<p><a id="org4ce47ab"></a></p>
<h2 id="benchmark-data"><a class="header" href="#benchmark-data">Benchmark data</a></h2>
<p>The following table contains all the key-values recorded for each
run. We classify them into the following groups:</p>
<ol>
<li>Variables selected by the experimenters whose knowledge of their
effect on running times is the primary objective</li>
<li>Variables not directly selected by the experimenters that can
plausibly have some impact on running times</li>
<li>Variables that are of no interest, and were recorded only for
the purpose of reproducibility or debugging the LumoSQL Build
and Benchmark system, are not expected to impact on running
times</li>
</ol>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> ZV = zero-variance variable (Y = yes), B = Bayes, D = Dan, L = Labhraich. Comments from L: 3* if we change the list of tests, <code>notforking-date</code> and <code>-id</code> will help knowing which list we ran, which could affect timings 3** they may affect timings if <code>tests-fail</code>, <code>-intr</code> are non-zero, <code>tests-ok</code> is not 17</caption>
<colgroup>
<col  class="org-left" />
<col  class="org-left" />
<col  class="org-left" />
<col  class="org-right" />
<col  class="org-right" />
<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">key</th>
<th scope="col" class="org-left">Example value</th>
<th scope="col" class="org-left">ZV</th>
<th scope="col" class="org-right">B</th>
<th scope="col" class="org-right">D</th>
<th scope="col" class="org-right">L</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">backend</td>
<td class="org-left">bdb-18.1.32</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-left">backend-date</td>
<td class="org-left">2014-09-20 06:24:32 UTC</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">3</td>
<td class="org-right">3</td>
<td class="org-right">3</td>
</tr>
<tr>
<td class="org-left">backend-id</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">3</td>
<td class="org-right">?</td>
<td class="org-right">3</td>
</tr>
<tr>
<td class="org-left">backend-name</td>
<td class="org-left">bdb</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-left">backend-version</td>
<td class="org-left">18.1.32</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-left">byte-order</td>
<td class="org-left">littleEndian</td>
<td class="org-left">Y</td>
<td class="org-right">3</td>
<td class="org-right">3</td>
<td class="org-right">3</td>
</tr>
<tr>
<td class="org-left">cpu-comment</td>
<td class="org-left">Skylake IBRS</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">3</td>
<td class="org-right">2</td>
<td class="org-right">2</td>
</tr>
<tr>
<td class="org-left">cpu-type</td>
<td class="org-left">x86<sub>64</sub></td>
<td class="org-left">&#xa0;</td>
<td class="org-right">3</td>
<td class="org-right">2</td>
<td class="org-right">2</td>
</tr>
<tr>
<td class="org-left">disk-comment</td>
<td class="org-left">SATA 7200RPM</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">2</td>
<td class="org-right">2</td>
<td class="org-right">2</td>
</tr>
<tr>
<td class="org-left">disk-read-time</td>
<td class="org-left">0.398953</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">2</td>
<td class="org-right">2</td>
<td class="org-right">2</td>
</tr>
<tr>
<td class="org-left">disk-write-time</td>
<td class="org-left">1.393422</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">2</td>
<td class="org-right">2</td>
<td class="org-right">2</td>
</tr>
<tr>
<td class="org-left">end-run</td>
<td class="org-left">1644063382</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">3</td>
<td class="org-right">3</td>
<td class="org-right">3</td>
</tr>
<tr>
<td class="org-left">notforking-date</td>
<td class="org-left">2022-02-16 09:51:24</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">3</td>
<td class="org-right">3</td>
<td class="org-right">3*</td>
</tr>
<tr>
<td class="org-left">notforking-id</td>
<td class="org-left">e2b80d918b83f129ac47cfc5bd9941a&#x2026;</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">3</td>
<td class="org-right">3</td>
<td class="org-right">3*</td>
</tr>
<tr>
<td class="org-left">option-datasize</td>
<td class="org-left">1</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-left">option-debug</td>
<td class="org-left">off</td>
<td class="org-left">Y</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-left">option-lmdb<sub>debug</sub></td>
<td class="org-left">off</td>
<td class="org-left">Y</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-left">option-lmdb<sub>fixed</sub><sub>rowid</sub></td>
<td class="org-left">off</td>
<td class="org-left">Y</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-left">option-lmdb<sub>transaction</sub></td>
<td class="org-left">optimistic</td>
<td class="org-left">Y</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-left">option-rowsum</td>
<td class="org-left">off</td>
<td class="org-left">Y</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-left">option-rowsum<sub>algorithm</sub></td>
<td class="org-left">sha3<sub>256</sub></td>
<td class="org-left">Y</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-left">option-sqlite3<sub>journal</sub></td>
<td class="org-left">default</td>
<td class="org-left">Y</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-left">os-type</td>
<td class="org-left">Linux</td>
<td class="org-left">Y</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">2</td>
</tr>
<tr>
<td class="org-left">os-version</td>
<td class="org-left">5.10.0-9-amd64</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">2</td>
<td class="org-right">1</td>
<td class="org-right">2</td>
</tr>
<tr>
<td class="org-left">sqlite-date</td>
<td class="org-left">2020-12-01 16:14:00 UTC</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">3</td>
<td class="org-right">3</td>
<td class="org-right">3</td>
</tr>
<tr>
<td class="org-left">sqlite-id</td>
<td class="org-left">1b256d97b553a9611efca188a3d995a&#x2026;</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">3</td>
<td class="org-right">1</td>
<td class="org-right">3</td>
</tr>
<tr>
<td class="org-left">sqlite-name</td>
<td class="org-left">3.35.5 2021-04-19 18:32:05 1b25&#x2026;</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">3</td>
<td class="org-right">1</td>
<td class="org-right">3</td>
</tr>
<tr>
<td class="org-left">sqlite-version</td>
<td class="org-left">3.35.5</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-left">target</td>
<td class="org-left">3.35.5</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">3</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-left">tests-fail</td>
<td class="org-left">0</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">3</td>
<td class="org-right">3</td>
<td class="org-right">3**</td>
</tr>
<tr>
<td class="org-left">tests-intr</td>
<td class="org-left">0</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">3</td>
<td class="org-right">3</td>
<td class="org-right">3**</td>
</tr>
<tr>
<td class="org-left">tests-ok</td>
<td class="org-left">17</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">3</td>
<td class="org-right">3</td>
<td class="org-right">3**</td>
</tr>
<tr>
<td class="org-left">title</td>
<td class="org-left">sqlite 3.35.5</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">3</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-left">when-run</td>
<td class="org-left">1644063269</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">3</td>
<td class="org-right">3</td>
<td class="org-right">3</td>
</tr>
<tr>
<td class="org-left">word-size</td>
<td class="org-left">8</td>
<td class="org-left">&#xa0;</td>
<td class="org-right">3</td>
<td class="org-right">1</td>
<td class="org-right">3</td>
</tr>
</tbody>
</table>
<p><a id="org5e20a6a"></a></p>
<h1 id="methods"><a class="header" href="#methods">Methods</a></h1>
<p><a id="org9602d2a"></a></p>
<h2 id="general-considerations"><a class="header" href="#general-considerations">General considerations</a></h2>
<ul>
<li>Some systems are faster than others, need to account for that
(Labhraich 20220316)</li>
<li>Consider normalizing test timings by the total run time (Labhraich
<ol>
<li></li>
</ol>
</li>
</ul>
<p><a id="org3d7cf37"></a></p>
<h2 id="model-matrix"><a class="header" href="#model-matrix">Model matrix</a></h2>
<p><code>log(realTime) ~ backendName * backendVersion + diskComment + optionDatasize + sqliteVersion + (1|cpuComment) + (1|osVersion)</code></p>
<p>Notes on zero-variance variables: these are excluded because they
have no variability in the collected data</p>
<ol>
<li><code>byte-order</code></li>
<li><code>option-debug</code></li>
<li><code>option-lmdb_debug</code></li>
<li><code>option-lmdb_fixed_rowid</code></li>
<li><code>option-lmdb_transaction</code></li>
<li><code>option-rowsum</code></li>
<li><code>option-rowsum_algorithm</code></li>
<li><code>option-sqlite3_journal</code></li>
<li><code>os-type</code></li>
</ol>
<p>Notes on control variables:</p>
<ol>
<li>Include the interaction between <code>backend-name</code> and
<code>backend-version</code> if there is interest in the latter main
effect. Drop <code>backend</code>, which is 1-1 to the interaction.</li>
<li>Include <code>diskComment</code>. <code>diskReadTime</code> and <code>DiskWriteTime</code> are
correlated with <code>diskComment</code>, no need to have the times in the
model.</li>
<li>Include <code>option-datasize</code> as categorical because the effect
might be non-lineal</li>
<li>Include <code>sqlite-version</code></li>
</ol>
<p>Notes on environmental variables:</p>
<ol>
<li>Include <code>cpuComment</code> for blocking, drop <code>cpuType</code>. There is only
one <code>cpuComment</code> with <code>cpuType=armv7l</code></li>
<li>Include <code>osVersion</code> for blocking, drop <code>osType</code></li>
</ol>
<p>Notes on reproducibility variables: these are excluded from model
as they were <span class="underline">not</span> recorded for their relevance with respect to the
question but for reproducibility or debugging purposes.</p>
<p>Notes on pending variables: their role in the model matrix is still
TBD</p>
<ol>
<li>All control variables not commented on.</li>
</ol>
<h2 id="other-considerations"><a class="header" href="#other-considerations">Other considerations</a></h2>
<ul>
<li>Comments from gabby_bch (2022-04-05)
<ul>
<li>error bar runs: 5.4.0.91-generic, 5.15.29</li>
<li>big error bars: 5.13.0-22-generic</li>
<li>sqlite old normal : 5.4.0-100-generic</li>
<li>anomaly: 5.4.0.104-generic</li>
<li>unmodified superior to lmdb: 5.10.92, 5.10.100</li>
<li>lmdb superior to unmodified: 5.15.24</li>
<li>compare cpu influence: 5.10.91, 5.10.92</li>
<li>raspberry pi: 5.10.63</li>
</ul>
</li>
</ul>
<p><a id="org47694e5"></a></p>
<h1 id="references"><a class="header" href="#references">References</a></h1>
<ol>
<li><a href="https://lumosql.org/src/lumosql/doc/trunk/doc/lumo-build-benchmark.md">LumoSQL Build and Benchmark System</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><!-- Copyright 2021 The LumoSQL Authors, see LICENSES/MIT -->
<!-- SPDX-License-Identifier: MIT -->
<!-- SPDX-FileCopyrightText: 2020 The LumoSQL Authors -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Dan Shearer, August 2020 -->
<h1 id="lumosql-minimal-at-rest-discretionary-access-control-system"><a class="header" href="#lumosql-minimal-at-rest-discretionary-access-control-system">LumoSQL Minimal At-Rest Discretionary Access Control System</a></h1>
<p>Role
Authority
Privilege
Subjects
Objects</p>
<h1 id="how-it-compares"><a class="header" href="#how-it-compares">How it compares</a></h1>
<ul>
<li>RBAC is not centrally administered by default ...</li>
<li>A minimal subset of DAC implemented ...</li>
<li>Privileges default to all for every user, because an embedded ...</li>
</ul>
<p>Nobody else does at-rest fine-grained control</p>
<p>Not policies. Not columns. Not nested roles. Limited privileges, subjects and objects.</p>
<h1 id="definitions"><a class="header" href="#definitions">Definitions</a></h1>
<p>A  role is a database entity that is used to group a combination of authorities 
and/or privileges together so they can be simultaneously granted or revoked. 
When roles are used, the assignment of authorities and privileges is greatly 
simplified. For example, instead of granting the same set of authorities and 
privileges to every individual in a particular job function, you can assign a set 
of authorities and privileges to a role that represents the job and then grant 
membership in that role to every user who performs that particular job. It is 
important to note that only users with SECADM authority are allowed to 
create roles (by executing the CREATE ROLE SQL statement)</p>
<h1 id="the-lumosql-security-design-and-implementation"><a class="header" href="#the-lumosql-security-design-and-implementation">The LumoSQL Security Design and Implementation</a></h1>
<p>SQLite is an embedded database, and traditionally there has been no need of
access security in the main SQLite use case. The application took care of any
security needs, with a little-used password system for database access provided
by the C API. However once connected, there is little distinction between a
superuser or other users, and no encryption. Various solutions exist that
modify the SQLite source code to give one kind of encryption or another, as
documented in the 
<a href="https://lumosql.org/src/lumodoc/doc/tip/doc/lumo-relevant-knowledgebase.md#list-of-relevant-sql-checksumming-related-knowledge">LumoSQL Knowledgebase</a>.</p>
<p>The LumoSQL approach is completely different. LumoSQL recognises that the
security and privacy requirements of the 21st century are very different to the
primary use case of SQLite over more than two decades, and so implements a
fairly complete security system, recognisable to anyone who has used one of the
mainstream networked SQL databases.</p>
<p>On the other hand, SQLite's strength is its simplicity and the things that come
with that including speed, compact code and relative ease of security review.
An example of how security models can become so complex they are impossible to verify is provided by Microsoft SQL Server, whose 
<a href="https://raw.githubusercontent.com/Microsoft/sql-server-samples/master/samples/features/security/permissions-posters/Microsoft_SQL_Server_2017_and_Azure_SQL_Database_permissions_infographic.pdf">Chart of Database Access Permissions</a> defies analysis. 
Similar charts can be drawn up for Oracle, DB2 et al, with undoubted artistic merit. These databases need to solve very different problems to LumoSQL, and they must do it in a way that is compatible with decades-old security design. LumoSQL does not have these constraints.</p>
<h1 id="goals"><a class="header" href="#goals">Goals</a></h1>
<p>LumoSQL aims to deliver:</p>
<ul>
<li>At-rest data encryption at the database, table and row level</li>
<li>Per-row integrity controls (eg checksums) stored in each row</li>
<li>Per-row encryption, self-contained within each row</li>
<li>Per-row RBAC, self-contained within each row</li>
</ul>
<p>LumoSQL will avoid:</p>
<ul>
<li>Large amounts of security metadata</li>
<li>Mandating a key management system</li>
<li>Introducing new encryption technologies</li>
<li>A large or complex security model</li>
</ul>
<h1 id="design-principles"><a class="header" href="#design-principles">Design Principles</a></h1>
<ol start="0">
<li>LumoSQL data will be secure when at rest. This is consistent with the embedded use case, when databases are often at rest and often copied without knowledge of the application.</li>
<li>Security will be defined by the LumoSQL data, and travel with the data at all times.</li>
<li>Anyone with a valid key can read the data according to the LumoSQL security specification, using any conformant software.</li>
<li>Users may implement any (or no) key authority system. This is consistent with the way SQLite is used.</li>
<li>LumoSQL will define a standard key management system for anyone wanting to use it.</li>
<li>The fundamental unit for LumoSQL security is the encrypted row. Although database and tables can also be encrypted.</li>
</ol>
<h2 id="lumosql-is-a-hybrid-access-control-system"><a class="header" href="#lumosql-is-a-hybrid-access-control-system">LumoSQL is a Hybrid Access Control System</a></h2>
<p>All SQL database security models implement <a href="https://en.wikipedia.org/wiki/Discretionary_access_control">Discretionary Access Control</a>. The traditional embedded use case for SQLite is already well-suited to implementing <a href="https://en.wikipedia.org/wiki/Mandatory_access_control">Mandatory Access Control</a> on end users, because the application has ultimate control. LumoSQL will not implement Mandatory Access Control.</p>
<p>Within Discretionary Access Control there are different models and tradeoffs.
LumoSQL implements features from some of the most common approaches.</p>
<pre><code class="language-pikchr indent toggle source-inline">Frame: [
B: box invis 
line invis &quot;LumoSQL&quot; bold color purple from 0.5cm s of B.w to 0.5cm s of B.e
C1: circle color black rad 2cm at 0.8cm n of B.c
line invis &quot;Role-Based&quot; &quot;Access Control&quot; &quot;(RBAC)&quot; from 1cm n of C1.w to 1cm n of C1.e
C2: circle color black rad 2cm at 1.5cm sw of B.c
line invis &quot;Any or No&quot; &quot;Key Authority&quot; from 0.2cm s of C2.w to 0.2cm s of C2.c
C3: circle color black rad 2cm at 1.5cm se of B.c
line invis &quot;At-rest&quot; &quot;Encryption&quot; from C2.e to C3.e
Title: line invis &quot;Discretionary Access Control&quot; &quot;Systems&quot; color green from 4.2cm above C2.w to 4.2cm above C2.e
]
Border: box color gray rad 1cm thin width Frame.width+1cm height Frame.height+1cm at Frame.center
</code></pre>
<h1 id="lumosql-is-a-minimal-access-control-system"><a class="header" href="#lumosql-is-a-minimal-access-control-system">LumoSQL is a Minimal Access Control System</a></h1>
<p>LumoSQL implements the smallest possible security model that will satisfy the
goals. Nevertheless there are some features that are not implemented by any
other mainstream database. LumoSQL security aims to be verifiable.</p>
<pre><code class="language-pikchr indent toggle source-inline">B:  dot invis at (0,0)
Title: box invis &quot;LumoSQL Compared With Typical SQL Access Control&quot; ljust color purple at B+(1cm,0.4cm)

circle invis color green rad 0.4cm &quot;✔&quot; big big big bold at B+(0.6cm,-0.3cm)
box invis &quot;Object types: Yes, but only database, table and row&quot; ljust fit

circle invis color green rad 0.4cm &quot;✔&quot; big big big bold at 0.5cm below last circle
box invis &quot;At-rest encryption for all object types: Yes.&quot; ljust fit

circle invis color green rad 0.4cm &quot;✔&quot; big big big bold at 0.5cm below last circle
box invis &quot;Simple defaults: Yes. For a new database, a user has all privileges by default, which can be selectively tightened&quot; ljust fit

circle invis color green rad 0.4cm &quot;✔&quot; big big big bold at 0.5cm below last circle
box invis &quot;RBAC: Yes. Read/Write access for each object, enforced by the object being encrypted&quot; ljust fit

circle invis color green rad 0.4cm &quot;✔&quot; big big big bold at 0.5cm below last circle
box invis &quot;Portability and compatibility: Yes. Encrypted LumoSQL rows can be imported into other databases and verified there, or even decrypted if they have the key&quot; ljust fit

circle invis color green rad 0.4cm &quot;✔&quot; big big big bold at 0.5cm below last circle
box invis &quot;Distributed and Decentralised Key Management: Yes. Users can choose to use the optional blockchain-based LumoSQL key management system&quot; ljust fit

circle invis color red rad 0.4cm &quot;✘&quot; big big big bold at 0.5cm below last circle
box invis &quot;Table Policies: No.&quot; ljust fit

circle invis color red rad 0.4cm &quot;✘&quot; big big big bold at 0.5cm below last circle
box invis &quot;Column security and encryption: No.&quot; ljust fit

circle invis color red rad 0.4cm &quot;✘&quot; big big big bold at 0.5cm below last circle
box invis &quot;Complex role definitions: No. A user is either a superuser or not, or in a group or not, and a group has read or write access, or not. That's it.&quot; ljust fit

circle invis color red rad 0.4cm &quot;✘&quot; big big big bold at 0.5cm below last circle
box invis &quot;Inherit privileges: No.&quot; ljust fit

circle invis color red rad 0.4cm &quot;✘&quot; big big big bold at 0.5cm below last circle
box invis &quot;Predefined key management: No. Users can implement any key management system they choose, in the usual SQLite philosophy&quot; ljust fit

circle invis color red rad 0.4cm &quot;✘&quot; big big big bold at 0.5cm below last circle
box invis &quot;Network security: No. LumoSQL is an embedded database and needs no network security code.&quot; ljust fit

circle invis color red rad 0.4cm &quot;✘&quot; big big big bold at 0.5cm below last circle
box invis &quot;Transport security: No. Any plain-text transport may be used to move LumoSQL data that is encrypted at rest.&quot; ljust fit


</code></pre>
<h1 id="lumosql-encrypts-three-kinds-of-objects"><a class="header" href="#lumosql-encrypts-three-kinds-of-objects">LumoSQL Encrypts Three Kinds of Objects</a></h1>
<p>This diagram illustrates the way that any or all of the three layers of at-rest
encryption can be active at once, and their different scopes. </p>
<pre><code class="language-pikchr indent toggle source-inline">      fill = bisque
      linerad = 15px
      leftmargin = 2cm

      define diamond { \
        box wid 150% invis
        line from last.w to last.n to last.e to last.s close rad 0 $1
      }

      oval &quot;SUBMIT TICKET&quot; width 150%
      down
      arrow 50%
NEW:  file &quot;New bug ticket&quot; &quot;marked \&quot;Open\&quot;&quot; fit
      arrow same
      box &quot;Triage,&quot; &quot;augment &amp;&quot; &quot;correct&quot; fit
      arrow same
DC:   box &quot;Developer comments&quot; fit
      arrow same
FR:   box &quot;Filer responds&quot; fit
      arrow 100%
REJ:  diamond(&quot;Reject?&quot;)
      right
      arrow 100% &quot;Yes&quot; above
      box &quot;Mark ticket&quot; &quot;\&quot;Rejected\&quot; &amp;&quot; &quot;\&quot;Resolved\&quot;&quot; fit with .w at previous.e
      arrow right 50%
REJF: file &quot;Rejected&quot; &quot;ticket&quot; fit
      arrow right 50%
REOP: diamond(&quot;Reopen?&quot;)
      down
REJA: arrow 75% from REJ.s &quot;  No; fix it&quot; ljust
CHNG: box &quot;Developer changes code&quot; with .n at last arrow.s fit
      arrow 50%
FIXD: diamond(&quot;Fixed?&quot;)
      right
FNO:  arrow &quot;No&quot; above
RES:  box &quot;Optional:&quot; &quot;Update ticket resolution:&quot; &quot;\&quot;Partial Fix\&quot;, etc.&quot; fit
      down
      arrow 75% &quot;  Yes&quot; ljust from FIXD.s
      box &quot;Mark ticket&quot; &quot;\&quot;Fixed\&quot; &amp; \&quot;Closed\&quot;&quot; fit
      arrow 50%
RESF: file &quot;Resolved ticket&quot; fit
      arrow same
END:  oval &quot;END&quot;

      line from 0.3&lt;FR.ne,FR.se&gt; right even with 0.25 right of DC.e then up even with DC.e then to DC.e -&gt;

      line from NEW.w left 0.5 then down even with REJ.w then to REJ.w -&gt;

      line from RES.e right 0.3 then up even with CHNG.e then to CHNG.e -&gt;

      line from REOP.s &quot;No&quot; aligned above down 0.4
      line from previous.s down to (previous.s, RESF.e) then to RESF.e -&gt;

      line from REOP.n &quot;Yes&quot; aligned below up 0.3
      line from previous.n up even with 0.6&lt;FR.ne,FR.se&gt; then to 0.6&lt;FR.ne,FR.se&gt; -&gt;


</code></pre>
<h1 id="lumosql-rbac-permissions-system"><a class="header" href="#lumosql-rbac-permissions-system">LumoSQL RBAC Permissions System</a></h1>
<p><a href="https://en.wikipedia.org/wiki/Role-based_access_control">Role-based Access
Control</a> (RBAC) is the
way that online relational databases make sure that only authorised users can
access information. The SQL standard has the concept of &quot;roles&quot;, rather like
job titles. As a simple example, someone with &quot;engineering cadet&quot; role will see
different things in the database to someone with the &quot;sales manager&quot; role. RBAC
gets complicated, because people in organisations will often be in several
different roles, and roles can have different levels of privilege. Privileges
are things like &quot;read-only access&quot; and &quot;full read/write and edit&quot; access, and
&quot;allowed to create new roles&quot;.</p>
<p>A full RBAC implementation covering the many dozens of combinations of RBAC
possibilities is far outside the LumoSQL goals described above.</p>
<h2 id="existing-sqlite-permission-schemes"><a class="header" href="#existing-sqlite-permission-schemes">Existing SQLite Permission Schemes</a></h2>
<p>SQLite has a <a href="https://www.sqlite.org/src/doc/trunk/ext/userauth/user-auth.txt">user authentication extension</a>
which provides some basic access control, and seems to be quite rarely used. If
you have a valid username and a password, then the SQLite API will allow a
database connection. All users are equivalent, except that an admin user can
modify the contents of the new system table sqlite_user . All access is by
means of the C API. Anyone with the SQLite database file can open it and make
changes regardless of user authentication, because there is no encryption.
Various proprietary products such as
<a href="https://www.sqlite.org/see/doc/release/www/index.wiki">SEE</a> or
<a href="https://www.zetetic.net/sqlcipher/">SQLCipher</a> can encrypt the entire SQLite
database, which works to some extent with SQLite user authentication.</p>
<h1 id="lumosql-rbac-goals-and-constraints"><a class="header" href="#lumosql-rbac-goals-and-constraints">LumoSQL RBAC Goals and Constraints</a></h1>
<p>LumoSQL RBAC aims to provide:</p>
<ul>
<li>
<p>Compatibility with existing SQLite binaries, when using the SQLite native db
format. If this means losing some more advanced features, that is acceptable
although regrettable. There is a big difference between the statements &quot;using
the native SQLite db format&quot;, and &quot;using the native SQLite BTree backend&quot;.  In
the latter case there is no more consrtaint with BTree than with LMDB or
anything else. In the former case, nothing can be stored in the binary image
that standard SQLite is not expecting, and this is a constraint.</p>
</li>
<li>
<p>Access control from SQL statements, with a similar user interface to the
major online databases, as described in this document</p>
</li>
<li>
<p>Fine-grained access control user interface per table, also with a similar
user interface to the major online databases</p>
</li>
<li>
<p>Ultra fine-grained per-row runtime access control</p>
</li>
<li>
<p>Ultra fine-grained at-rest access control, unique in the world of relational databases</p>
</li>
<li>
<p>Access control from SQLite pragmas, dot commands and commandline, however this will probably
always be a subset of the user interface exposed via SQL</p>
</li>
</ul>
<p>This point above of &quot;Ultra fine-grained at-rest access control&quot; means that,
even if someone has a LumoSQL database in their possession, they will only be
able to acces the specific rows in the specific tables to which they have a
valid username and password, or other such cryptographic key. This is really
unprecedented in relational databases.</p>
<p>The high-level design of the permissions system is taken from Postgres version 14:</p>
<ul>
<li>LumoSQL will implement a small but strict subset of Postgres permissions</li>
<li>LumoSQL will extend the Postgres design only in that it can apply per-row</li>
<li>These comments relate only to the design as exposed in SQL statements</li>
</ul>
<p>LumoSQL RBAC will be built in to the main code. It cannot be an extension
because of the fine-grained nature of the design. RBAC builds on the existing
per-row checksum code in LumoSQL.</p>
<p>LumoSQL RBAC does not require support from the storage subsystem, ie whether
the native BTree or LMDB or some other key-value storage system, RBAC will
still work because we are storing rows as opaque encrypted blocks.</p>
<p>The above must necessarily be implemented in a different way to any existing
database,  even though the SQL user interface is similar to others and entirely
derived from Postgres. The unique functionality of at-rest security and RBAC
means that the data must be stored as opaque rows, and while there will always
be RBAC-specific metadata stored in LumoSQL tables, it is possible and expected
that a single row's encrypted data can be exported and sent elsewhere, and the
RBAC remains functional to anyone with the correct key.</p>
<p>An example of how implementation must be different is that LumoSQL cannot have
an equivalent to the Postgres BYPASSRLS role attribute to bypass Row Level
Security, because for most operations nobody can bypass LumoSQL per-row RBAC.
If you don't have the key, a row is a block of bits with a little metadata.</p>
<p>Another example is that, conversely, everyone who can read a LumoSQL database
has permissions to dump all rows or at least all tables, because being an
embedded library it is guaranteed we can access the file. However the encrypted
tables or rows will never be plain text without the key, even for a dump
operation.</p>
<h1 id="interfaces-to-lumosql-security-system"><a class="header" href="#interfaces-to-lumosql-security-system">Interfaces to LumoSQL Security System</a></h1>
<p>The primary means of interacting with the security system is via the SQL commands.</p>
<p>For interacting with database objects (that is, whole database encryption) the
SQLite <a href="https://www.sqlite.org/see/doc/trunk/www/readme.wiki">Security Encryption Extension C API</a> is used, with some
of the key/rekey details modified for LumoSQL's different range of ciphers.</p>
<h2 id="enabling-and-disabling-row-level-permissions"><a class="header" href="#enabling-and-disabling-row-level-permissions">Enabling and Disabling Row-level Permissions</a></h2>
<p>Per-row access control is enabled by changing the definition as per the
<a href="https://www.postgresql.org/docs/14/sql-altertable.html">Postgres ALTER TABLE command</a>. ALTER TABLE
ENABLE/DISABLE refer to whether or not it is possible to use per-row security
in this table, not whether or not the feature is actually used. This matters when, for example,
we want to quickly determine if any rows <em>might</em> be protected in a database. If none of the tables 
have row level security enabled then there should not be any encrypted rows.</p>
<p>Example: ALTER TABLE foobar ENABLE ROW LEVEL SECURITY;</p>
<p>The way <a href="https://www.sqlite.org/schematab.html">SQLite implements the schema</a>
is helpful for implemnting per-row security. It differs from other databases
due to the nature of many embedded use cases. See the heading 
<a href="https://www.sqlite.org/lang_altertable.html">&quot;Why ALTER TABLE is such a problem for SQLite&quot;</a>,
which explains why SQLite stores the schema as the literal plain text CREATE
TABLE statements. This gives LumoSQL a lot more freedom to improve the
internals of row-based security without impacting on backwards compatibility
with other versions of LumoSQL.</p>
<h2 id="roles"><a class="header" href="#roles">Roles</a></h2>
<p>Adapted from <a href="https://www.postgresql.org/docs/14/user-manag.html">roles in Postgres 14</a>.</p>
<p>Roles take the place of users and groups. They can be called anything.</p>
<p>Roles have attributes. LumoSQL has only two attributes: LOGIN and
SUPERUSER (see https://www.postgresql.org/docs/13/role-attributes.html )
A superuser has complete access. A login user has default access,
and can have access added to or subtracted from the default access level.</p>
<p>Roles are managed with CREATE/DROP/ALTER ROLE statements, except
for defining access privileges in a detailed way.</p>
<p>Example: &quot;CREATE ROLE admins SUPERUSER&quot;</p>
<h2 id="privileges"><a class="header" href="#privileges">Privileges</a></h2>
<p>Adapted from <a href="https://www.postgresql.org/docs/14/ddl-priv.html">privileges in Postgres 14</a>.</p>
<p>LumoSQL privileges are hard-coded and cannot be extended.</p>
<p>Some privileges are assigned to roles and objects by default.</p>
<p>The complete list of LumoSQL privileges is:</p>
<ul>
<li>SELECT</li>
<li>UPDATE</li>
<li>INSERT</li>
<li>DELETE</li>
<li>CREATE</li>
</ul>
<p>The UPDATE and DELETE privileges imply SELECT. CREATE only applies to tables.
The keyword &quot;ALL&quot; is also allowed.</p>
<p>Note: In LumoSQL, SUPERUSER privileges cover much more than ALL, and these
additional privileges superuser has cannot be assigned in any way.</p>
<p>Default privileges of ALL are assigned to all roles for 
schema objects other than DATABASE and TABLE (and therefore 
for rows within each table.) This may need to be changed in
future versions of the LumoSQL per-row RBAC.</p>
<h2 id="granting-membership-to-a-role"><a class="header" href="#granting-membership-to-a-role">Granting membership to a Role</a></h2>
<p>This is one of two very different uses of and meanings for the GRANT statement, and is
adapted from <a href="https://www.postgresql.org/docs/14/sql-grant.html">GRANT in Postgres 14</a>. 
This usage is specific to roles, which typically means users such as an identifiable human or program.</p>
<p>The GRANT command grants membership in a role to one or more other
roles. Membership in a role is significant because it conveys the
privileges granted to a role to each of its members.</p>
<p>Only SUPERUSER can grant membership in roles.</p>
<p>Example: GRANT admins TO svetlana;</p>
<p>This means role svetlana is now a member of the role admins.</p>
<p>REVOKE undoes GRANT.</p>
<h2 id="granting-permissions-to-an-object"><a class="header" href="#granting-permissions-to-an-object">Granting permissions to an Object</a></h2>
<p>This is the second use for the GRANT statement, specific to database objects.</p>
<p>There are three objects that can be assigned permissions:</p>
<ul>
<li>DATABASE</li>
<li>TABLE</li>
<li>TABLE.ROW</li>
</ul>
<p>Nothing else in a schema can have its permissions assigned or changed. All
other objects are accessible with ALL privileges at all times, no matter who
the user is, or no identified user at all. This is the nature of an embedded system,
where we expect the user who can open a database will always have permissions to </p>
<p>Examples: GRANT INSERT ON some_table TO nikita;
GRANT ALL ON some_table TO nikita WHERE ROWID=54321;</p>
<p>REVOKE undoes GRANT. REVOKing table permissions does not revoke
row permissions, or vice versa.</p>
<h2 id="unresolved-questions"><a class="header" href="#unresolved-questions">Unresolved Questions</a></h2>
<ul>
<li>What does it mean when a binary encrypted row is dumped? From the <a href="https://sqlite.org/cli.html">SQLite documentation</a>:</li>
</ul>
<p>: Use the &quot;.dump&quot; command to convert the entire contents of a database into a single UTF-8 text file. </p>
<p>We cannot turn an encrypted row into UTF-8 text without the key, and we cannot control who uses the dump command.</p>
<p>For example a dump session might look like:</p>
<pre><code>
$ lumosql rbac-example-db.lumo
@lumosql&gt; .dump
    :
INSERT INTO &quot;my_table&quot; VALUES(X'AB12340CDE.... 12234DEF');

</code></pre>
<ul>
<li>Related to the above, but to be very clear, an encrypted row is not another datatype, and the BLOB affinity cannot help here.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="encryption"><a class="header" href="#encryption">Encryption</a></h1>
<ol>
<li>LumoSQL open source encryption to replace <a href="https://www.zetetic.net/sqlcipher/">SQLCipher</a>/<a href="https://www.hwaci.com/sw/sqlite/see.html">SEE</a>. The authors of SQLite produce the closed-source SQLite Encryption Extension (SEE). There are several open source solutions to replace SEE, the most popular of which is SQLCipher, which has an open source version with limited functionality. Closed source encryption is not acceptable for privacy or security in the 21st century. The first version of this will be exposing the page-level crypto in LMDBv1.0 via the new LumoSQL crypto API, with metadata handled by the new pragmas.</li>
</ol>
<p>LumoSQL will also support LMDBv1.0 page-level encryption.</p>
<div style="break-before: page; page-break-before: always;"></div><!-- SPDX-License-Identifier: MIT -->
<!-- SPDX-FileCopyrightText: 2022 The LumoSQL Authors, see LICENSES/MIT -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Dan Shearer, December 2021 -->
<h1 id="about-lumions"><a class="header" href="#about-lumions">About Lumions</a></h1>
<p>The privacy and security features of <a href="https://lumosql.org/src/lumosql">LumoSQL</a>
are based on each row in a database being a portable binary blob
that is encrypted and signed.  We added the optional facility for each blob to
have within it different levels of access to the data, in what is often called
RBAC/Role-based Access Control, or <a href="https://en.wikipedia.org/wiki/Attribute-based_access_control">Attribute-based Access Control</a>.</p>
<p>We soon realised this our specification for a database row should be a single
global standard for transport and storage of any data, and that there was no
existing standard. The security and privacy requirements of the 21st century
demand something like this, so we created it.</p>
<p>From our nascent draft RFC:</p>
<blockquote>
<blockquote>
<p><em>To illustrate the main use case, Lumions would not be needed if JSON had
sophisticated privacy and encryption features, with a single unique JSON
namespace and a standard way of referring to other JSON objects.</em></p>
</blockquote>
</blockquote>
<p>This is a collaboration involving the <a href="https://lumosql.org/">LumoSQL team</a>, the
<a href="http://www.etrovub.be/">Department of Electronics and Informatics</a> of the
<a href="https://www.vub.be/">Vrije Universiteit Brussel</a> and others. The VUB is providing the 
mathematical heavy lifting for the concept of securely updating binary blobs.</p>
<h1 id="lumion-rfc-specification"><a class="header" href="#lumion-rfc-specification">Lumion RFC Specification</a></h1>
<p>This directory contains the early Lumion specification. We are maintaining it using
IETF tools for RFC generation because we think it should become an RFC.  This
RFC draft is the only specification for Lumions, and is the reference for
LumoSQL and other software we write that handle Lumions in various ways.</p>
<p>The draft RFC <a href="draft-shearer-desmet-calvelli-lumionsrfc-00.txt">draft-shearer-desmet-calvelli-lumionsrfc-00.txt</a> is 
taking shape. We have many references to add and other knowledge we have gained
through implementation. This is going to be a long journey but we have started.</p>
<p>We maintain the text in the markdown file
<a href="draft-shearer-desmet-calvelli-lumionsrfc-00.html">draft-shearer-desmet-calvelli-lumionsrfc-00.md</a>,
with IETF-specific markdown extensions as described in
<a href="https://github.com/danyork/writing-internet-drafts-in-markdown/">Dan York's RFC Markdown Examples</a>.</p>
<p>The IETF does not support markdown, their specification is entirely in XML.
The Lumions RFC uses a pre-processing tool called mmark to read IETF-specific
markdown and produce IETF-compatible XML. The IETF has a tool called xml2rfc
that will emit either a standard .txt file (similar to all RFCs for the last
half century) or a pdf.</p>
<h1 id="toolchain"><a class="header" href="#toolchain">Toolchain</a></h1>
<p>The Lumion RFC is maintained in Markdown, as specified for and processed by the
<a href="https://github.com/mmarkdown/mmark">mmark IETF Markdown tool</a> tool, which
tracks the RFC file format specification v3, as per the
<a href="https://datatracker.ietf.org/doc/html/rfc7991">draft RFC 7991</a>.</p>
<p>The only mmark dependency is the python tool xml2rfc. Always use the <a href="https://pypi.org/project/xml2rfc/">xml2rfc version number used by Pypi</a> even if you do not use &quot;pip install&quot;, because that is what <a href="https://mmark.miek.nl/post/faq/#what-version-of-xml2rfc-is-supported">mmark defines as &quot;latest version&quot;</a>.</p>
<p>The Pipy version of xml2rfc approximately tracks the [official IETF
repo](https://svn.ietf.org/svn/tools/xml2rfc/trunk xml2rfc) which is maintained
by the <a href="https://xml2rfc.tools.ietf.org/">comprehensive IETF project</a>. This
project is formalising a 50 year-old file format with great care.</p>
<p>To create the Lumoion RFC from the markdown:</p>
<ul>
<li>&quot;pip install xml2rfc&quot;, or use some other installation method that yields a version &gt;= Pypi. Older operating systems will not give a good version via &quot;pip&quot;, so either learn about pip or change OS version.</li>
<li>Install the Go language (often called &quot;golang&quot; in package repositories)</li>
<li>git clone https://github.com/mmarkdown/mmark ; cd mmark</li>
<li>go get &amp;&amp; go build</li>
<li>./mmark -version     &lt;-- test the binary</li>
<li>cd rfc ; make        &lt;-- this should rebuild the .txt files for the sample RFCs</li>
</ul>
<p>Test the toolchain for the Lumion RFC:</p>
<ul>
<li>copy the file draft-shearer-desmet-calvelli-lumionsrfc-00.md to mmark/rfc</li>
<li>make </li>
</ul>
<p>If this generates draft-shearer-desmet-calvelli-lumionsrfc-00.txt then your
toolchain is working. Change paths etc to your taste.</p>
<p>You may wish to try &quot;make pdf&quot;. xml2rfc will reliably tell you what additional
dependencies it needs, if any.</p>
<h1 id="inspiration-for-the-lumoions-rfc-content-headings"><a class="header" href="#inspiration-for-the-lumoions-rfc-content-headings">Inspiration for the Lumoions RFC Content Headings</a></h1>
<p>In terms of the content of the <em>text</em> version of the RFC (not the details of the markdown), 
the <a href="https://datatracker.ietf.org/doc/html/rfc5424">Syslog Specification in RFC 5424</a> is perhaps reasonably
close to the sections the Lumions RFC needs.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lumion-rfc"><a class="header" href="#lumion-rfc">Lumion RFC</a></h1>
<pre><code>



Lumions Working Group                                         D. Shearer
Internet-Draft                                                R. De Smet
Intended status: Informational                               C. Calvelli
Expires: 4 August 2022                                           LumoSQL
                                                         31 January 2022


 Lumions: Portable, Private, Secure, Unique, Updatable Data Primitives
              draft-shearer-desmet-calvelli-lumionsrfc-00

Abstract

   This memo defines Lumions, a new kind of secure, unique data
   encapsulation primitive designed for reliable, fine-grained storage
   and movements of arbitary data between arbitary storage mechanisms
   and across arbitary networks.  Lumions are also compatible with
   decentralised, distributed key management.  To illustrate the main
   use case, Lumions would not be needed if JSON had sophisticated
   privacy and encryption features, with a single unique JSON namespace
   and a standard way of referring to other JSON objects.

Status of This Memo

   This Internet-Draft is submitted in full conformance with the
   provisions of BCP 78 and BCP 79.

   Internet-Drafts are working documents of the Internet Engineering
   Task Force (IETF).  Note that other groups may also distribute
   working documents as Internet-Drafts.  The list of current Internet-
   Drafts is at https://datatracker.ietf.org/drafts/current/.

   Internet-Drafts are draft documents valid for a maximum of six months
   and may be updated, replaced, or obsoleted by other documents at any
   time.  It is inappropriate to use Internet-Drafts as reference
   material or to cite them other than as &quot;work in progress.&quot;

   This Internet-Draft will expire on 4 August 2022.

Copyright Notice

   Copyright (c) 2022 IETF Trust and the persons identified as the
   document authors.  All rights reserved.

   This document is subject to BCP 78 and the IETF Trust's Legal
   Provisions Relating to IETF Documents (https://trustee.ietf.org/
   license-info) in effect on the date of publication of this document.
   Please review these documents carefully, as they describe your rights
   and restrictions with respect to this document.  Code Components



Shearer, et al.           Expires 4 August 2022                 [Page 1]

Internet-Draft                 lumionsrfc                   January 2022


   extracted from this document must include Revised BSD License text as
   described in Section 4.e of the Trust Legal Provisions and are
   provided without warranty as described in the Revised BSD License.

Table of Contents

   1.  Introduction  . . . . . . . . . . . . . . . . . . . . . . . .   2
     1.1.  Terminology . . . . . . . . . . . . . . . . . . . . . . .   3
   2.  Definitions . . . . . . . . . . . . . . . . . . . . . . . . .   3
   3.  Feature Levels  . . . . . . . . . . . . . . . . . . . . . . .   4
     3.1.  Mandatory Minimum Requirements  . . . . . . . . . . . . .   4
     3.2.  Optional: Key Authority . . . . . . . . . . . . . . . . .   5
     3.3.  Optional: Versioning  . . . . . . . . . . . . . . . . . .   5
     3.4.  Optional: Access Control  . . . . . . . . . . . . . . . .   5
     3.5.  Optional: Checksums . . . . . . . . . . . . . . . . . . .   5
   4.  Properties of Lumions . . . . . . . . . . . . . . . . . . . .   5
   5.  Description of Lumions  . . . . . . . . . . . . . . . . . . .   6
   6.  Lumions and Key Management  . . . . . . . . . . . . . . . . .   7
   7.  Goals and Constraints . . . . . . . . . . . . . . . . . . . .   8
   8.  Lumion Data Format  . . . . . . . . . . . . . . . . . . . . .   8
   9.  Lumion Data Formal ABNF Specification . . . . . . . . . . . .   9
   10. Lumion UUID Format  . . . . . . . . . . . . . . . . . . . . .  10
   11. List of Lumion Ciphers, Signatures and Hashes . . . . . . . .  10
   12. Example Use Cases . . . . . . . . . . . . . . . . . . . . . .  10
     12.1.  Data Tracking and Portability  . . . . . . . . . . . . .  11
     12.2.  Time Travelling Data for Snapshotting  . . . . . . . . .  11
     12.3.  Non-Fungible Token (NFT) Applications  . . . . . . . . .  11
     12.4.  Online Backups . . . . . . . . . . . . . . . . . . . . .  11
   13. Performance Considerations  . . . . . . . . . . . . . . . . .  11
   14. Security Considerations . . . . . . . . . . . . . . . . . . .  11
   15. Related Work  . . . . . . . . . . . . . . . . . . . . . . . .  12
   16. IANA Considerations . . . . . . . . . . . . . . . . . . . . .  12
     16.1.  Content-type registration for 'apoplication/lumion'  . .  12
   17. Informative References  . . . . . . . . . . . . . . . . . . .  13
   Authors' Addresses  . . . . . . . . . . . . . . . . . . . . . . .  13

1.  Introduction

   A Lumion is a one-dimensional array of data signed with a public key
   which MUST contain a checksum, a version number and a universally
   unique identifier.  A Lumion is binary data and MUST be stored in
   network byte order.









Shearer, et al.           Expires 4 August 2022                 [Page 2]

Internet-Draft                 lumionsrfc                   January 2022


   In addition a Lumion MAY be encrypted with one or more schemes
   defined in this standard which together implement various forms of
   Role-based Access Control.  These schemes offer different levels of
   access depending on the token supplied.  After being updated with a
   valid write access, a Lumion will have an updated checksum.  The
   updated signature will be valid in all situations where the previous
   version of the signature was valid.

   A Lumion has keys implemented as public/private key pairs, and there
   can be any (or no) key management authorities.  The simplest case of
   a key management authority is where a program on a device creates a
   Lumion, making that program on that device the issuing authority.
   That program may subsequently be uninstalled, or the private key data
   it created be deleted or lost, making it a very transient key
   manaagement authority.

   Distinct from any other key management scheme users may implement,
   there is one specific key management authority scheme described in
   this RFC which stores lists of Lumion keys in an application of a
   public blockchain.  This gives Lumions the optional ability to have a
   decentralised, globally distributed key authority.

   Situations where Lumion properties are helpful include internet-
   connected devices such as mobile phones; transparency requirements
   related to privacy; and data portability requirements between clouds.

   A new media type &quot;application/lumion&quot; is defined as a helpful hint
   for high-level applications.

1.1.  Terminology

   The keywords MUST, MUST NOT, REQUIRED, SHALL, SHALL NOT, SHOULD,
   SHOULD NOT, RECOMMENDED, MAY, and OPTIONAL, when they appear in this
   document, are to be interpreted as described in [RFC2119].

2.  Definitions

   Lumion Generator: software that can produce a Lumion for any supplied
   raw data.  A Generator may be standalone or built into eg a database.
   A Lumion Generator must also be able to read Lumions, and is a full
   implementation of this RFC.

   Lumion Reader: is software that can access at least some data inside
   a Lumion, provided it has a key to do so, where a key is required by
   a particular Lumion.  A Lumion Reader implements some of the
   verification and reading functionality in this RFC.





Shearer, et al.           Expires 4 August 2022                 [Page 3]

Internet-Draft                 lumionsrfc                   January 2022


   Lumion Recogniser: is very simple software, perhaps a function or a
   script, that can detect the unique naming convention used by Lumions
   as defined in this RFC, and extract the universally unique identifier
   used by a particular Lumion.  A Recogniser can extract Lumions from
   non-Lumion data, and search for a particular Lumion.  A Recogniser
   will not be able to reliably determine whether any given Lumion is
   valid or not.

   Payload Data: an arbitary binary string within a Lumion of arbitary
   length less than 2^64 bytes.

   Payload Metadata: A checksum or version number specific to the
   Payload Data.

   Metadata: all data to do with access control, checksums and version
   numbers for the Lumion as a whole, the UUID and more.

   Access Control: The RBAC system implemented for Lumions, where valid
   users are anyone who has a valid key.  A valid key can only be used
   to sign a Lumion if it is used for the correct purpose.  For example,
   a read-only key cannot produce a valid signature for a Lumion after
   writing to it.

   Key Management Authority: a scheme selected by users to manage their
   Lumion keys.  This could be any system at all, from a plain text file
   on a server on the internet to a Kerberos server.  In the case of an
   embedded database library, the key management authority will often be
   either an individual app on the device (eg a banking app) or the
   device's platform-wide key management authority (eg the identity
   systems built into many versions of Android, and Apple phones.)

   Lumion Registry: One particular key management authority defined in
   this RFC for storing Lumion keys in a public blockchain.

3.  Feature Levels

3.1.  Mandatory Minimum Requirements

   A Lumion will always:

   *  Have a standardised Lumion UUID
   *  Be signed

   We would not expect plain text Lumions to be common, but they are
   valid.  A plain text Lumion with a signature is no different in
   principle to a signed plain text MIME email.  So long as the
   signature is valid we know that the data has not been changed.




Shearer, et al.           Expires 4 August 2022                 [Page 4]

Internet-Draft                 lumionsrfc                   January 2022


   There is no requirement for a key management authority, even on a
   device, because it is also valid (and may sometimes be useful) for a
   Lumion Generator to discard all knowledge of keys once it has
   generated a Lumion.

3.2.  Optional: Key Authority

   There are multiple ways of implementing a Key Authority.  They are
   all explained in the section &quot;Lumion Key Management&quot;.

3.3.  Optional: Versioning

   Both payload and metadata can be versioned with 64-bit version
   numbers.  These versions are internal versions, incremented each time
   the Lumion is updated and re-signed.

3.4.  Optional: Access Control

   This is a simple version of Role-based Access Control, with a list of
   valid keys stored in the Lumion Metadata.

3.5.  Optional: Checksums

   A signature is already a form of a checksum.  But in addition to this
   overall Lumion checksum, a checksum is also used as part of the
   Access Control system.

4.  Properties of Lumions

   Standardised: A Lumion can be operated on by any software that
   complies with this RFC.

   Integrity: Corruption can always be detected, at multiple levels
   (overall, or in the payload, or in the metadata).

   Uniquely Recognisable Among All Data: A Lumion will always be
   recognisable as a Lumion from its name-based UUID.

   Uniquely Identifiable Among All Lumions: A Lumion will always be
   unique among all Lumions due to the one-way hash part of its UUID.

   Secure: If there is no valid key available (because the original
   Lumion Generator did not store the key correctly, or the key was
   lost, etc) then a Lumion cannot be decrypted.

   Portable: Can be copied across architectures, networks, storage
   systems without losing or gaining any information.




Shearer, et al.           Expires 4 August 2022                 [Page 5]

Internet-Draft                 lumionsrfc                   January 2022


   Non-repudiable: The original key authority might be unreliable and
   transient (ifor example, because the originating phone got swallowed
   by a diprodoton) but any Lumions generated on that phone and intended
   to have a common local authority will always be identifiable as
   having the same original source.

   Self-contained security: no external key authority or integrity
   authority is needed.  Discriminated access control is provided solely
   from the information within the Lumion.

   Globally distributed namespace: Just by having a Lumion UUID, that
   means every Lumion is part of an ad hoc global storage system.

   Sequenced Internally: Since Lumions have an internal version number,
   anyone with copies of all version of a Lumion can view them as a
   time-sequenced stream.  (It is possible for a Lumion to keep all
   previous versions of its payload within itself, although whether this
   is scaleable or feasible is highly application-dependent.)

   Sequenced Externally: Lumions have fields of Left, Right, Below and
   Above, sized to contain a Lumion UUID.  The contents of these fields
   can be updated at any time, meaning that Lumions can optionally and
   frequently will form part of a logical structure such as a Merkle
   tree, thus creating a sequence that can be navigated forward/back/up/
   down, depending on the structure.  This sequence data can also be
   interpreted as time sequence data, if the Lumion Generator intended
   to produce that.  Timestamps are not required to be assigned by the
   Lumion Generator for time sequence data, because if a sequence of
   Lumions is ordered then a Lumion Reader can interpret that according
   to any temporal origin and offset it chooses.

   Time Travelling: Sequences of either the internal or external
   versioning can be interpreted as snapshotted point-in-time state
   information.  Such information can always be played back to
   reconstruct a view of the world at any point in time.  Even where
   there are no timestamps, the relative versions can still be replayed
   in either direction.

5.  Description of Lumions

   Any of the three types of data may be in plain text, although they
   usually will not be plain text because much of the value of a Lumion
   is in its encrypted properties.  A plain text Lumion is still signed,
   and still has a universally unique ID.

   Data in a Lumion may be automatically generated by one of these kinds
   of processes:




Shearer, et al.           Expires 4 August 2022                 [Page 6]

Internet-Draft                 lumionsrfc                   January 2022


   *  cryptographic checksums
   *  symmetric encryption
   *  public key encryption
   *  public key signing
   *  one-way hashes appended to a name-based uniqueness algorithm

   For each of these there are multiple possible ciphers and
   implementation techniques.

   Portability requires that data is stored in Network Byte Order.

6.  Lumions and Key Management

   There are four different levels of scope that involve key management:

   1.  The system within a Lumion, ie implementing access control so
       that a validly-signed Lumoion remains validly signed even after
       it has been updated by someone with a valid write key, and only
       allows reads by someone with a valid read or read+write key.  All
       of that is about how the Lumion is maintained as a data artefact.
       These valid keys could have been generated by anyone anywhere,
       and stored anywhere.  The Lumion neither knows nor cares.  But it
       still has to do some degree of key management because it has list
       of keys and their access rights inside it.

   2.  How a Lumion Generator creates the Lumion in the first places and
       the list of keys inside the Lumion.  There will also be the other
       half of keys to be stored somewhere (presumably inside a LumoSQL
       database, in a Lumion.)  That incldues symmetric keys, and
       signatures.  So this too is key management.  New users, the
       extent to which revocation is supported, etc.  I expect this will
       be mostly internal to LumoSQL, driven by the SQL interface (?)

   3.  Key management via an Authority, any authority.  A LumoSQL user
       is building an app, and might choose to make LDAP or Active
       Directory or Kerberos the Authority, or an Oracle database, etc.
       LumoSQL doesn't know or care, only that the keys are in the right
       places at the right time.  Will this be done through the C API,
       or SQL only?

   4.  Key management via the Lumion Registry, which is the only (and
       totally optional) scheme that LumoSQL is configured to support.
       This is the scheme I described where Lumions are stored in a
       blockchain, specifically Ethereum, as an implementation of a
       standard Ethereum smart contract.  This is where we could have
       many billions of rows with their UUID registered and also the
       users with access rights registered there too.  See the later
       section headed &quot;Lumion Registry&quot;.



Shearer, et al.           Expires 4 August 2022                 [Page 7]

Internet-Draft                 lumionsrfc                   January 2022


7.  Goals and Constraints

   XXXX THIS SECTION DOES NOT EXIST YET XXXX

8.  Lumion Data Format

   A Lumion is laid out like this:

      +--------+-----------------+------------------------+---------+
      |  UUID  | Metadata Block  | Payload Metadata Block | Payload |
      +--------+-----------------+------------------------+---------+

   These fields are always present in a Lumion.

   The UUID is described in the section &quot;Lumion UUID Format&quot;, and is
   always 256 bits wide.

   The Metadata Block is laid out like this:

      +-----------+----------+-------------------------+--------------+
      | Signature | Features | Payload Metadata Offset | Other Metad. |
      +-----------+----------+-------------------------+--------------+

   The Lumion Signature is a digital signature from one of those allowed
   in this RFC.  See the section &quot;Lumion Ciphers, Signatures and
   Hashes&quot;.

   The Lumion Feature list is a 32-bit bitmask with values as in the
   following table:

   XXXXX MORE GOES HERE XXXXX

   Payload Metadata Offset is a 64-bit integer.

   Other Metadata contains all RBAC metadata, and some non-RBAC
   Metadata:

   *  Left, Right, Below and Above pointers.  These pointers are Lumion
      UUIDs, meaning that trees, lists and other structures can be
      implemented with Lumions.  At least one of these fields MUST be
      non-zero if the External Lumion Version Count is non-zero.
   *  List of valid Lumion Access Keys
   *  XXXXX MORE GOES HERE XXXXX

   The Payload Metadata Block is laid out like this:






Shearer, et al.           Expires 4 August 2022                 [Page 8]

Internet-Draft                 lumionsrfc                   January 2022


      +----------------+---------------------+------------------------+
      | Payload Length | Payload Vers. Count | Other Payload Metadata |
      +----------------+---------------------+------------------------+

   Payload Length is a 64-bit integer.

   Payload Version Count is a 64-bit integer.

   Other Payload Metadata relates to RBAC, such as Last Edited By, which
   is a keyid listed in the Metadata Block.  XXXXX

9.  Lumion Data Formal ABNF Specification

   A Lumion has the following ABNF [RFC5234] definition:

   (this is NOT valid Lumion ABNF because we're still at the high-level
   sketch stage.  But it is quite atmospheric, don't you think?  A bit
   like mood music.)

     SYSLOG-MSG      = HEADER SP STRUCTURED-DATA [SP MSG]

     HEADER          = PRI VERSION SP TIMESTAMP SP HOSTNAME
                       SP APP-NAME SP PROCID SP MSGID
     PRI             = &quot;&lt;&quot; PRIVAL &quot;&gt;&quot;
     PRIVAL          = 1*3DIGIT ; range 0 .. 191
     VERSION         = NONZERO-DIGIT 0*2DIGIT
     HOSTNAME        = NILVALUE / 1*255PRINTUSASCII

     APP-NAME        = NILVALUE / 1*48PRINTUSASCII
     PROCID          = NILVALUE / 1*128PRINTUSASCII
     MSGID           = NILVALUE / 1*32PRINTUSASCII

     TIMESTAMP       = NILVALUE / FULL-DATE &quot;T&quot; FULL-TIME
     FULL-DATE       = DATE-FULLYEAR &quot;-&quot; DATE-MONTH &quot;-&quot; DATE-MDAY
     DATE-FULLYEAR   = 4DIGIT
     DATE-MONTH      = 2DIGIT  ; 01-12
     DATE-MDAY       = 2DIGIT  ; 01-28, 01-29, 01-30, 01-31 based on
                               ; month/year
     FULL-TIME       = PARTIAL-TIME TIME-OFFSET
     PARTIAL-TIME    = TIME-HOUR &quot;:&quot; TIME-MINUTE &quot;:&quot; TIME-SECOND
                       [TIME-SECFRAC]
     TIME-HOUR       = 2DIGIT  ; 00-23
     TIME-MINUTE     = 2DIGIT  ; 00-59
     TIME-SECOND     = 2DIGIT  ; 00-59
     TIME-SECFRAC    = &quot;.&quot; 1*6DIGIT
     TIME-OFFSET     = &quot;Z&quot; / TIME-NUMOFFSET
     TIME-NUMOFFSET  = (&quot;+&quot; / &quot;-&quot;) TIME-HOUR &quot;:&quot; TIME-MINUTE




Shearer, et al.           Expires 4 August 2022                 [Page 9]

Internet-Draft                 lumionsrfc                   January 2022


     STRUCTURED-DATA = NILVALUE / 1*SD-ELEMENT
     SD-ELEMENT      = &quot;[&quot; SD-ID *(SP SD-PARAM) &quot;]&quot;
     SD-PARAM        = PARAM-NAME &quot;=&quot; %d34 PARAM-VALUE %d34
     SD-ID           = SD-NAME
     PARAM-NAME      = SD-NAME
     PARAM-VALUE     = UTF-8-STRING ; characters '&quot;', '\' and
                                    ; ']' MUST be escaped.
     SD-NAME         = 1*32PRINTUSASCII
                       ; except '=', SP, ']', %d34 (&quot;)

     MSG             = MSG-ANY / MSG-UTF8
     MSG-ANY         = *OCTET ; not starting with BOM
     MSG-UTF8        = BOM UTF-8-STRING
     BOM             = %xEF.BB.BF

     UTF-8-STRING    = *OCTET ; UTF-8 string as specified in RFC 3629

     OCTET           = %d00-255
     SP              = %d32
     PRINTUSASCII    = %d33-126
     NONZERO-DIGIT   = %d49-57
     DIGIT           = %d48 / NONZERO-DIGIT
     NILVALUE        = &quot;-&quot;

10.  Lumion UUID Format

   This is a combination of a name-based namespace and a robust hash,
   similar to type 5 UUIDs in [RFC4122].

   RFC4122 UUIDs MUST NOT be used because of the constrained
   environments many Lumion-using applications are deployed in and which
   therefore do not have knowledge of namespaces that look like DNS or
   which imply a network even exists.  In addition RFC4122 does not
   include any hash more recent than SHA-1, which is now deprecated.

   XXXXX MORE GOES HERE XXXXX

11.  List of Lumion Ciphers, Signatures and Hashes

   *  SHA-3/SHA-256
   *  BLAKE3
   *  Curve 25519
   *  XXXXX MORE CIPHERS HERE XXXXX

12.  Example Use Cases






Shearer, et al.           Expires 4 August 2022                [Page 10]

Internet-Draft                 lumionsrfc                   January 2022


12.1.  Data Tracking and Portability

   XXXXX EXPLAIN HERE - THIS IS AN EASY AND OBVIOUS ONE XXXXX

12.2.  Time Travelling Data for Snapshotting

   This is about using the versioning information embedded within
   Lumions (either internal or external) to come up with time series
   data.  It might in fact be more about ordinal data, because wallclock
   time is not part of the Lumion definition in this RFC.  A

   Each Lumion MUST have pointers called Left, Right, Below, Above, as
   well as an external or internal version number.

12.3.  Non-Fungible Token (NFT) Applications

   *  Compatible with existing NFT registries
   *  First-ever updatable NFTs

   XXXXX MORE GOES HERE XXXXX

12.4.  Online Backups

   A time-ordered lists of Lumions is also a form of backups.  Ad-hoc
   backups will be possible so long as the smallest unit is a Lumion and
   only whole Lumions are transferred.  The UUID, versioning and ordinal
   information optionally contained in a Lumion means that a consistent
   backup can always be calculated assuming a reasonable percentage of
   Lumions are present.

13.  Performance Considerations

   XXXXXX

14.  Security Considerations

   While a valid Lumion is entirely self-contained from a security point
   of view, it is important to remember that Lumions do NOT provide any
   guarantee of anonymity.  Lumions MAY be used for this purpose despite
   the presence of a UUID if the Lumion Generator is implemented in a
   very particular way (for example, the Lumion Generator only ever
   produces a single Lumion before being deleted permanently.)
   Transparency and traceability is vital to the Lumion concept, which
   is why it has a UUID.  For normal usage the UUID prevents Lumions
   providing anonymity.






Shearer, et al.           Expires 4 August 2022                [Page 11]

Internet-Draft                 lumionsrfc                   January 2022


15.  Related Work

   XXXXX

16.  IANA Considerations

   This memo calls for IANA to register a new MIME content-type
   application/pidf+xml, per [MIME].

   The registration template for this is below.

16.1.  Content-type registration for 'apoplication/lumion'

   To: ietf-types@iana.org Subject: Registration of MIME media type
   application/lumion

   MIME media type name: application

   MIME subtype name: lumion

   Required parameters: (none) Optional parameters: (none)

   Encoding considerations: (none)

   Security considerations:

     This content type carries a payload with metadata, where the only
     information that can be deduced relates to the Lumion envelope.
     Everything else is encrypted. A Lumion thus is self-contained from
     a security point of view.

   Interoperability considerations: This content type provides a common
   format for transporting data in a secure and privacy-compliant
   manner.

   Published specification: (none)

   Applications which use this media type: Databases

   Additional information: Magic number(s): XXXX File extension(s):
   .lumion (optional)

   Person &amp; email address to contact for further information: Dan
   Shearer EMail: dan@shearer.org

   Intended usage: Globally, at scale

   Author/Change controller: (none)



Shearer, et al.           Expires 4 August 2022                [Page 12]

Internet-Draft                 lumionsrfc                   January 2022


17.  Informative References

   [RFC2119]  Bradner, S., &quot;Key words for use in RFCs to Indicate
              Requirement Levels&quot;, BCP 14, RFC 2119,
              DOI 10.17487/RFC2119, March 1997,
              &lt;https://www.rfc-editor.org/info/rfc2119&gt;.

   [RFC4122]  Leach, P., Mealling, M., and R. Salz, &quot;A Universally
              Unique IDentifier (UUID) URN Namespace&quot;, RFC 4122,
              DOI 10.17487/RFC4122, July 2005,
              &lt;https://www.rfc-editor.org/info/rfc4122&gt;.

   [RFC5234]  Crocker, D., Ed. and P. Overell, &quot;Augmented BNF for Syntax
              Specifications: ABNF&quot;, STD 68, RFC 5234,
              DOI 10.17487/RFC5234, January 2008,
              &lt;https://www.rfc-editor.org/info/rfc5234&gt;.

Authors' Addresses

   Dan Shearer
   LumoSQL

   Email: dan@shearer.org, dan@lumosql.org


   Ruben De Smet
   LumoSQL

   Email: me@rubdos.be


   Claudio Calvelli
   LumoSQL

   Email: webmaster@lumosql.org
















Shearer, et al.           Expires 4 August 2022                [Page 13]
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><!--- SPDX-License-Identifier: CC-BY-SA-4.0 --->
<!--- SPDX-FileCopyrightText: 2020 The LumoSQL Authors --->
<!--- SPDX-ArtifactOfProjectName: LumoSQL --->
<!--- SPDX-FileType: Documentation --->
<!--- SPDX-FileComment: Original by Dan Shearer, 2020 --->
<h1 id="table-of-contents-7"><a class="header" href="#table-of-contents-7">Table of Contents</a></h1>
<ul>
<li><a href="2.1-development-landscape.html#lumosql-context">LumoSQL Context</a></li>
<li><a href="2.1-development-landscape.html#the-sqlite-landscape">The SQLite Landscape</a>
<ul>
<li><a href="2.1-development-landscape.html#lumosql-and-the-sqlite-project">LumoSQL and The SQLite Project</a></li>
</ul>
</li>
<li><a href="2.1-development-landscape.html#sqlite-downstreams">SQLite Downstreams</a></li>
<li><a href="2.1-development-landscape.html#how-widely-used-is-sqlite-compared-to-others">How Widely-used is SQLite Compared to Others?</a></li>
</ul>
<p><img src="./images/lumo-ecosystem-intro.png" alt="" title="LumoSQL logo" /></p>
<h1 id="lumosql-context"><a class="header" href="#lumosql-context">LumoSQL Context</a></h1>
<p>LumoSQL is not a fork of SQLite. LumoSQL integrates several bodies of code
including SQLite into a new database. The idea of building on SQLite isn't new.
The reach and breadth of the SQLite code is astonishing and far more than the
LumoSQL project realised at the beginning. Before LumoSQL nobody has tried to
document this as one whole.</p>
<h1 id="the-sqlite-landscape"><a class="header" href="#the-sqlite-landscape">The SQLite Landscape</a></h1>
<p>The Internet runs on data stored in rows, ie Relational Databases (RDBMS). There are 
many exceptions and alternatives but RDBMS are where the deciding data is stored including:</p>
<ul>
<li>in routers that direct internet traffic</li>
<li>in mobile phones that enable the user to navigate the internet</li>
<li>business applications or those that deal with reconciling important numbers</li>
<li>in millions of everyday web applications, from Wikipedia to ticket booking systems</li>
</ul>
<p>The most widely-used relational database is almost certainly SQLite, and nearly
all SQLite usage is in embedded devices, frequently with internet access as the
Internet of Things. The question &quot;How widely-used is SQLite?&quot; is addressed in specific
section below.</p>
<p>In addition there are SQLite-derived projects that address traditional online
database use cases in a similar way only in a tiny fraction (4%-10%) of size of
code. SQLite sticks to its strict original project definition. LumoSQL seeks to
be still rigorous and also inclusive and accepting of the many new
SQLite-based features fragmented across the web.</p>
<p>The databases to compare LumoSQL against are those which together are the
most-used on the internet today: </p>
<ul>
<li>Oracle MySQL - open source, with closed source licensing available, ubiquitous on the internet</li>
<li>MariaDB - a mostly-compatible fork of MySQL, seemingly taking over from MySQL on the internet</li>
<li>Postgresql - open source, highly compliant with SQL standards,</li>
<li>Oracle's traditional RDBMs - the closed source database that makes its owners the most money</li>
<li>Microsoft SQL Server - ubiquitous in business applications, less common on the internet </li>
<li>SQLite - source code freely available, highly compliant with most SQL standards</li>
</ul>
<p>All of these are online databases, while SQLite is embedded - more on that below.</p>
<h2 id="lumosql-and-the-sqlite-project"><a class="header" href="#lumosql-and-the-sqlite-project">LumoSQL and The SQLite Project</a></h2>
<p>This section pulls out distinctive features of the sqlite.org project that
matter to LumoSQL.  Some are negative features to which LumoSQL wants to
respond by way of an improvement or extension, but others are more like a
spring that gives LumoSQL a great start in new use cases. Some of these points
are mentioned by D Richard Hipp in his <a href="https://changelog.com/podcast/201">2016 Changelog Podcast Interview</a>.</p>
<p><strong>sqlite.org is a responsive and responsible upstream</strong>, for issues they feel
are in scope. For example, see section 4.1.4 in
<a href="https://www.sqlite.org/testing.html">https://www.sqlite.org/testing.html</a> where Manuel Rigger is credited for his
work in raising many dozens of bugs, which as can be seen at
<a href="https://www.sqlite.org/src/timeline?y=t&amp;u=mrigger&amp;n=all">https://www.sqlite.org/src/timeline?y=t&amp;u=mrigger&amp;n=all</a> have been addressed
promptly. LumoSQL should not fork SQLite, partly because the developers are doing a 
very good job at maintaining the things they care about, and partly because SQLite has a high 
profile and attracts technical review.</p>
<p><strong>Sqlite.org is entirely focussed on its existing scope and traditional user
needs</strong> and <a href="https://sqlite.org/about.html">SQLite imposes strict limits</a> for
example “Think of SQLite not as a replacement for Oracle but as a replacement for
fopen()” which eliminates many of the possibilities LumoSQL is now exploring
that go beyond any version of fopen(). Many things that SQLite can used
for are <a href="https://sqlite.org/whentouse.html">declared out of scope</a> by the
SQLite project including those that are high concurrency, write-intensive, or
networked. LumoSQL feels there is a tension between this scope for SQLite and
the 5G IoT world very many applications are all three, and many SQLite
deployments are tending towards disregarding the warnings from the SQLite
project. </p>
<p><strong>Sqlite has a very strict and reliable view on maintaining backwards
compatibility both binary and API (except when it comes to encryption, see
further down.)</strong> The Sqlite foundation aims to keep SQLite3 interfaces and
formats stable until the year 2050, according to Richard Hipp in the
podcast interview, as once requested by an airframe construction company
(Airbus). Whatever happens in years to come SQLite has definitely delivered on
this to date. This means that there are many things SQLite cannot do which
LumoSQL can, and it also means that LumoSQL needs to suppor strict
compatibility with SQLite3 interfaces and formats at least as an option if not
the default.</p>
<p><strong>Sqlite.org not entirely an open project.</strong> No code is accepted from contributors
external to the company HWACI, which is owned by D Richard Hipp, the original
author and lead of SQLite. From the <a href="https://www.sqlite.org/copyright.html">SQLite copyright page</a>:</p>
<blockquote>
<p><em>In order to keep SQLite completely free and unencumbered by copyright, the
project does not accept patches. If you would like to make a suggested change,
and include a patch as a proof-of-concept, that would be great.  However please
do not be offended if we rewrite your patch from scratch.</em></p>
</blockquote>
<p>That fact is not hidden in any way, and there have been many years of
successful SQLite use on this basis. It is definitely not acceptable under the
Next Generation Internet initiative nor (quite possibly) under draft and
current EU laws or at least recommendations, as discussed elsewhere in SQLite
documentation. On the other hand, there is a <a href="https://system.data.sqlite.org/index.html/doc/trunk/www/contribute.wiki">contribution
policy</a>
. LumoSQL is a fully open project, and since it started in 2019 not 1999,
LumoSQL does not have to navigate historical decisions in the way SQLite does.</p>
<p>The <a href="https://sqlite.com/lts.html">SQLite Long Term Support</a> page states:</p>
<blockquote>
<p><em>“In addition to supporting SQLite through the year 2050, the developers also
promise to keep the SQLite <a href="https://sqlite.com/cintro.html">C-language API</a> and
<a href="https://sqlite.com/fileformat2.html">on-disk format</a> fully backwards
compatible. This means that applications written to use SQLite today should be
able to link against and use future versions of SQLite released decades in the
future.”</em></p>
</blockquote>
<p>As well as the success of this policy and its obvious advantages, there are some 
serious problems with it too:</p>
<ul>
<li>
<p>this kind of backwards-compatibility was one of the problems that led
to Microsoft Windows stagnation for years, see <a href="https://joomla.digital-peak.com/blog/178-the-case-against-backward-compatibility">The Case Against
Backwards Compatibility</a>.
The problem isn't clear-cut, as shown by <a href="https://www.dpdk.org/blog/2019/10/10/why-is-abi-stability-important/">Why is ABI Stability Important?</a>
and Greg Kroah-Hartman's paper <a href="https://www.kernel.org/doc/Documentation/process/stable-api-nonsense.rst">Stable API Nonsense</a>,
noting that the Linux distribution companies such as Red Hat charge money to maintain a time-limited
stable API.  LumoSQL has the luxury of choosing what extensions to stable formats and APIs to introduce, and how.</p>
</li>
<li>
<p>Since SQLite crypto implementation details are secret, any encrypted database 
created by the official closed-source SQLite is almost certainly incompatible 
with any other encryption project by third parties, despite file format compatibility 
being a major feature of SQLite. Yet with encryption being a requirement for 
many applications, this means there is no guarantee of SQLite file format
compatibility.</p>
</li>
<li>
<p>this rigid rule becomes difficult when it comes to a long-term
storage format discussions.  See below for discussion of how the
SQLite binary format has almost zero internal integrity checking.
LumoSQL aims to add options to address this problem.</p>
</li>
</ul>
<p><strong>Sqlite is less open source than it appears</strong>. The existence of so many SQLite
spin-offs is evidence that SQLite code is highly available. However there are
several aspects of SQLite that mean it cannot be considered open source, in
ways that are increasingly important in the 21st century:</p>
<ul>
<li>
<p>Public domain is not is not recognised in some countries such as
Germany and Australia, which means that legal certainty is not
possible for users in these countries who need it or want it. Public
Domain is not a valid grant of right, which is why it is not one of
the <a href="https://opensource.org/licenses">Open Source Initiative licenses</a> nor
does it appear on the <a href="https://spdx.org/licenses/">SPDX License List</a>.</p>
</li>
<li>
<p>sqlite.org charge USD6000 for “perpetual right-to-use for the SQLite source
code&quot;, and implies that this right-to-use grants legal benefits. This is an
additional reason why the SQLite public domain licensing approach cannot be
open source, because you have to pay money in order to be sure where you stand.
Paying money for open source is encouraged throughout the open source movement,
however if that casts doubt on the validity of the open source license then it
cannot be open source in the first place.</p>
</li>
<li>
<p>As <a href="https://www.sqlite.org/th3.html">sqlite.org states</a>, the most important test harness is only available commercially. 
This is perfectly legal, but is definitely not the way open source projects
generally behave.</p>
</li>
<li>
<p>The only supported encryption code is closed source, and even the mailing list 
for discussing this is not public. This is not expected behaviour for an open 
source project.</p>
</li>
</ul>
<p>This really matters, because in an analogous move, MariaDB now publish some of their
cryptographic and network scaling code under their <a href="https://mariadb.com/bsl-faq-mariadb/">Business Source License</a>
which requires &quot;Power Users&quot; to pay.  These pressures are not compatible with
modern open source and LumoSQL wants to provide an alternative.</p>
<p><strong>SQLite Code has Many Independent Downstreams</strong> Multiple front ends, backends
and network replication, clustering and encryption solutions exist today. This
is all code that appears to be working, or at least it has not obviously
decayed even if it might not work as advertised, and some it is very actively
developed and supported. Much of this code has been inspected to a first level
by LumoSQL, some of it has been built and run. Just because the foregoing can
be made to work in a demonstration does not mean that it is suitable for
production, nor that it should be part of LumoSQL design. But it speaks of a
very strong demand for more features, and a rich pool of experimentation to
learn from. This is covered in more detail in the next section. </p>
<p><strong>Sqlite file format does not have technical or organisational longevity
built-in</strong> Sqlite.org commit to supporting the on-disk file format until 2050,
without evidence of an enduring legal body. Similarly, claims are made for the
reliability of the on-disk format even though there are virtually no intergity
measures built in.  As can be seen from <a href="https://www.sqlite.org/fileformat2.html">the published
specification</a> the use of checksums is
limited to pages in the writeback journal, and WAL records, and one 32-bit
integer for when the official closed source encryption is used. Therefore file
format provides no general means of validating the data it contains. It would
be possible to extend the file format in a backwards-compatible manner to
include such checks, but that does not appear to have been discussed or
suggested. This does not provide modern standards of reliable at-rest storage,
transport and archiving, and it does not meet minimum privacy standards
including those mandated by law in many countries, especially the EU.
sqlite.org is not unique in this, it is only fair to say that other SQL
databases implement this feature awkwardly or not at all, and that it is
particularly obvious in the case of SQLite because it an embedded local on-disk
database library. SQLite could be the first database to have a modern reliable
on-disk format, and it would not be difficult to make it so except for promises 
not to break bit-for-bit compatibility. This issue is dealt with in more detail in 
<a href="./lumo-architecture">LumoSQL architecture</a> and <a href="./lumo-implementation.html">implementation</a> documentation.</p>
<p><strong>the current Sqlite on-disk file format may be less robust than it seems</strong>.
Unscientific ad-hoc enquiries indicate that many IT professionals are familiar
with SQLite file corruption. Examples of ad-hoc enquiries are: asking for a
show of hands in a room of developers, speaking to the maintainers of SQLite
file inspection tools, and chatting to Android developers.  The LumoSQL team as
of March 2020 are looking at how to artificially create crashes and corruptions
as part of testing (and perhaps benchmarking).</p>
<h1 id="sqlite-downstreams"><a class="header" href="#sqlite-downstreams">SQLite Downstreams</a></h1>
<p>There is still a lot for LumoSQL to explore because there is just so much code, but
as of March 2020 we are confident code could be assembled from here and there
on the internet to demonstrate the following features:</p>
<ul>
<li>SQLite with Berkely bdb backend</li>
<li>SQLite with LevelDB backend</li>
<li>SQLite with network access from quite a few projects, only some of them listed</li>
<li>SQLite with MySQL-compatible frontend, from Oracle</li>
<li>SQLite with MySQL-compatible frontend, from Bedrock</li>
<li>SQLite with LMDB at WAL-level and then replicated LMDB with consensus, from ActorDB</li>
<li>SQLite with private blockchain-esque for fast replication to 6-8 nodes, from Bedrock</li>
<li>SQLite with GIS extensions</li>
<li>SQLite accessible locally over ODBC</li>
<li>SQLite with at-rest encryption in many different ways</li>
</ul>
<p>And plenty more. </p>
<p>Due to the fragmented nature of SQLite landscape, what we <em>cannot</em> demonstrate is:</p>
<ul>
<li>APIs for the above features in the same codebase</li>
<li>any of these interesting features running together in the same
binary or using the same database files</li>
<li>consolidated contact, code curation, bug tracking, licensing across
these features and projects</li>
</ul>
<p>and that is why there really isn't an SQLite ecosystem, just a landscape. </p>
<p>For those users that even realise there are extended feature versions of SQLite
around, they have to pick and choose which of these features they want, and
usually that means going without some other feature, and it just has a chaotic
feel to it. On the other hand, while SQLite has achieved so much, there are
compelling reasons to look for an alternative. LumoSQL aims to be that
alternative.</p>
<h1 id="how-widely-used-is-sqlite-compared-to-others"><a class="header" href="#how-widely-used-is-sqlite-compared-to-others">How Widely-used is SQLite Compared to Others?</a></h1>
<p>It is easy to establish that SQLite3 is deployed in billions of applications around the world, and as
sqlite.org has showed it is easy to <a href="https://www.sqlite.org/mostdeployed.html">credibly quote very large deployment numbers</a>.</p>
<p>But what of SQLite in relation to the databases it can be reasonably compared
to, especially as the use cases for SQLite have become so broad?  There isn't a good 
answer, and this is something that LumoSQL hopes to change over time as it gains 
features that feel familiar to users outside traditional embedded development.</p>
<p>SQLite is considered by many to not be database at all, any more than bdb (the
original Unix bdb, as still used by a diminishing number of applications) or
LMDB. These are all embeddable database libraries. That affects how we measure
popularity. This view is also easy to challenge on technical grounds, if the 
only difference is closeness of the coupling between the application and the database.</p>
<p>The difference in measurement between the online databases and sqlite is significant in these ways:</p>
<ul>
<li>Measures of database popularity with developers don’t map well to a database library (see
the widely-regarded <a href="https://db-engines.com/en/ranking">https://db-engines.com/en/ranking</a> ) because embedded applications use
fewer database developers than cloud and other higher-level applications. The commercial
value is concentrated in different parts of the stack for these two use cases, although not the
commercial risk.</li>
<li>Industry surveys are indicative but well short of science-based methods, and almost never
focussed on embeddable databases and yet nevertheless SQLite is usually included. For
example <a href="https://www.jetbrains.com/research/devecosystem-2018/">6000 developers surveyed by Jetbrains</a> and this <a href="http://highscalability.com/blog/2019/3/6/2019-database-trends-sql-vs-nosql-top-databases-single-vs-mu.html">Developer Week survey</a></li>
<li>Being an embeddable library, SQLite is not intended for use in cloud applications and the
like, and therefore most surveys of databases do not cover them. As many people including
the SQLite developers point out, it is possible to run a web application off SQLite just as
much as it is possible to run an email server using Sendmail. Readers of this document are
unlikely to be confused that either of these activities are a good idea.</li>
<li>In contrast, embedded databases are rarely addressed as a sector because the embedded
development industry does not tend to be very interested in how data is stored.</li>
</ul>
<p>(Not counting bdb. We can reasonably discount bdb as a currently-relevant database, because its
prevalence is <a href="https://en.wikipedia.org/wiki/Berkeley_DB#Past_users">decreasing rapidly</a>
and noting the heading “Deprecated in favour of LMDB”, and also looking at the reactions
worldwide when Oracle changed the license of the bdb code. bdb is being erased from the world of
free software applications.)</p>
<div style="break-before: page; page-break-before: always;"></div><!--- SPDX-License-Identifier: CC-BY-SA-4.0 --->
<!--- SPDX-FileCopyrightText: 2020 The LumoSQL Authors --->
<!--- SPDX-ArtifactOfProjectName: LumoSQL --->
<!--- SPDX-FileType: Documentation --->
<!--- SPDX-FileComment: Original by Dan Shearer, 2019 --->
<h1 id="table-of-contents-8"><a class="header" href="#table-of-contents-8">Table of Contents</a></h1>
<ul>
<li><a href="3.7-relevant-codebases.html#codebases-relevant-to-lumosql">Codebases Relevant to LumoSQL</a></li>
<li><a href="3.7-relevant-codebases.html#what-is-a-relevant-codebase">What is a Relevant Codebase?</a></li>
<li><a href="3.7-relevant-codebases.html#useful-dead-sqlite-forks">Useful Dead SQLite Forks</a></li>
<li><a href="3.7-relevant-codebases.html#oracle-bdb-and-oracle-bdb-sql-codebases">Oracle BDB and Oracle BDB-SQL Codebases</a></li>
<li><a href="3.7-relevant-codebases.html#distributed-or-clustered-codebases">Distributed or Clustered Codebases</a></li>
<li><a href="3.7-relevant-codebases.html#modular-code-to-potentially-incorporate-in-lumosql">Modular Code to Potentially Incorporate in LumoSQL</a></li>
<li><a href="3.7-relevant-codebases.html#on-disk-file-format-related-knowledge">On-disk File Format-related Knowledge</a></li>
<li><a href="3.7-relevant-codebases.html#list-of-relevant-benchmarking-and-test-knowledge">List of Relevant Benchmarking and Test Knowledge</a></li>
<li><a href="3.7-relevant-codebases.html#sqlite-and-encryption-issues">SQLite and Encryption Issues</a></li>
<li><a href="3.7-relevant-codebases.html#list-of-from-scratch-mysql-sql-and-mysql-server-implementations">List of from-scratch MySQL SQL and MySQL Server implementations</a></li>
</ul>
<h1 id="codebases-relevant-to-lumosql"><a class="header" href="#codebases-relevant-to-lumosql">Codebases Relevant to LumoSQL</a></h1>
<p><img src="./images/lumo-relevant-codebases-intro.jpg" alt="Ingredients" title="FuQin Market in Chengdu, CC license, https://chopsticksinmysuitcase.typepad.com/.a/6a0133f251ecbe970b0133f60d5807970b-500wi" /></p>
<h1 id="what-is-a-relevant-codebase"><a class="header" href="#what-is-a-relevant-codebase">What is a Relevant Codebase?</a></h1>
<p>There are four dimensions to codebases relevant to LumoSQL:</p>
<ol>
<li>Code that is a derivative of SQLite code adding a feature or improvement</li>
<li>Code that is, or could become, a candidate for a LumoSQL backend store</li>
<li>Code that has nothing to do with SQLite but implements an interesting database feature we want to use in LumoSQL</li>
<li>Code that supports the development of LumoSQL such as testing, benchmarking or analysing relevant codebasesembarrasing</li>
</ol>
<p>This is a discussion document, describing our findings as we look at hundreds
of thousands of lines of other people's code. In addition there is the <a href="./2.4-relevant-knowledgebase.html">Full Knowledgebase Relevant to LumoSQL</a> .
There is a lot more in the Knowledgebase than there is in this more detailed document.</p>
<h1 id="useful-dead-sqlite-forks"><a class="header" href="#useful-dead-sqlite-forks">Useful Dead SQLite Forks</a></h1>
<p>Code becomes unmaintained for many reasons, and some interesting code is definitely dead. We have considered the following codebases.</p>
<table><thead><tr><th>Project</th><th>Last modified</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/LMDB/sqlightning">sqlightning</a></td><td>2013</td><td>SQLight ported to the LMDB key-value store</td></tr>
<tr><td><a href="https://github.com/btrask/sqlheavy">SQLHeavy</a></td><td>2016</td><td>SQLite ported to LevelDB, LMDB, RocksDB and more, with a key-value store library abstraction</td></tr>
<tr><td><a href="https://github.com/btrask/libkvstore">libkvstore</a></td><td>2016</td><td>The backend library used by SQLHeavy</td></tr>
<tr><td><a href="https://sqlite.org/src4/tree?ci=trunk">SQLite 4</a></td><td>2014</td><td>Abandoned new version of SQLite with improved backend support and other features</td></tr>
</tbody></table>
<p>The library <code>libkvstore</code> is after the style of what we are implementing at the API interception point in <code>backend.c</code>, and the author remains active in this general area.</p>
<p>SQLHeavy never got beyond prototype stage but it did support multiple K-V stores. Despite looking at a great deal of code we never noticed SQLHeavy until we had already done our own work to revive sqlightning. This is a reminder that we may still be missing important existing contributions.</p>
<p>The defunct version 4 of SQLite intended to implement <a href="https://sqlite.org/src4/doc/trunk/www/storage.wiki">pluggable storage engines</a>, to similar effect
to LumoSQL. The intention was that &quot;Transaction commit and rollback is handled
by the storage engine&quot;, suggesting that SQLite4 was intended to be quite conservative technically, not using 
MVCC-compliant K-V stores, and still using Write-Ahead Logs.</p>
<h1 id="oracle-bdb-and-oracle-bdb-sql-codebases"><a class="header" href="#oracle-bdb-and-oracle-bdb-sql-codebases">Oracle BDB and Oracle BDB-SQL Codebases</a></h1>
<p>As of June 2020, Oracle announced that it has dropped support for the BDB port
to SQLite. DB 18.1.32 is the last version to carry this, which is based on
SQLite from 2017. This is the reference and basis for the BDB backend in
LumoSQL. </p>
<table><thead><tr><th>Project</th><th>Last modified</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://fossies.org/linux/misc/db-18.1.32.tar.gz">Sleepycat/Oracle BDB</a></td><td>current</td><td>The original ubiquitous Unix K-V store, disused in open source since Oracle's 2013 license change; the API template for most of the k-v btree stores around. Now includes many additional features including full MVCC transactions, networking and replication. This link is a mirror of code from download.oracle.com, which requires a login</td></tr>
<tr><td><a href="https://fossies.org/linux/misc/db-18.1.32.tar.gz">Sleepycat/Oracle BDB-SQL</a></td><td>current</td><td>Port of SQLite to the Sleepycat/Oracle transactional bdb K-V store. As of 5th March 2020 this mirror is identical to Oracle's login-protected tarball for db 18.1.32</td></tr>
</tbody></table>
<p>It turns out though that Oracle is not the only developer of KV stores based on BDB
code. ComDB, listed under &quot;Clustered Codebases&quot;, uses a KV store derived from
BDB before Oracle bought Sleepcat Software, meaning before the license changed
from BSD to AGPL. ComDB added row-level locks to BDB, and prefaulting/readahead among 
other features. It is not yet clear whether it is still possible to extract the BDB 
library from ComDB and use it standalone elsewhere (such as a LumoSQL backend.)</p>
<h1 id="actual-or-potential-lumosql-backends"><a class="header" href="#actual-or-potential-lumosql-backends">Actual or Potential LumoSQL Backends</a></h1>
<p>During 2020 at least, the only backends to be considered for LumoSQL will be btree-style KV 
stores capable of serialised transactions. As identified in the Knowledbase there are only 
six or so under active development, 5 of them in C and 1 in Go. Of these, three are the first
candidates to be a LumoSQL backend.</p>
<p>Since LumoSQL implements a single backend API for the VDBE to talk to, that API
can be externally exposed. This means that all LumoSQL backend KV stores can be
exposed via a single library API, meaning nothing has to change if the user
decides to switch to a different KV store. This is possible because all of the
KV stores have a very similar API dealing with identical objects, despite their
substantial differences in implementation.</p>
<p>From our research we expect a transactional KV store implemented in C or Go to
be between 11-16k lines of code, excluding comments and test code.</p>
<table><thead><tr><th>Project</th><th>Last modified</th><th>Description</th></tr></thead><tbody>
<tr><td>SQLite's built-in KV store</td><td>current</td><td>As used by billions.</td></tr>
<tr><td><a href="https://github.com/LMDB/">LMDB</a></td><td>current</td><td>11k LOC</td></tr>
<tr><td><a href="https://bintray.com/version/files/homebrew/mirror/berkeley-db/18.1.40">Oracle BDB KV Store</a></td><td>current</td><td>16k LOC</td></tr>
<tr><td><a href="https://github.com/btrask/libkvstore">libkvstore</a></td><td>2016</td><td>Defunct, but similar to the API that lumo-backend.h implements</td></tr>
</tbody></table>
<p>The other KV stores listed in the Knowledgebase are also all potential
backends, but we haven't spent enough time looking at them. </p>
<h1 id="distributed-or-clustered-codebases"><a class="header" href="#distributed-or-clustered-codebases">Distributed or Clustered Codebases</a></h1>
<p>The following five projects are widely-varying examples of how SQLite data can
be distributed, whether across just a few local nodes or across a much higher
number of internet-connected nodes.</p>
<table><thead><tr><th>Project</th><th>Last modified</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/rqlite/rqlite">rqlite</a></td><td>current</td><td>Distributed database with networking and Raft consensus on top of SQLite nodes</td></tr>
<tr><td><a href="https://github.com/Expensify/Bedrock">Bedrock</a></td><td>current</td><td>WAN-replicated blockchain multimaster database built on SQLite. Has MySQL emulation</td></tr>
<tr><td><a href="https://github.com/bloomberg/comdb2">Comdb</a></td><td>current</td><td>Clustered HA RDBMS built on SQLite and a forked old Sleepcat BDB, synchronous replication, stored procedures</td></tr>
<tr><td><a href="https://github.com/biokoda/actordb">ActorDB</a></td><td>current</td><td>SQLite with a data sharding/distribution system across clustered nodes. Each node stores data in LMDB, which is connected to SQLite at the SQLite WAL layer</td></tr>
<tr><td><a href="https://github.com/wal-g/wal-g">WAL-G</a></td><td>current</td><td>Backup/replication tool that intercepts the WAL journal log for each of Postgres, Mysql, MonogoDB and Redis</td></tr>
</tbody></table>
<p>Unlike all other solutions we have found, rqlite builds its replication on top of SQLite nodes rather than underneath the SQLite storage API layer.</p>
<p>ActorDB uses LMDB but still has a WAL.</p>
<p>WAL-G illustrates a useful side-effect of having a WAL, in that it can be used as a list of transactions for archival as well as replay reasons. A non-WAL storage databases such as LMDB can also generate transaction logs for these sorts of purpose, but they aren't for replay reasons. </p>
<p>Oracle BDB-SQL discussed in the previous section also has replication. </p>
<p>Comdb has a large team of active developers, and is the most ambitious of the SQLite-derived databases. Bloomberg LP created Comdb in 2004 and designed it for scale and yet with synchronous replication and full RDBMS functionality, the opposite of NoSQL solutions. SQLite has been ported to an old version of Sleepycat Berkely DB, which has been forked and extensively modified. The BDB interface is still largely used. This <a href="http://www.vldb.org/pvldb/vol9/p1377-scotti.pdf">2016 paper on Comdb</a> by the Bloomberg team is a very thorough introduction to the architecture and the BDB btree modifications. Comdb creates a VDBE layer abstraction such that VDBE operates on tables with rows, not a key-value store.</p>
<p>| <a href="https://github.com/kripken/sql.js/">sql.js</a> | current | SQLite compiled to JavaScript WebAssembly through Emscripten |</p>
<h1 id="modular-code-to-potentially-incorporate-in-lumosql"><a class="header" href="#modular-code-to-potentially-incorporate-in-lumosql">Modular Code to Potentially Incorporate in LumoSQL</a></h1>
<table><thead><tr><th>Project</th><th>Last modified</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/gdev2018/sqlite3odbc">sqlite3odbc</a></td><td>current</td><td>ODBC driver for SQLite by <a href="http://www.ch-werner.de/sqliteodbc/">Christian Werner</a> as used by many projects including LibreOffice</td></tr>
</tbody></table>
<p>SQLite3ODBC is a wrapper around the whole of the SQLite library. None of the
LumoSQL API interception points can be used for this, nevertheless, ODBC is an
important cross-platform standard heavily used on Windows and IBM's operating
systems. The potential benefit to this is that many Windows users would the be 
able to use LumoSQL as a drop-in data source like any other ODBC connector. The 
maintenance costs could well be quite low since it does not modify SQLite in any way.</p>
<p>| <a href="https://www.gaia-gis.it/fossil/libspatialite/index">Spatialite</a>| current | Geospatial GIS extension to SQLite, similar to PostGIS |</p>
<p><em>update:</em> sadly no, Spatialite cannot be incorporated into LumoSQL, but Spatialite
should be included in the LumoSQL test suite, as follows...</p>
<p>GIS features are a vertical use case, but one that is extreme popular and
widely-used, and increasingly as part of public information and journalism.
For example, PostGIS has very large numbers of users and would likely have been
merged into PostgreSQL long ago, except for incompatible licenses.  Spatialite
has a similar licensing problem, because it is only offered under copyleft
licenses including MPL and two GPLs, and so cannot be included as part of
LumoSQL. However because this is such an important use case, and because
Spatialite tracks LumoSQL so carefully, the LumoSQL test suite should include
building with Spatialite and running Spatialite's own test suite. This brings 
up another class of tests, because Spatialite isn't the only important 
source-available SQLite addon.</p>
<p>| <a href="https://github.com/gigimushroom/DatabaseBackendEngine">Gigimushroom's Database Backend Engine</a>|2019| A good example of an alternative BTree storage engine implemented using SQLite's Virtual Table Interface. This approach is not what LumoSQL has chosen for many reasons, but this code demonstrates virtual tables can work, and also that storage engines implemented at virtual tables can be ported to be LumoSQL backends.|</p>
<h1 id="on-disk-file-format-related-knowledge"><a class="header" href="#on-disk-file-format-related-knowledge">On-disk File Format-related Knowledge</a></h1>
<p>The on-disk file format is important to many SQLite use cases, and introspection tools are both important and rare. Other K-V stores also have third-party on-disk introspection tools. There are advantages to having investigative tools that do not use the original/canonical source code to read and write these databases. The SQLite file format is promoted as being a stable, backwards-compatible transport (recommend by the Library of Congress as an archive format) but it also has significant drawbacks as discussed elsewhere in the LumoSQL documentation.</p>
<table><thead><tr><th>Project</th><th>Last modified</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://www.sciencedirect.com/science/article/pii/S1742287618300471">A standardized corpus for SQLite database forensics</a></td><td>current</td><td>Sample SQLite databases and evaluations of 5 tools that do extraction and recovery from SQLite, including Undark and SQLite Deleted Records Parser</td></tr>
<tr><td><a href="https://github.com/fastogt/fastonosql">FastoNoSQL</a></td><td>current</td><td>GUI inspector and management tool for on-disk databases including LMDB and LevelDB</td></tr>
<tr><td><a href="https://github.com/inflex/undark">Undark</a></td><td>2016</td><td>SQLite deleted and corrupted data recovery tool</td></tr>
<tr><td><a href="https://github.com/mdegrazia/SQLite-Deleted-Records-Parser">SQLite Deleted Records Parser</a></td><td>2015</td><td>Script to recover deleted entries in an SQLite database</td></tr>
<tr><td><a href="https://github.com/catwell/cw-lua/tree/master/lua-mdb">lua-mdb</a></td><td>2016</td><td>Parse and investigate LMDB file format</td></tr>
</tbody></table>
<h1 id="list-of-relevant-benchmarking-and-test-knowledge-1"><a class="header" href="#list-of-relevant-benchmarking-and-test-knowledge-1">List of Relevant Benchmarking and Test Knowledge</a></h1>
<p>Benchmarking is a big part of LumoSQL, to determine if changes are an improvement. The trouble is that SQLite and other top databases are not really benchmarked in realistic and consistent way, despite SQL server benchmarking using tools like TPC being an obsessive industry in itself, and there being myriad of testing tools released with SQLite, Postgresql, MariaDB etc. But in practical terms there is no way of comparing the most-used databases with each other, or even of being sure that the tests that do exist are in any way realistic, or even of simply reproducing results that other people have found. LumoSQL covers so many codebases and use cases that better SQL benchmarking is a project requirement. Benchmarking and testing overlap, which is addressed in the code and docs.</p>
<p>The well-described <a href="https://sqlite.org/testing.html">testing of SQLite</a> involves some open code, some closed code, and many ad hoc processes. Clearly the SQLite team have an internal culture of testing that has benefitted the world. However that is very different to reproducible testing, which is in turn very different to reproducible benchmarking, and that is even without considering whether the benchmarking is a reasonable approximation of actual use cases.</p>
<p>To highlight how poorly SQL benchmarking is done: there are virtually no test harnesses that cover encrypted databases and/or encrypted database connections, despite encryption being frequently required, and despite crypto implementation decisions making a very big difference in performance.</p>
<table><thead><tr><th>Project</th><th>Last modified</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://www.cs.utexas.edu/%7Evijay/papers/apsys17-sqlite.pdf">Dangers and complexity of sqlite3 benchmarking</a></td><td>n/a</td><td>Helpful 2017 paper: &quot;...changing just one parameter in SQLite can change the performance by 11.8X... up to 28X difference in performance&quot;</td></tr>
<tr><td><a href="https://www.sqlite.org/sqllogictest/doc/trunk/about.wiki">sqllogictest</a></td><td>2017</td><td><a href="https://www.sqlite.org/sqllogictest/artifact/2c354f3d44da6356">sqlite.org code</a> to <a href="https://gerardnico.com/data/type/relation/sql/test">compare the results</a> of many SQL statements between multiple SQL servers, either SQLite or an ODBC-supporting server</td></tr>
<tr><td><a href="https://github.com/sqlite/sqlite/tree/master/test">TCL SQLite tests</a></td><td>current</td><td>These are a mixture of code coverage tests, unit tests and test coverage. Actively maintained.</td></tr>
<tr><td><a href="https://github.com/brianfrankcooper/YCSB/">Yahoo Cloud Serving Benchmark</a></td><td>current</td><td>Benchmarking tool for K-V stores and cloud-accessible databases</td></tr>
<tr><td><a href="https://github.com/greenrobot/android-database-performance">Example Android Storage Benchmark</a></td><td>2018</td><td>This code is an example of the very many Android benchmarking/testing tools. This needs further investigation</td></tr>
<tr><td><a href="https://github.com/akopytov/sysbench">Sysbench</a></td><td>current</td><td>A multithreaded generic benchmarking tool, with one well-supported use case being networked SQL servers, and <a href="https://www.percona.com/blog/2019/04/25/creating-custom-sysbench-scripts/">MySQL in particular</a></td></tr>
</tbody></table>
<h1 id="sqlite-and-encryption-issues"><a class="header" href="#sqlite-and-encryption-issues">SQLite and Encryption Issues</a></h1>
<p>Encryption is a major problem for SQLite users looking for open code. There are no official implementations in open source, although the APIs are documented (seemingly by an SCM mistake years ago (?), see sqlite3-dbx below) and most solutions use the SQLite extension interface. This means that there are many mutually-incompatible implementations, several of them seeming to be very popular. None appear to have received encryption certification (?) and none seem to publish test results to reassure users about compatibility with SQLite upstream or with the file format. Besides the closed source solution from sqlite.org, there are also at least three other closed source options not listed here. This choice between either closed source or fragmented solutions is a poor security approach from the point of view of maintenance as well as peer-reviewed security. This means that SQLite in 2020 does not have a good approach to privacy. Privacy is more than encryption, but encryption is an essential pre-requisite for privacy.</p>
<table><thead><tr><th>Project</th><th>Last modified</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://www.sqlite.org/see/doc/release/www/readme.wiki">SQLite Encryption Extension</a></td><td>current</td><td>Info about the (closed source) official SQLite crypto solution, illustrating that there is little to be compatible with in the wider SQLite landscape (although see the decade-old crypto implementation described in sqlite3-dbx below)</td></tr>
<tr><td><a href="https://github.com/sqlcipher/sqlcipher">SQLCipher</a></td><td>current</td><td>Adds at-rest encryption to SQLite <a href="https://www.zetetic.net/sqlcipher/design/">at the pager level</a>, using OpenSSL (the default) or optionally other providers</td></tr>
<tr><td><a href="https://github.com/resilar/sqleet">sqleet</a></td><td>current</td><td>Implements SHA256 encryption, also at the pager level</td></tr>
<tr><td><a href="https://github.com/newsoft/sqlite3-dbx">sqlite3-dbx</a></td><td>kinda-current</td><td>Interesting documentation that perhaps sqlite.org never meant to publish their crypto APIs?</td></tr>
<tr><td><a href="https://github.com/darkman66/SQLite3-Encryption">SQLite3-Encryption</a></td><td>current</td><td>No crypto libraries (DIY crypto!) and based on the similar-sounding official SQLite3-with-Encryption project</td></tr>
</tbody></table>
<p>While there are many more crypto projects, the architecture issues are the same for all of them. SQLCipher makes some very helpful comments in their <a href="https://www.zetetic.net/sqlcipher/design/">design document</a> under the heading Packaging:</p>
<blockquote>
<p>SQLCipher is an extension to SQLite, but it does not function as a loadable plugin for many reasons. Instead, SQLCipher modifies SQLite itself, and is maintained as a separate version of the source tree. SQLCipher releases are baselined against a specific source version of SQLite. However, the project minimizes alterations to core SQLite code to reduce the risk of breaking changes during upstream SQLite merges.</p>
<p>The reasons that SQLCipher is packaged this way, as opposed to a &quot;plugin&quot; or extension to the SQLite amalgamation, follow:</p>
<ul>
<li>Enabling an SQLite codec requires the compile-time definition of SQLITE_HAS_CODEC, which is not present on standard, unmodified SQLite builds.</li>
<li>Even when enabled, SQLite isn't setup to load codecs as plugins. While SQLite does have a plugin function for loadable extensions, it does not extend access to any system internals (it mainly used to allow custom user functions).</li>
<li>SQLCipher makes calls to internal functions that are not part of the public SQLite API. Sometimes these APIs change, even in between minor SQLite versions. Thus, each update adn merge requires inspection, testing and verification. Making SQLCipher portable across multiple versions of SQLite would not be feasible, nor could it to use only the public API (for instance, even the first critical step of attaching the codec callback to the pager uses an internal API).</li>
<li>SQLCipher modifies supporting functions to introduce special pragmas, built in functions, etc (e.g. PRAGMA cipher_*). Injecting this functionality in a plugin architecture wouldn't be possible.</li>
<li>SQLCipher's test harness relies on support in testfixture to take advantage of the test API and various internal checks (memory reference counting, etc.)</li>
<li>Even if it were possible to use a loadable plugin, dynamic libraries aren't available on all supported platforms, for example iOS</li>
</ul>
</blockquote>
<p>This last point is generally relevant to LumoSQL, being one good reason among several why &quot;just write a VFS module&quot; isn't the answer to what LumoSQL is trying to do.</p>
<p>Comparing all these projects, it seems important to consider whether page-level
encryption and page-level locking should be looked at as closely related. The
only page-level locking key-value store there is to study is the modified
Berkeley DB underneath Comdb2 (ibid); anything else will need to be done with a
modified LMDB or possibly Btree. There is no evidence that the official SQLite
SEE solution does anything to the page level, but that is being followed up.
Furthermore we need to look at metadata store (because otherwise we can only
guess whether a given series of bytes is an encrypted database or not. </p>
<h1 id="list-of-from-scratch-mysql-sql-and-mysql-server-implementations"><a class="header" href="#list-of-from-scratch-mysql-sql-and-mysql-server-implementations">List of from-scratch MySQL SQL and MySQL Server implementations</a></h1>
<p>If we want to make SQLite able to process MySQL queries there is a lot of existing code in this area to consider. There are at least 80 projects on github which implement some or all of the MySQL network-parse-optimise-execute SQL pathway, a few of them implement all of it. None so far reviewed used MySQL or MariaDB code to do so. Perhaps that is because the SQL processing code alone in these databases is many times bigger than the whole of SQLite, and it isn't even clear how to add them to this table if we wanted to. Only a few of these projects put a MySQL frontend on SQLite, but two well-maintained projects do, showing us two ways of implementing this.</p>
<table><thead><tr><th>Project</th><th>Last modified</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/Expensify/Bedrock">Bedrock</a></td><td>current</td><td>The MySQL compatibility seems to be popular and is actively supported but it is also small. It speaks the MySQL/MariaDB protocol accurately but doesn't seem to try very hard to match MySQL SQL language semantics and extensions, rather relying on the fact that SQLite substantially overlaps with MySQL.</td></tr>
<tr><td><a href="https://github.com/pingcap/tidb/">TiDB</a></td><td>current</td><td>Distributed database with MySQL emulation as the primary dialect and referred to throughout the code, with frequent detailed bugfixes on deviations from MySQL SQL language behaviour.</td></tr>
<tr><td><a href="https://github.com/phpmyadmin/sql-parser">phpMyAdmin parser</a></td><td>current</td><td>A very complete parser for MySQL code, demonstrating that completeness is not the unrealistic goal some claim it to be</td></tr>
<tr><td><a href="https://github.com/src-d/go-mysql-server">Go MySQL Server</a></td><td>current</td><td>A MySQL server written in Go that executes queries but mostly leaves the backend for the user to implement. Intended to put a compliant MySQL server on top of arbitrary backend sources.</td></tr>
<tr><td><a href="https://github.com/ClickHouse/ClickHouse/tree/146109fe27074229a38cd704d60f23ec7bd2ed67/base/mysqlxx">ClickHouse MySQL Frontend</a></td><td>current</td><td>Yandex' <a href="https://clickhouse.tech/">Clickhouse</a> has a MySQL frontend.</td></tr>
</tbody></table>
<div style="break-before: page; page-break-before: always;"></div><!-- SPDX-License-Identifier: CC-BY-SA-4.0 -->
<!-- SPDX-FileCopyrightText: 2020 The LumoSQL Authors -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Dan Shearer, 2019 -->
<h1 id="table-of-contents-9"><a class="header" href="#table-of-contents-9">Table of Contents</a></h1>
<ul>
<li><a href="2.4-relevant-knowledgebase.html#knowledge-relevant-to-lumosql">Knowledge Relevant to LumoSQL</a></li>
<li><a href="2.4-relevant-knowledgebase.html#list-of-sqlite-code-related-knowledge">List of SQLite Code-related Knowledge</a></li>
<li><a href="2.4-relevant-knowledgebase.html#list-of-mvcc-capable-kv-store-related-knowledge">List of MVCC-capable KV store-related Knowledge</a></li>
<li><a href="2.4-relevant-knowledgebase.html#list-of-on-disk-sqlite-format-related-knowledge">List of On-disk SQLite Format-related Knowledge</a></li>
<li><a href="2.4-relevant-knowledgebase.html#list-of-relevant-sql-checksumming-related-knowledge">List of Relevant SQL Checksumming-related Knowledge</a></li>
<li><a href="2.4-relevant-knowledgebase.html#list-of-relevant-benchmarking-and-test-knowledge">List of Relevant Benchmarking and Test Knowledge</a></li>
<li><a href="2.4-relevant-knowledgebase.html#list-of-just-a-few-sqlite-encryption-projects">List of Just a Few SQLite Encryption Projects</a></li>
<li><a href="2.4-relevant-knowledgebase.html#list-of-from-scratch-mysql-sql-and-mysql-server-implementations">List of from-scratch MySQL SQL and MySQL Server implementations</a></li>
</ul>
<h1 id="knowledge-relevant-to-lumosql"><a class="header" href="#knowledge-relevant-to-lumosql">Knowledge Relevant to LumoSQL</a></h1>
<p>LumoSQL has many antecedents and relevant codebases.  This document is intended
to be a terse list of published source code for reference of LumoSQL
developers. Although it is stored with the rest of the LumoSQL documentation
and referred to throughout, it is a standalone document.</p>
<p>Everything listed here is open source, except for software produced by
sqlite.org or the commercial arm hwaci.com. There are many closed-source
products that extend and reuse SQLite in various ways, none of which have been
considered by the LumoSQL project.</p>
<h1 id="list-of-sqlite-code-related-knowledge"><a class="header" href="#list-of-sqlite-code-related-knowledge">List of SQLite Code-related Knowledge</a></h1>
<p>SQLite code has been incorporated into many other projects, and besides there are many other relevant key-value stores and libraries.</p>
<table><thead><tr><th>Project</th><th>Last modified</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/LMDB/sqlightning">sqlightning</a></td><td>2013</td><td>SQLite ported to the LMDB key-value store; all in C</td></tr>
<tr><td><a href="https://www.openldap.org/pub/hyc/mdb-paper.pdf">Original MDB Paper</a></td><td>2012</td><td>Paper by Howard Chu describing the motivations, design and constraints of the LMDB key-value store</td></tr>
<tr><td><a href="https://github.com/btrask/sqlheavy">SQLHeavy</a></td><td>2016</td><td>sqlightning updated, and ported to LevelDB, LMDB, RocksDB and more, with a key-value store library abstraction; all in C</td></tr>
<tr><td><a href="https://sqlite.org/src4/tree?ci=trunk">SQLite 4</a></td><td>2014</td><td>Abandoned new version of SQLite with improved backend support and other features</td></tr>
<tr><td><a href="https://bintray.com/version/files/homebrew/mirror/berkeley-db/18.1.32">Oracle BDB port of SQLite</a></td><td>2020</td><td>The original ubiquitous Unix K-V store, disused in open source since Oracle's 2013 license change to AGPL; the API template for most of the k-v btree stores around. Now includes many additional features including full MVCC transactions, networking and replication. This link is a mirror of code from download.oracle.com, which requires a login. This is version 18.1.32, which was the last with a port of SQLite</td></tr>
<tr><td><a href="https://fossies.org/linux/misc/db-18.1.32.tar.gz">Sleepycat/Oracle BDB-SQL</a></td><td>current</td><td>Port of SQLite to the Sleepycat/Oracle transactional bdb K-V store. As of 5th March 2020 this mirror is identical to Oracle's login-protected tarball for db 18.1.32</td></tr>
<tr><td><a href="https://github.com/rqlite/rqlite">rqlite</a></td><td>current</td><td>Distributed database with networking and Raft consensus on top of SQLite nodes</td></tr>
<tr><td><a href="https://github.com/Expensify/Bedrock">Bedrock</a></td><td>current</td><td>WAN-replicated blockchain multimaster database built on SQLite. Has MySQL emulation</td></tr>
<tr><td><a href="https://github.com/bloomberg/comdb2">Comdb</a></td><td>current</td><td>Clustered HA RDBMS built on SQLite and a forked old Sleepcat BDB, synchronous replication, stored procedures</td></tr>
<tr><td><a href="https://github.com/kripken/sql.js/">sql.js</a></td><td>current</td><td>SQLite compiled to JavaScript WebAssembly through Emscripten</td></tr>
<tr><td><a href="https://github.com/biokoda/actordb">ActorDB</a></td><td>current</td><td>SQLite with a data sharding/distribution system across clustered nodes. Each node stores data in LMDB, which is connected to SQLite at the SQLite WAL layer</td></tr>
<tr><td><a href="https://github.com/wal-g/wal-g">WAL-G</a></td><td>current</td><td>Backup/replication tool that intercepts the WAL journal log for each of Postgres, Mysql, MonogoDB and Redis</td></tr>
<tr><td><a href="https://github.com/gdev2018/sqlite3odbc">sqlite3odbc</a></td><td>current</td><td>ODBC driver for SQLite by <a href="http://www.ch-werner.de/sqliteodbc/">Christian Werner</a> as used by many projects including LibreOffice</td></tr>
<tr><td><a href="https://www.gaia-gis.it/fossil/libspatialite/index">Spatialite</a></td><td>current</td><td>Geospatial GIS extension to SQLite, similar to PostGIS</td></tr>
<tr><td><a href="https://github.com/gigimushroom/DatabaseBackendEngine">Gigimushroom's Database Backend Engine</a></td><td>2019</td><td>A good example of an alternative BTree storage engine implemented using SQLite's Virtual Table Interface. This approach is not what LumoSQL has chosen for many reasons, but this code demonstrates virtual tables can work, and also that storage engines implemented at virtual tables can be ported to be LumoSQL backends.</td></tr>
</tbody></table>
<h1 id="list-of-mvcc-capable-kv-store-related-knowledge"><a class="header" href="#list-of-mvcc-capable-kv-store-related-knowledge">List of MVCC-capable KV store-related Knowledge</a></h1>
<p>There are hundreds of active K-V stores, but few of them do
<a href="https://en.wikipedia.org/wiki/Multiversion_concurrency_control">MVCC</a>. MVCC is
a requirement for any performant backend to SQLite, because although SQLite
does do transaction management at the SQL level, it assumes the KV store will
handle contention. SQLite translates SQL transactions into KV store
transactions. There are many interesting KV stores that target use cases that
do not need or want MVCC listed in <em>Section 8 Related Work</em> in 
<a href="https://www.microsoft.com/en-us/research/uploads/prod/2018/03/faster-sigmod18.pdf">Microsoft's SIGMOD paper on the FASTER KV store</a> 
including Redis, RocksDB, Aerospike, etc.  We have done little source code
review of non-MVCC KV stores, and have not considered them for LumoSQL
backends.</p>
<table><thead><tr><th>Project</th><th>Last modified</th><th>Description</th></tr></thead><tbody>
<tr><td>SQLite's built-in KV store</td><td>current</td><td><strong>Included as a reminder</strong> that SQLite is <em>also</em> the world's most widely-used key-value store, and it does full MVCC. LumoSQL plans to expose this via an API for the first time</td></tr>
<tr><td><a href="https://github.com/LMDB/">LMDB</a></td><td>current</td><td>Maintained by the OpenLDAP project, LMDB is a standalone KV store written in C, and is the only KV store that is implemented using mmap. LMDB is the first backend target for LumoSQL</td></tr>
<tr><td><a href="https://bintray.com/version/files/homebrew/mirror/berkeley-db/18.1.40">Oracle BDB KV Store</a></td><td>2020</td><td>Version 18.1.40 (the link given here) and later of BDB do not include the SQLite port, but they are still the same MVCC KV store. LumoSQL may consider continuing to develop the SQLite port.</td></tr>
<tr><td><a href="https://github.com/btrask/libkvstore">libkvstore</a></td><td>2016</td><td>The k-v store abstraction library used by SQLHeavy</td></tr>
<tr><td><a href="https://github.com/bloomberg/comdb2">Comdb's BDB fork</a></td><td>current</td><td>The BDB fork has had row-level locking added to it and has other interesting features besides being the second SQLite-on-BDB port</td></tr>
<tr><td><a href="https://github.com/malbrain/database/wiki">Karl Malbrain's DB</a></td><td>current</td><td>This C library appears to be a maintained and well-researched and documented KV store, but without widespread deployment it is unlikely to be competitive. Perhaps that is unfair, and LumoSQL hopes to find out. The code suggests some interesting approaches to lockless design</td></tr>
<tr><td><a href="www.cs.cmu.edu/%7E./pavlo/papers/p781-wu.pdf">Serial Safety Net Concurrency</a></td><td>2015</td><td>Karl Malbrain implements the Serial Safety Net (SSN) concurrency control protocol discussed in this paper. This paper also gives an analysis of other concurrency control methods including timestamps.</td></tr>
<tr><td><a href="https://github.com/dgraph-io/badger">Badger</a></td><td>current</td><td>Written in Go. Implements MVCC using a separate WAL file for each of its memory tables. Untested and only reviewed briefly.</td></tr>
</tbody></table>
<h1 id="list-of-on-disk-sqlite-format-related-knowledge"><a class="header" href="#list-of-on-disk-sqlite-format-related-knowledge">List of On-disk SQLite Format-related Knowledge</a></h1>
<p>The on-disk file format is important to many SQLite use cases, and introspection tools are both important and rare. Other K-V stores also have third-party on-disk introspection tools. There are advantages to having investigative tools that do not use the original/canonical source code to read and write these databases. The SQLite file format is promoted as being a stable, backwards-compatible transport (recommend by the Library of Congress as an archive format) but it also has significant drawbacks as discussed elsewhere in the LumoSQL documentation.</p>
<table><thead><tr><th>Project</th><th>Last modified</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://www.sciencedirect.com/science/article/pii/S1742287618300471">A standardized corpus for SQLite database forensics</a></td><td>current</td><td>Sample SQLite databases and evaluations of 5 tools that do extraction and recovery from SQLite, including Undark and SQLite Deleted Records Parser</td></tr>
<tr><td><a href="https://github.com/fastogt/fastonosql">FastoNoSQL</a></td><td>current</td><td>GUI inspector and management tool for on-disk databases including LMDB and LevelDB</td></tr>
<tr><td><a href="https://github.com/inflex/undark">Undark</a></td><td>2016</td><td>SQLite deleted and corrupted data recovery tool</td></tr>
<tr><td><a href="https://github.com/mdegrazia/SQLite-Deleted-Records-Parser">SQLite Deleted Records Parser</a></td><td>2015</td><td>Script to recover deleted entries in an SQLite database</td></tr>
<tr><td><a href="https://github.com/catwell/cw-lua/tree/master/lua-mdb">lua-mdb</a></td><td>2016</td><td>Parse and investigate LMDB file format</td></tr>
</tbody></table>
<p>(The forensics and data recovery industry has many tools that diagnose SQLite
database files. Some are open source but many are not. A list of tools commonly
cited by forensics practicioners, none of which LumoSQL has downloaded or tried
is: Belkasoft Evidence Center, BlackBag BlackLight, Cellebrite UFED Physical
Analyser, DB Browser for SQLite, Magnet AXIOM and Oxygen Forensic Detective.)</p>
<h1 id="list-of-relevant-sql-checksumming-related-knowledge"><a class="header" href="#list-of-relevant-sql-checksumming-related-knowledge">List of Relevant SQL Checksumming-related Knowledge</a></h1>
<table><thead><tr><th>Project</th><th>Last modified</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/XKCP/XKCP">eXtended Keccak Code Package</a></td><td>current</td><td>Code from https://keccak.team for very fast peer-reviewed hashing</td></tr>
<tr><td><a href="https://www.periscopedata.com/blog/hashing-tables-to-ensure-consistency-in-postgres-redshift-and-mysql">SQL code for Per-table Multi-database Solution</a></td><td>2014</td><td>Periscope's SQL row hashing solution for Postgres, Redshift and MySQL</td></tr>
<tr><td><a href="https://www.percona.com/blog/2018/10/12/track-postgresql-row-changes-using-public-private-key-signing/">SQL code for Public Key Row Tracking</a></td><td>2018</td><td>Percona's SQL row integrity solution for Postgresql using public key crypto</td></tr>
<tr><td><a href="https://sqlite.org/cksumvfs.html">Richard Hipp's SQLite Checksum VFS</a></td><td>2019</td><td>This shows the potential benefits from maintaining checksums. There are many limitations, but its more than any other mainstream database ships by default</td></tr>
</tbody></table>
<h1 id="list-of-relevant-benchmarking-and-test-knowledge-2"><a class="header" href="#list-of-relevant-benchmarking-and-test-knowledge-2">List of Relevant Benchmarking and Test Knowledge</a></h1>
<p>Benchmarking is a big part of LumoSQL, to determine if changes are an
improvement. The trouble is that SQLite and other top databases are not really
benchmarked in realistic and consistent way, despite SQL server benchmarking
using tools like TPC being an obsessive industry in itself, and there being
myriad of testing tools released with SQLite, Postgresql, MariaDB etc. But in
practical terms there is no way of comparing the most-used databases with each
other, or even of being sure that the tests that do exist are in any way
realistic, or even of simply reproducing results that other people have found.
LumoSQL covers so many codebases and use cases that better SQL benchmarking is
a project requirement. Benchmarking and testing overlap, which is addressed in
the code and docs.</p>
<p>The well-described <a href="https://sqlite.org/testing.html">testing of SQLite</a>
involves some open code, some closed code, and many ad hoc processes. Clearly
the SQLite team have an internal culture of testing that has benefited the
world. However that is very different to reproducible testing, which is in turn
very different to reproducible benchmarking, and that is even without
considering whether the benchmarking is a reasonable approximation of actual
use cases. As the development of LumoSQL has proceeded, it has become clear
that the TCL testing harness shipped with SQLite code contains specific
dependencies on the behaviour of the SQLite btree backend. While LumoSQL with
the original btree backend aims to always pass these tests, differences such as
locking behaviour, assumed key lengths, and even the number of database files a
backend uses all mean that the SQLite TCL test suite is not generally useful.</p>
<p>To highlight how poorly SQL benchmarking is done: there are virtually no test
harnesses that cover encrypted databases and/or encrypted database connections,
despite encryption being frequently required, and despite crypto implementation
decisions making a very big difference in performance. Encryption and security are
not the only ways a database impacts privacy, so privacy is a valid dimension for 
database testing - and a fundamental goal for LumoSQL. Testing all databases in 
the same way for privacy is challenging.</p>
<p>SQL testing is also very difficult. As the Regression Testing paper below says:
&quot;A problem with testing SQL in DBMSs lies in the fact that the state of the
database must be considered when deducing testing outcomes&quot;. SQL statements
frequently change the state of the server during their execution, in different
ways on different servers. This can change the behaviour or at least the
performance of the next statement, and so on.</p>
<table><thead><tr><th>Project</th><th>Last modified</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://www.cs.utexas.edu/%7Evijay/papers/apsys17-sqlite.pdf">Dangers and complexity of sqlite3 benchmarking</a></td><td>n/a</td><td>Helpful 2017 paper: &quot;...changing just one parameter in SQLite can change the performance by 11.8X... up to 28X difference in performance&quot;</td></tr>
<tr><td><a href="https://www.sqlite.org/sqllogictest/doc/trunk/about.wiki">sqllogictest</a></td><td>2017</td><td><a href="https://www.sqlite.org/sqllogictest/artifact/2c354f3d44da6356">sqlite.org code</a> to <a href="https://gerardnico.com/data/type/relation/sql/test">compare the results</a> of many SQL statements between multiple SQL servers, either SQLite or an ODBC-supporting server</td></tr>
<tr><td><a href="https://github.com/sqlite/sqlite/tree/master/test">TCL SQLite tests</a></td><td>current</td><td>These are a mixture of code covereage tests, unit tests and test coverage. Actively maintained.</td></tr>
<tr><td><a href="https://github.com/brianfrankcooper/YCSB/">Yahoo Cloud Serving Benchmark</a></td><td>current</td><td>Benchmarking tool for K-V stores and cloud-accessible databases</td></tr>
<tr><td><a href="https://github.com/greenrobot/android-database-performance">Example Android Storage Benchmark</a></td><td>2018</td><td>This code is an example of the very many Android benchmarking/testing tools. This needs further investigation</td></tr>
<tr><td><a href="https://github.com/akopytov/sysbench">Sysbench</a></td><td>current</td><td>A multithreaded generic benchmarking tool, with one well-supported use case being networked SQL servers, and <a href="https://www.percona.com/blog/2019/04/25/creating-custom-sysbench-scripts/">MySQL in particular</a></td></tr>
<tr><td><a href="https://www.diva-portal.org/smash/get/diva2:736996/FULLTEXT01.pdf">Regression Testing of SQL</a></td><td>n/a</td><td>2014 paper &quot;a framework for minimizing the required developer effort formanaging and running SQL regression tests&quot;</td></tr>
<tr><td><a href="https://pdfs.semanticscholar.org/c2da/33304627649b599f80a5428354e116ba6201.pdf">Enhancing the Performance of SQLite</a></td><td>n/a</td><td>2019 paper that does profiling and develops performance testing metrics for SQLite</td></tr>
<tr><td><a href="https://github.com/microsoft/sqlite-tracer">SQLite Profiler and Tracer</a></td><td>2018</td><td>Microsoft SQLite statement profiler and timer, helpful for comparing LumoSQL backends</td></tr>
<tr><td><a href="https://discuss.zetetic.net/t/sqlcipher-performance-optimization/14">SQLCipher Performance Optimisation</a></td><td>n/a</td><td>2014 comments on the additional performance metric problems that come with SQLite encryption</td></tr>
<tr><td><a href="https://sci-hub.se/10.1007/s10617-014-9149-2">Performance analysis ... on flash file systems</a></td><td>2019</td><td>Discussion of SQLite and 2 others on Flash, examining the cost to flash of SQL operations</td></tr>
</tbody></table>
<h1 id="list-of-just-a-few-sqlite-encryption-projects"><a class="header" href="#list-of-just-a-few-sqlite-encryption-projects">List of Just a Few SQLite Encryption Projects</a></h1>
<p>Encryption is a major problem for SQLite users looking for open code. There are no official implementations in open source, although the APIs are documented (seemingly by an SCM mistake years ago (?), see sqlite3-dbx below) and most solutions use the SQLite extension interface. This means that there are many mutually-incompatible implementations, several of them seeming to be very popular. None appear to have received encryption certification (?) and none seem to publish test results to reassure users about compatibility with SQLite upstream or with the file format. Besides the closed source solution from sqlite.org, there are also at least three other closed source options not listed here. This choice between either closed source or fragmented solutions is a poor security approach from the point of view of maintenance as well as peer-reviewed security. This means that SQLite in 2020 does not have a good approach to privacy.</p>
<table><thead><tr><th>Project</th><th>Last modified</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://www.sqlite.org/see/doc/release/www/readme.wiki">SQLite Encryption Extension</a>(SEE)</td><td>current</td><td>Info about the proprietary, closed source official SQLite crypto solution, illustrating that there is little to be compatible with in the wider SQLite landscape. This is a standalone product. The API is published and used by some open source code.</td></tr>
<tr><td><a href="https://github.com/sqlcipher/sqlcipher">SQLCipher</a></td><td>current</td><td>Adds at-rest encryption to SQLite <a href="https://www.zetetic.net/sqlcipher/design/">at the pager level</a>, using OpenSSL (the default) or optionally other providers. Uses an open core licensing model, and the less-capable open source version is BSD licensed with a requirement that users publish copyright notices. Uses the SEE API.</td></tr>
<tr><td><a href="https://docs.oracle.com/cd/E17276_01/html/bdb-sql/sql_encryption.html">Oracle BDB Encryption</a></td><td>2018</td><td>Exposes the (old and insecure) BDB encryption facilities via the SEE API with one minor change.</td></tr>
<tr><td><a href="https://github.com/resilar/sqleet">sqleet</a></td><td>current</td><td>Implements SHA256 encryption, also at the pager level. Public Domain (not Open Source, similar to SQLite)</td></tr>
<tr><td><a href="https://github.com/newsoft/sqlite3-dbx">sqlite3-dbx</a></td><td>kinda-current</td><td>Accidentally-published but unretracted code on sqlite.org fully documents crypto APIs used by SEE</td></tr>
<tr><td><a href="https://github.com/darkman66/SQLite3-Encryption">SQLite3-Encryption</a></td><td>current</td><td>No crypto libraries (DIY crypto!) and based on the similar-sounding SQLite3-with-Encryption project</td></tr>
<tr><td><a href="https://github.com/utelle/wxsqlite3/">wxSqlite3</a></td><td>current</td><td>wxWidgets C++ wrapper, that also implements SEE-equivalent crypto. Licensed under the LGPL</td></tr>
<tr><td><a href="https://github.com/LMDB/lmdb/tree/mdb.master3">LMDB v1.0pre with encryption</a></td><td>current</td><td>The LMDB branch mdb.master3 is a prerelease of LMDBv1.0 with page-level encryption. This could be used in an architecturally similar way to the role of BDB encryption in the Oracle BDB+SQLite port</td></tr>
<tr><td><a href="https://github.com/resilar/sqleet/issues/12">Discussion of SQLCipher vs sqleet</a></td><td>2019</td><td>Authors of sqleet and wxSQLite3 discuss SQLCipher, covering many weaknesses and some strengths</td></tr>
</tbody></table>
<p>... there are many more crypto projects for SQLite. </p>
<h1 id="list-of-from-scratch-mysql-sql-and-mysql-server-implementations-1"><a class="header" href="#list-of-from-scratch-mysql-sql-and-mysql-server-implementations-1">List of from-scratch MySQL SQL and MySQL Server implementations</a></h1>
<p>If we want to make SQLite able to process MySQL queries there is a lot of existing code in this area to consider. There are at least 80 projects on github which implement some or all of the MySQL network-parse-optimise-execute SQL pathway, a few of them implement all of it. None so far reviewed used MySQL or MariaDB code to do so. Perhaps that is because the SQL processing code alone in these databases is many times bigger than the whole of SQLite, and it isn't even clear how to add them to this table if we wanted to. Only a few of these projects put a MySQL frontend on SQLite, but two well-maintained projects do, showing us two ways of implementing this.</p>
<table><thead><tr><th>Project</th><th>Last modified</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://github.com/Expensify/Bedrock">Bedrock</a></td><td>current</td><td>The MySQL compatibility seems to be popular and is actively supported but it is also small. It speaks the MySQL/MariaDB protocol accurately but doesn't seem to try very hard to match MySQL SQL language semantics and extensions, rather relying on the fact that SQLite substantially overlaps with MySQL.</td></tr>
<tr><td><a href="https://github.com/pingcap/tidb/">TiDB</a></td><td>current</td><td>Distributed database with MySQL emulation as the primary dialect and referred to throughout the code, with frequent detailed bugfixes on deviations from MySQL SQL language behaviour.</td></tr>
<tr><td><a href="https://github.com/phpmyadmin/sql-parser">phpMyAdmin parser</a></td><td>current</td><td>A very complete parser for MySQL code, demonstrating that completeness is not the unrealistic goal some claim it to be</td></tr>
<tr><td><a href="https://github.com/src-d/go-mysql-server">Go MySQL Server</a></td><td>current</td><td>A MySQL server written in Go that executes queries but mostly leaves the backend for the user to implement. Intended to put a compliant MySQL server on top of arbitrary backend sources.</td></tr>
<tr><td><a href="https://github.com/ClickHouse/ClickHouse/tree/146109fe27074229a38cd704d60f23ec7bd2ed67/base/mysqlxx">ClickHouse MySQL Frontend</a></td><td>current</td><td>Yandex' <a href="https://clickhouse.tech/">Clickhouse</a> has a MySQL frontend.</td></tr>
</tbody></table>
<div style="break-before: page; page-break-before: always;"></div><!--- SPDX-License-Identifier: CC-BY-SA-4.0 --->
<!--- SPDX-FileCopyrightText: 2020 The LumoSQL Authors --->
<!--- SPDX-ArtifactOfProjectName: LumoSQL --->
<!--- SPDX-FileType: Documentation --->
<!--- SPDX-FileComment: Original by Dan Shearer, 2020 --->
<h1 id="table-of-contents-10"><a class="header" href="#table-of-contents-10">Table of Contents</a></h1>
<ul>
<li><a href="3.6-development-notes.html#lumosql-2019-prototype-conclusions-and-lessons">LumoSQL 2019 Prototype Conclusions And Lessons</a>
* <a href="3.6-development-notes.html#facts-established-by-2019-lumosql-prototype">Facts Established by 2019 LumoSQL Prototype</a>
* <a href="3.6-development-notes.html#lessons-learned-from-sqlightning">Lessons Learned from sqlightning</a></li>
</ul>
<p><img src="./images/lumo-ecosystem-intro.png" alt="" title="LumoSQL logo" /></p>
<h1 id="lumosql-2019-prototype-conclusions-and-lessons"><a class="header" href="#lumosql-2019-prototype-conclusions-and-lessons">LumoSQL 2019 Prototype Conclusions And Lessons</a></h1>
<p>The original LumoSQL question was: “does the 2013 sqlightning work still stand
up, and does it represent a way forward?” </p>
<p>The answers are many kinds of both Yes and No:</p>
<ol>
<li><strong>Yes</strong>, with some porting and archeology, sqlightning does still
work and is relevant. It did move the world forward, it just took a while.</li>
<li><strong>No</strong>, SQLite in 2020 is not vastly slower than sqlightning, as it
was in 2013. SQLite has improved its btreee code. We have not yet
run the concurrency testing which seems likely to show benefits for
using LMDB.</li>
<li><strong>Yes</strong>, sqlightning does represent a way forward to a more robust,
private and secure internet. This is no longer an argument about
pure speed, although LumoSQL is not slower than SQLite as far as we
know to date.</li>
<li><strong>No</strong>, nobody can be really definitive about SQLite this way or
that, because of the dearth of benchmarking or testing and complete
absence of published results. We have started to address this
already. </li>
<li><strong>Yes</strong>, LMDB underneath SQLite has some key benefits over SQLite
including in concurrency support and file integrity.</li>
<li><strong>No</strong>, we are not going to carry forward any of the LMDB prototype code. 
But it will forever live on fondly in our hearts.</li>
</ol>
<h2 id="facts-established-by-2019-lumosql-prototype"><a class="header" href="#facts-established-by-2019-lumosql-prototype">Facts Established by 2019 LumoSQL Prototype</a></h2>
<p>Using both technical and non-technical means, the LumoSQL Prototype project
established in a matter of a few person-weeks that:</p>
<ul>
<li>
<p>The SQLite project has built-in problems (some of them in this chapter)
that LumoSQL can address in part, while remaining a compatible superset of SQLite
at both API and on-disk level, and not forking SQLite.</p>
</li>
<li>
<p>The porting and archeology involved in getting sqlightning going as
an LMDB backend to SQLite was quite a lot more work than taking existing working
code for other K-V store backends, most of which are currently maintained and in use. </p>
</li>
<li>
<p>All major SQL databases including SQLite suffer from historical
architecture decisions, including the use of Write Ahead Logs to do
concurrency, and lack of validation by default at the data store
level. This is a particularly big failing for SQLite given its emphasis on 
file format on IoT devices.</p>
</li>
<li>
<p>All major SQL databases other than SQLite are in the millions of
lines of source code, compared to SQLite at 350k SLOC . The
difference is not due to the embedded use case. Some are tens of millions.</p>
</li>
<li>
<p>There is a great lack of published, systematic test results for multiple databases. There is no
online evidence of SQLite tests being run regularly, and there
are no published results from running SQLite test code . The same is true
for all the other major databases. Even with continuous integration tools,
do any open source database developers include testing in their published
CI results? We could not find any.</p>
</li>
<li>
<p>There is no published SQLite benchmarking. LumoSQL has done some,
and published it. We plan to do a lot more.</p>
</li>
<li>
<p>Benchmarking databases in a way which both reflects the real world and is
also repeatable and general is difficult, and while SQLite is easier than
the other databases because of its simplicity, the paper <a href="https://www.cs.utexas.edu/%7Evijay/papers/apsys17-sqlite.pdf">Dangers and
complexity of sqlite3
benchmarking</a>
highlights the difficulty &quot;...changing just one parameter in SQLite can change
the performance by 11.8X... up to 28X difference in performance&quot;.  We have
developed some benchmarking solutions we think will practically benefit
thousands of projects used on the internet, and hopefully change their practice
based on information they can verify for themselves.</p>
</li>
</ul>
<h2 id="lessons-learned-from-sqlightning"><a class="header" href="#lessons-learned-from-sqlightning">Lessons Learned from sqlightning</a></h2>
<ul>
<li>
<p>LMDB, SQLite Btree.c, BDB and ComDB are all quite similar transactional
KV stores compared to all others in open source, and all written in C. 
We are sure that with a little wrapping and perhaps one bugfix, LMDB
can fully express the SQLite KV store semantics and vice versa. The same
is true for BDB.</p>
</li>
<li>
<p>btree.c:sqlite3BtreeBeginTrans had some internal LMDB cursor
structures. Rewrote using the LMDB API instead; more tests passed .</p>
</li>
<li>
<p>The SQL spec allows a transaction to be opened without specifying whether it is RO
or RW. The sqlightning code handled the upgrade case of RO-&gt;RW by copying the
transaction to a new (larger) RW structure, copying its cursors and restarting.
This results in an &quot;internal error&quot; when the original btree.c returns &quot;database
is locked&quot;; we have now fixed this bug in the modified btree.c to match the behaviour
of the original btree.c . </p>
</li>
<li>
<p>There are only limited tests available for SQLite (visible in public)
that exercise concurrency, races and deadlocks. There is a lot of
scope for these sorts of problems and we need to address them at many
levels including testing specifically for this.</p>
</li>
<li>
<p>SQLite does not have sophisticated concurrent transaction handling
compared to Postgresql, Oracle and MariaDB etc, being much more ready
to return 'LOCKED' rather than some concurrency algorithms. We will
return to this problem in later versions of LumoSQL.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><!-- SPDX-License-Identifier: CC-BY-SA-4.0 -->
<!-- SPDX-FileCopyrightText: 2020 The LumoSQL Authors -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Dan Shearer, 2020 -->
<h1 id="table-of-contents-11"><a class="header" href="#table-of-contents-11">Table of Contents</a></h1>
<ul>
<li><a href="api.html#sqlite-api-interception-points">SQLite API Interception Points</a></li>
</ul>
<p><img src="./images/lumo-implementation-intro.jpg" alt="" title="Metro Station Construction Futian Shenzhen China, CC license, https://www.flickr.com/photos/dcmaster/36740345496" /></p>
<h2 id="sqlite-api-interception-points"><a class="header" href="#sqlite-api-interception-points">SQLite API Interception Points</a></h2>
<p>LumoSQL identifies choke points at which APIs can be intercepted to provide modular choices for backends, front-end parcers, encryption and networking.</p>
<p>The API interception points are:</p>
<ol>
<li>
<p>Setup APIs/commandline/Pragmas, where we pass in info about what
front/backends we want to use or initialise. Noting that SQLite is
zero-config and so supplying no information to LumoSQL must always be an option.
Nevertheless, if a user wants to select a particular backend, or have
encryption or networking etc there will be some setup. Sqlite.org provides a
large number of controls in pragmas and the commandline already.</p>
</li>
<li>
<p>SQL processing front ends. Code exists (see <a href="./3.7-relevant-codebases.html">Relevant Codebases</a>
that implements MySQL-like behaviour in parallel with supporting SQLite semantics.
There is a choice codebases to do that with, covering different approaches to the problem.</p>
</li>
<li>
<p>Transaction interception and handling, which in the case of the LMDB
backend will be pass-through but in other backends may be for replicated
storage, or backup. This interception point would be in <code>wal.c</code> if all
backends used a writeahead log and used it in a similar way, but they do not.
Instead this is where the new <code>backend.c</code> API interception point will be
used - see further down in this document.  This is where, for example, we can
choose to add replication features to the standard SQLite btree storage
backend.</p>
</li>
<li>
<p>Storage backends, being a choice of native SQLite btree or LMDB today, and
swiftly after that other K-V stores. This is the choke point where we expect to
introduce <a href="./3.7-relevant-codebases#libkv">libkv</a>, or a modification of libkv.</p>
</li>
<li>
<p>Network layers, which will be at all of the above, depending whether they
are for client access to the parser, or replicating transactions, or being
plain remote storage etc.</p>
</li>
</ol>
<p>In most if not all cases it needs to be possible to have multiple choices
active at once, including the obvious cases of multiple parsers and multiple
storage backends, for example. This is because one of the important new use
cases for LumoSQL is conversion between formats, dialects and protocols.</p>
<p>Having designed the API architecture we can then produce a single LumoSQL tree
with these choke point APIs in place and proof of two things:</p>
<ol>
<li>
<p>ability to have stock-standard identical SQLite APIs and on-disk
btree format, and</p>
</li>
<li>
<p>an example of an alternative chunk of code at each choke point:
MySQL; T-pipe writing out the transaction log in a text file; LMDB .
Not necessarily with the full flexibility of having all code active at
once if that's too hard (ie able to take any input SQL and store in
any backend)</p>
<p>and then, having demonstrated we have a major step forward for the entire world,</p>
</li>
<li>
<p>Identify what chunks of SQLite we really don't want to support any more.
Like maybe the ramdisk pragma given that we can/should/might have an
in-memory storage backend, which initially might just be LMDB with overcommit
switched off. This is where testing and benchmarking really matters.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><!-- SPDX-License-Identifier: CC-BY-SA-4.0 -->
<!-- SPDX-FileCopyrightText: 2020 The LumoSQL Authors -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Dan Shearer, 2020 -->
<h2 id="sqlite-virtual-machine-layer"><a class="header" href="#sqlite-virtual-machine-layer">SQLite Virtual Machine Layer</a></h2>
<p>In order to support multiple backends, LumoSQL needs to have a more general way
of matching capabilities to what is available, whether a superset or a subset of
what SQLite currently does. This needs to be done in such a way that it remains
easy to track upstream SQLite.</p>
<p>The SQLite architecture has the SQL virtual machine in the middle of everything:</p>
<p><code>vdbeapi.c</code> has all the functions called by the parser
<code>vdbe.c</code> is the implementation of the virtual machine, and and it is
from here that calls are made into btree.c</p>
<p>All changes to SQLite storage code will be in vdbe.c , to insert an
API shim layer for arbitary backends. All BtreeXX function calls will
be replaced with backendXX calls.</p>
<p><code>lumo-backend.c</code> will contain:</p>
<ul>
<li>a switch between different backends</li>
<li>a virtual method table of function calls that can be stacked, for
layering some generic functionality on any backends that need it as
follows</li>
</ul>
<p><code>lumo-index-handler.c</code> is for backends that need help with index
and/or key handling. For example some cannot have arbitary length
keys, like LMDB. RocksDB and others do not suffer from this.
<code>lumo-transaction-handler.c</code> is for backends that do not have full
transaction support. RocksDB for example is not MVCC, and this will
add that layer. Similarly this is where we can implement functionality
to upgrade RO transactions to RW with a commit counter.
<code>lumo-crypto.c</code> provides encryption services transparently backends
depending on a decision made in lumo-backend.c, which will cover
everything except backend-specific metadata. Full disk encryption of
everything has to happen at a much lower layer, like SQLite's idea of
a VFS. The VFS concept will not translate entirely, because the very first
alternative backend is based on mmap, and which will need special handling. So we are for now expecting to implement a lumo-vfs-mmap.c and a lumo-vfs.c .
<code>lumo-vfs.c</code> provides VFS services to backends, and is invoked by
backends. <code>lumo-vfs.c</code> may call lumo-crypto for full file encryption
including backend metadata depending on the VFS being implemented.</p>
<p>Backend implementations will be in files such as <code>backend-lmdb.c</code>,
<code>backend-btree.c</code>, <code>backend-rocksdb.c</code> etc.</p>
<p>This new architecture means:</p>
<ol>
<li>Features such as WALs or paging or network paging etc are specific to the backend, and invisible to any other LumoSQL or SQLite code.</li>
<li>Bug-for-bug compatibility with the orginal SQLite btree.c can be maintained (except in the case of encryption, which no open source users have access to anyway.)</li>
<li>New backends with novel features (and LMDB is novel enough, for a first example!) can be introduced without disturbing other code, and being able to be benchmarked and tested safely.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><!-- SPDX-License-Identifier: CC-BY-SA-4.0 -->
<!-- SPDX-FileCopyrightText: 2020 The LumoSQL Authors -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Dan Shearer, 2020 -->
<h1 id="table-of-contents-12"><a class="header" href="#table-of-contents-12">Table of Contents</a></h1>
<ul>
<li><a href="WALs.html#database-storage-systems">Database Storage Systems</a>
<ul>
<li><a href="WALs.html#wals-in-sqlite">WALs in SQLite</a></li>
<li><a href="WALs.html#single-level-store">Single-level Store</a></li>
</ul>
</li>
</ul>
<h1 id="how-lumosql-architecture-differs-from-sqlite"><a class="header" href="#how-lumosql-architecture-differs-from-sqlite">How LumoSQL Architecture Differs from SQLite</a></h1>
<p><img src="./images/lumo-architecture-lumosql-theoretical-future.svg" alt="" title="Where LumoSQL architecture is headed" /></p>
<h1 id="database-storage-systems"><a class="header" href="#database-storage-systems">Database Storage Systems</a></h1>
<p>LumoSQL explores several features that are in advance of every other
widely-used database. With the first prototype complete with an LMDB backend,
LumoSQL is already the first major SQL database to offer an alternative to batch
processing, since it has a backend that does not use Write-Ahead Logs.  LumoSQL
also needs to be able to use both the original SQLite and additional storage
mechanisms, and any or all of these storage backends at once. Not all future
storage will be on local disk, or btree key-values.</p>
<p><a href="https://en.wikipedia.org/wiki/Write-ahead_logging">Write-ahead Logging in Transactional Databases</a> has been the only
way since the 1990s that atomicity and durability are provided in
databases. A version of same technique is used in filesystems, where is is
called <a href="https://en.wikipedia.org/wiki/Journaling_file_system">journalling</a>.
Write-ahead Logging (WAL) is a method of making sure that all modifications to
a file are first written to a separate log, and then they are merged (or
updated) into a master file in a later step. If this update operation is
aborted or interrupted, the log has enough information to undo the updates and
reset the database to the state before the update began. Implementations need
to solve the problem of WAL files growing without bound, which means some kind
of whole-database snapshot or checkpoint is required.</p>
<p>WALs seek to address issues with concurrent transactions, and reliability in
the face of crashes or errors. There are decades of theory around how to
implement WAL, and it is a significant part of any University course in
database internals. As well as somewhat-reliable commit and rollback, it is the
WAL that lets all the main databases in use offer online backup features, and
point-in-time recovery. Every WAL feature and benefit comes down to being able
to have a stream of atomic operations that can be replayed forwards or
backwards.</p>
<p>WAL is inherently batch-oriented. The more a WAL-based database tries to be to
real time, the more expensive it is to keep all WAL functionality working. </p>
<p>The WAL implementation in the most common networked databases is comprehensive
and usually kept as a rarely-seen technical feature. Postgresql is an exception, 
going out of its way to inform administrators how the WAL system works and what 
can be done with access to the log files.</p>
<p>All the most common networked databases describe their WAL implementation and
most offer some degree of control over it:</p>
<ul>
<li><a href="https://www.postgresql.org/docs/12/wal-intro.html">Postgresql</a></li>
<li><a href="https://docs.microsoft.com/en-us/sql/relational-databases/sql-server-transaction-log-architecture-and-management-guide?view=sql-server-ver15">SQL Server</a></li>
<li><a href="https://docs.oracle.com/en/database/oracle/oracle-database/19/cncpt/process-architecture.html#GUID-B6BE2C31-1543-4504-9763-6FFBBF99DC85">Oracle Log Writer Process</a></li>
<li><a href="https://dev.mysql.com/doc/refman/8.0/en/optimizing-innodb-logging.html">MySQL ReDo Log</a></li>
<li><a href="https://mariadb.com/kb/en/library/innodb-undo-log/">MariaDB Undo Log</a></li>
</ul>
<p>Companies have invested billions of Euros into these codebases, with stability
and reliability as their first goal. And yet even with all the runtime
advantages of huge resources and stable datacentre environments - even these
companies can't make WALs fully deliver on reliability. </p>
<p>These issues are well-described in the case of Postgresql. Postgresql has an
easier task than SQLite in the sense it is not intended for unpredictable
embedded use cases, and also that Postgresql has a large amount of code
dedicated to safe WAL handling.  Even so, Postgresql still requires its users
to make compromises regarding reliability. For example <a href="https://dzone.com/articles/postgresql-why-and-how-wal-bloats">this WAL mitigation
article</a>
describes a few of the tradeoffs of merge frequency vs reliability in the case
of a crash. This is a very real problem for every traditional database and that
includes SQLite - which does not have a fraction of the WAL-handling code of
the large databases, and which is frequently deployed in embedded use cases
where crashes and resets happen very frequently.</p>
<h2 id="wals-in-sqlite"><a class="header" href="#wals-in-sqlite">WALs in SQLite</a></h2>
<p>SQLite WALs are special.</p>
<p>The <a href="https://www.sqlite.org/draft/wal.html">SQLite WAL</a> requires multiple
files to be maintained in synch, otherwise there will be corruption. Unlike the
other databases listed here, SQLite has no pre-emptive corruption detection and
only fairly basic on-demand detection.</p>
<h2 id="single-level-store"><a class="header" href="#single-level-store">Single-level Store</a></h2>
<p>Single-level store concepts are well-explained in <a href="./lumo-relevant-knowledgebase.html#list-of-sqlite-code-related-knowledge">Howard Chu's 2013 MDB Paper</a>:</p>
<blockquote>
<p>One fundamental concept behind the MDB approach is known as &quot;Single-Level
Store&quot;. The basic idea is to treat all of computer memory as a single address
space. Pages of storage may reside in primary storage (RAM) or in secondary
storage (disk) but the actual location is unimportant to the application. If
a referenced page is currently in primary storage the application can use it
immediately, if not a page fault occurs and the operating system brings the
page into primary storage. The concept was introduced in 1964 in the Multics
operating system but was generally abandoned by the early 1990s as data
volumes surpassed the capacity of 32 bit address spaces. (We last knew of it
in the Apollo DOMAIN operating system, though many other Multics-influenced
designs carried it on.) With the ubiquity of 64 bit processors today this
concept can again be put to good use. (Given a virtual address space limit of
63 bits that puts the upper bound of database size at 8 exabytes. Commonly
available processors today only implement 48 bit address spaces, limiting us
to 47 bits or 128 terabytes.) Another operating system requirement for this
approach to be viable is a Unified BufferCache. While most POSIX-based
operating systems have supported an mmap() system call for many years, their
initial implementations kept memory managed by the VM subsystem separate from
memory managed by the filesystem cache. This was not only wasteful
(again, keeping data cached in two places at once) but also led to coherency
problems - data modified through a memory map was not visible using
filesystem read() calls, or data modified through a filesystem write() was not
visible in the memory map. Most modern operating systems now have filesystem
and VM paging unified, so this should not be a concern in most deployments.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="observations-on-savepoints-in-sqlite-internals"><a class="header" href="#observations-on-savepoints-in-sqlite-internals">Observations on Savepoints in SQLite Internals</a></h1>
<p>Dan Shearer
dan@shearer.org</p>
<p>Thanks to Uilbeheist.</p>
<h2 id="savepoint-statement-in-sqlite-vs-the-standard"><a class="header" href="#savepoint-statement-in-sqlite-vs-the-standard">SAVEPOINT statement in SQLite vs the standard</a></h2>
<p>The SAVEPOINT statement has been in the ISO SQL standard since 1992, and is in
most SQL implementations. SQLite users can take advantage of a unique SQLite
feature, where they can choose to avoid using all BEGIN/COMMIT|END statements
in favour of named savepoints. The exception is BEGIN IMMEDIATE (==EXCLUSIVE in
WAL mode), because savepoints do not an equivalent to IMMEDIATE.</p>
<p>sqlite3 extends both SQL standard transactions and savepoints to be a superset
of both. An SQLite BEGIN/COMMIT transaction is a special un-named case of a
savepoint, and a named saveppoint outside a BEGIN/COMMIT has an implied BEGIN.
A savepoint cannot ever be followed by a BEGIN because there can only be one
open main transaction at once, and a BEGIN always marks the start of a main
transaction.</p>
<p>This above context helps understand the 
<a href="https://sqlite.org/lang_savepoint.html">SQLite SAVEPOINT documentation</a> which
says:</p>
<blockquote>
<p>SAVEPOINTs are a method of creating transactions, similar to BEGIN and
COMMIT, except that the SAVEPOINT and RELEASE commands are named and may be
nested.</p>
</blockquote>
<p>Other implementations of SQL stick to the less flexible definition used in
the SQL standard, with <a href="https://mariadb.com/kb/en/savepoint/">MariaDB</a>,<br />
<a href="https://www.postgresql.org/docs/8.1/sql-savepoint.html">Postgresql</a>, 
<a href="https://docs.microsoft.com/en-us/sql/t-sql/language-elements/save-transaction-transact-sql?view=sql-server-ver15">Microsoft SQL Server</a>
and <a href="https://docs.oracle.com/cd/B19306_01/server.102/b14200/statements_10001.htm">Oracle Server</a>
seeming to be more or less identical. </p>
<p>MariaDB can seem as if it behaves like SQLite, but that is only due to it being
silent rather than throwing an error when a savepoint is used outside
BEGIN/COMMIT. From the MariaDB documentation: &quot;if SAVEPOINT is issued and no
transaction was started, no error is reported but no savepoint is created&quot;. In
fact MariaDB behaves like other implementations. </p>
<h2 id="savepoints-in-sqlite-code"><a class="header" href="#savepoints-in-sqlite-code">Savepoints in SQLite Code</a></h2>
<p>Internal terminology: Where savepoints are not used within a standard
transaction, source code comments call it a &quot;transaction savepoint&quot;. Similarly
an internal name for a standard BEGIN/COMMIT transaction is &quot;anonymous
savepoint&quot; while a &quot;non-transaction savepoint&quot; is the usual kind that follows a
BEGIN.</p>
<p>vdbe.c maintains the struct Savepoint declared in sqliteInt.h, while pager.c
maintains an array of struct PagerSavepoint. These parallel structures all come
down to the same objects on disk. </p>
<h3 id="vdbec"><a class="header" href="#vdbec">vdbe.c</a></h3>
<p>The opcode OP_Savepoint is the only relevant code in vdbe, which has some
savepoint logic and calls btree.c/sqlite3BtreeSavepoint(). vdbe deals with the
savepoints names and assigns each a sequence number. </p>
<h3 id="btreec"><a class="header" href="#btreec">btree.c</a></h3>
<p>btree.c implments sqlite3BtreeSavepoint() which uses
sqlite3PagerOpenSavepoint() to do the work. There is not much savepoint logic
in btree.c however it is btree.c that implements transactions and
subtransactions. (Subtransactions map onto subjournals but btree.c doesn't know
anything about them.)</p>
<h3 id="pagerc"><a class="header" href="#pagerc">pager.c</a></h3>
<p>Savepoint logic is mostly implemented in pager.c, by manipulating the objects
in the Pager.aSavepoint[] array . pager.c has the complete implementation of
sub-journals, which are maintained to match savepoint nesting. pager.c does not
know about savepoint names, only the sequence numbers vdbe.c assigned. It is
pager code that does the actual rollback to the correct savepoint, no other
code is involved in this.</p>
<p>Note: Savepoint code in pager.c seems to be quite intertwined with journal
states, but very little difference between using WALs or not. pagerUseWal() and
the aWalData[] array seem hardly used suggesting that savepoint implications
for WAL mode are little different from the others.</p>
<div style="break-before: page; page-break-before: always;"></div><!-- SPDX-License-Identifier: CC-BY-SA-4.0 -->
<!-- SPDX-FileCopyrightText: 2020 The LumoSQL Authors -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Dan Shearer, 2020 -->
<h1 id="table-of-contents-13"><a class="header" href="#table-of-contents-13">Table of Contents</a></h1>
<ul>
<li><a href="online-database-servers.html#lumosql-architecture">LumoSQL Architecture</a></li>
<li><a href="online-database-servers.html#online-database-servers">Online Database Servers</a></li>
<li><a href="online-database-servers.html#sqlite-as-an-embedded-database">SQLite as an Embedded Database</a></li>
</ul>
<h1 id="lumosql-architecture"><a class="header" href="#lumosql-architecture">LumoSQL Architecture</a></h1>
<p><img src="./images/lumo-architecture-intro.jpg" alt="" title="Shanghai Skyline from Pxfuel, CC0 license, https://www.pxfuel.com/en/free-photo-oyvbv" /></p>
<h1 id="online-database-servers"><a class="header" href="#online-database-servers">Online Database Servers</a></h1>
<p>All of the most-used databases other than SQLite work over a network, here
called &quot;online databases&quot;. This includes Postgresql, MariaDB, MySQL, SQLServer,
Oracle, and so on.</p>
<p><img src="./images/lumo-architecture-online-db-server.svg" alt="" title="What an online server database looks like" /></p>
<p>An online database server has clients that connect to the server over a
network. Once a network connection is opened, SQL queries are made by the
client and data is returned from the server. Although all databases use one of
the variants of the same SQL language, the means of connection is specific to each
database. </p>
<p>For example, on a typical Debian Linux server there are these well-known ports:</p>
<pre><code>foo@zanahoria:/etc$ grep sql /etc/services

ms-sql-s        1433/tcp                        # Microsoft SQL Server
ms-sql-m        1434/tcp                        # Microsoft SQL Monitor
mysql           3306/tcp                        # MySQL
postgresql      5432/tcp                        # PostgreSQL Database
mysql-proxy     6446/tcp                        # MySQL Proxy
</code></pre>
<p>with many other port assignments for other databases.</p>
<p>In the diagram above, each UserApp has a network connection to the SQL Database
Server on TCP port, for example 5432 if it is Postgresql. The UserApps could be
running from anywhere on the internet, including on mobile devices. There is a
limit to how many users one single database server can serve, in the many
thousands at least, but often reached for internet applications.</p>
<p><img src="./images/lumo-architecture-online-db-server-scale.svg" alt="" title="How an online database server scales" /></p>
<p>The most obvious way to scale an online database is to add more RAM, CPU and storage to a single server. This way all code runs in a single address space and is called &quot;Scaling Up&quot;. The alternative is to add more servers, and distribute queries between them. This is called &quot;Scale Out&quot;.</p>
<p>Nati Shalom describes the difference in the <a href="http://ht.ly/cAhPe">article Scale-Out vs Scale-Up</a>:</p>
<blockquote>
<p>One of the common ways to best utilize multi-core architecture in a context
of a single application is through concurrent programming. Concurrent
programming on multi-core machines (scale-up) is often done through
multi-threading and in-process message passing also known as the Actor
model.Distributed programming does something similar by distributing jobs
across machines over the network. There are different patterns associated
with this model such as Master/Worker, Tuple Spaces, BlackBoard, and
MapReduce. This type of pattern is often referred to as scale-out
(distributed).</p>
<p>Conceptually, the two models are almost identical as in both cases we break a
sequential piece of logic into smaller pieces that can be executed in
parallel. Practically, however, the two models are fairly different from an
implementation and performance perspective. The root of the difference is the
existence (or lack) of a shared address space. In a multi-threaded scenario
you can assume the existence of a shared address space, and therefore data
sharing and message passing can be done simply by passing a reference. In
distributed computing, the lack of a shared address space makes this type of
operation significantly more complex. Once you cross the boundaries of a
single process you need to deal with partial failure and consistency. Also,
the fact that you can’t simply pass an object by reference makes the process
of sharing, passing or updating data significantly more costly (compared with
in-process reference passing), as you have to deal with passing of copies of
the data which involves additional network and serialization and
de-serialization overhead.</p>
</blockquote>
<h1 id="sqlite-as-an-database-library"><a class="header" href="#sqlite-as-an-database-library">SQLite as an Database Library</a></h1>
<p>The user applications are tightly connected to the SQLite library. Whether by
dynamic linking to a copy of the library shared across the whole operating
system, or static linking so that it is part of the same program as the user
application, there is no networking involved. Making an SQL query and getting a
response involves a cascade of function calls from the app to the library to
the operating system and back again, typically taking less than 10 milliseconds
at most depending on the hardware used. An online database cannot expect to get
faster results than 100 milliseconds, often much more depending on network and
hardware. And online database relies on the execution of hundreds of millions
of more lines of code on at least two computers, whereas SQLite relies on the
execution of some hundreds of thousand on just one computer.</p>
<p><img src="./images/lumo-architecture-sqlite-overview.svg" alt="" title="Overview of a SQLite being an embedded database server" /></p>
<p><img src="./images/lumo-architecture-sqlite-parts.svg" alt="" title="The simplest view of the three parts to SQLite in typical embedded use" /></p>
<p><img src="./images/lumo-architecture-lumosql-theoretical-future.svg" alt="" title="Where LumoSQL architecture is headed" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="announcing-lumosql-database-phase-ii"><a class="header" href="#announcing-lumosql-database-phase-ii">Announcing LumoSQL Database Phase II</a></h1>
<p>| July 27, 2021 |
|Dan Shearer    |
|dan@lumosql.org|</p>
<p>LumoSQL is a derivative of SQLite, the most-used software in the world.
Our focus is on privacy, at-rest encryption, reproducibility and the
things needed to support these goals. The NLNet Foundation continues to
support LumoSQL, this time via the NGI Assure fund linked below.</p>
<p>In LumoSQL Phase II, the team is focussing on:</p>
<ul>
<li>
<p><a href="./rfc/README.html">Lumions as a universal data transport</a> as well as the
fundamental unit of private and secure data storage in LumoSQL.</p>
</li>
<li>
<p>Implementing a small <a href="./rbac-design.html">subset of the Postgresql 13 RBAC</a>
permissions model via
the SQL statements CREATE ROLE/GRANT/REVOKE etc. An important addition to
Postgres is to allow per-row permissions as well as per-table.</p>
</li>
<li>
<p>An extension of Phase I's hidden column mechanism, now to include hidden
tables. When using the native SQLite file format, hidden columns (similar to
ROWID) and tables are intended to be invisible to unmodified SQLite binaries.
These columns and tables implement row-based and table-based encryption and
more.</p>
</li>
<li>
<p>Further integration of the LMDB key-value store as an optional backend, from
version 0.9.11 onwards, and also the LMDBv1.0 pre-release. This work aims to
implement remaining SQLite and LMDB API features, and to prepare for the
page-level database encryption that is coming with LMDBv1.0.</p>
</li>
<li>
<p>Improved documentation, assembling the knowledge we gained in Phase I,
discussing the infrastructure as we now understand it, and covering the new
features for privacy and encryption. We do realise there is plenty of
work to do on this front :-)</p>
</li>
</ul>
<p>LumoSQL retains and builds on Phase I features including:</p>
<ul>
<li>
<p>The build, benchmark and test tools to measure unmodified SQLite and LumoSQL
in various configurations</p>
</li>
<li>
<p>The Not-Fork tool which allows multiple upstream codebases to be combined
across multiple versions without forking</p>
</li>
</ul>
<p>Further info from these URLs:</p>
<p><a href="https://lumosql.org/src/lumosql">LumoSQL Database</a></p>
<p><a href="https://lumosql.org/src/not-forking">Not-Forking reproducibility tool</a></p>
<p><a href="https://nlnet.nl/assure/">NLNet NGI Assure fund</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lumosql-database-v04-released"><a class="header" href="#lumosql-database-v04-released">LumoSQL Database v0.4 Released</a></h1>
<p><em>10th January 2021</em></p>
<p>With version 0.4, LumoSQL infrastructure has come of age:</p>
<ul>
<li>The benchmarking tools are generally useful for SQLite users, including vanilla SQLite</li>
<li>The build tool is now much more consistent and reliable, and written in Tcl</li>
<li>The example benchmarking report tool is more comprehensive (also written in Tcl)</li>
<li><a href="https://lumosql.org/src/not-forking">not-forking</a> is much improved, and ready for use in other projects</li>
</ul>
<p>The <a href="https://lumosql.org/src/lumosql/doc/tip/README.md">LumoSQL Project README</a> contains a much more 
useful introduction and quickstart. </p>
<p>The <a href="https://lumosql.org/src/lumosql/doc/tip/doc/lumo-build-benchmark.md">Build and Benchmarking</a> documentation is comprehensive.</p>
<p>The <a href="https://lumosql.org/src/lumosql/doc/tip/doc/lumo-benchmark-filter.md">benchmark filter documentation</a> is comprehensive.</p>
<p>As specific targets for the next LumoSQL version some 
<a href="https://lumosql.org/src/lumosql/doc/tip/README.md#limitations-of-lumosql">LumoSQL Limitations</a>
are listed in the README. Otherwise all the LumoSQL goals continue to roll
along, including varying forms of encryption, verification and validation.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lumosql-database-v03-released"><a class="header" href="#lumosql-database-v03-released">LumoSQL Database v0.3 Released</a></h1>
<p>10th November 2020</p>
<p>The <a href="https://lumosql.org/src/lumosql">LumoSQL</a> project provides answers to questions not many people 
are asking about SQLite. SQLite is easily both the most-used database <em>and</em> most-used K-V store
in the world, and so these questions are relevant:</p>
<ul>
<li>How much faster is SQLite improving, version on version, and with various switches selected?</li>
<li>What would SQLite look like if it used a more modern journaling model instead of WAL files?</li>
<li>Would it be nice for BDB, the ancient Berkeley Database, to magically appear as an SQLite storage backend?</li>
<li>Does SQLite work faster if it has <a href="http://www.lmdb.tech/doc/">LMDB</a> as a storage backend?</li>
<li>How on earth do you benchmark SQLite anyway, and compare with itself and others?</li>
</ul>
<p><a href="https://lumosql.org/src/lumosql/doc/tip/README.md">LumoSQL is SQLite gently modified</a> to have 
multiple different backends, with <a href="https://lumosql.org/benchmarking">benchmarking</a> and more. 
There is also the separate <a href="https://lumosql.org/src/lumodoc/">LumoDoc</a> 
project, which isn't just documentation for LumoSQL but also the results of our research and
testing, including a 
<a href="https://lumosql.org/src/lumodoc/doc/trunk/doc/lumo-relevant-knowledgebase.md">list of relevant codebases</a>. </p>
<p>The last talk at the <strong>SQLite and Tcl 
Conference</strong> <a href="https://sqlite.org/forum/forumpost/521ebc1239">later on today</a> will by Dan Shearer
speaking about LumoSQL. So we thought we should make the first public release... and here it is.</p>
<h2 id="history"><a class="header" href="#history">History</a></h2>
<p>Long-time SQLite watchers may recall some prototype code by Howard Chu in 2013
called &quot;sqlightning&quot;, which was btree.c modified to call LMDB internals. That code is
what inspired LumoSQL. LumoSQL 0.3 has had significant code contributions from 
first Keith Maxwell and then Uilebheist in assisting the project founder, Dan Shearer. 
One of our main achievements is to demonstrate how much more code is needed, so patches 
are welcomed. Do please drop in on the <a href="https://lumosql.org/src/lumosql/forum">LumoSQL forum</a> or otherwise
<a href="https://lumosql.org/src/lumosql/doc/tip/CONTRIBUTING.md">consider contributing</a>. Many 
others have helped significantly with LumoSQL 0.3, including <a href="https://nlnet.nl">NLNet</a> with sponsorship,
besides a long list of essential cheerleaders, encouragers and practical supporters.</p>
<h2 id="the-big-honking-end-goal"><a class="header" href="#the-big-honking-end-goal">The Big Honking End Goal</a></h2>
<p>The end goal is to develop a stable backend storage API in SQLite. This depends on 
many things, including being minimally invasive and maximally pleasing to drh :-)
Even if it cannot be committed to SQLite for some good reason, we will be able to 
carry it in LumoSQL.</p>
<p>But before we can even think of a storage API we need to understand what
different backends might be and what they need. Even key-value stores with
quite similar APIs such as SQLite native, LMDB and BDB have different
understandings of MVCC, key sizes, locking and more. The proposed API would
need to abstract all of this. We've been studying the interactions between
src/vdbe*, btree* and pager* as some may have noticed on the SQLite forum. 
There are not very many MVCC K-V stores suitable for linking to an embedded C
library, but we want to collect all those that are.</p>
<h2 id="nope-not-a-fork"><a class="header" href="#nope-not-a-fork">Nope, Not a Fork</a></h2>
<p>LumoSQL has avoided forking SQLite by developing the
<a href="https://lumosql.org/src/not-forking">not-forking</a> tool. This tool could 
be helpful for anyone trying to stay in synch with multiple upstreams. It
knows how to fetch sources, parse version numbers and make non-controversial merges
even in cases where a straight patch or fossil merge would fail. It can also replace
entire files, and more.</p>
<h1 id="lumosql-features-as-of-version-03"><a class="header" href="#lumosql-features-as-of-version-03">LumoSQL Features as of version 0.3</a></h1>
<pre><code>$ make what
SQLITE_VERSIONS=3.34.0           # whatever the latest version is
USE_LMDB=yes
SQLITE_FOR_LMDB=3.8.3.1
LMDB_VERSIONS=0.9.9 0.9.16 0.9.27
USE_BDB=yes
SQLITE_FOR_BDB=3.18.2
BDB_VERSIONS=
BDB_STANDALONE=18.1.32
TARGETS=
    3.34.0
    3.8.3.1
    3.18.2
    3.8.3.1+lmdb-0.9.9
    3.8.3.1+lmdb-0.9.16
    3.8.3.1+lmdb-0.9.27
    +bdb-18.1.32                 # the +means it is not yet a clean LumoSQL not-fork config
</code></pre>
<p>This <a href="https://lumosql.org/src/lumosql/doc/tip/doc/lumo-test-build.md">builds</a> the versions listed.
With <a href="https://lumosql.org/src/not-forking">Not-forking</a> we can
walk up and down the version tree. </p>
<pre><code>make benchmark
</code></pre>
<p>Will perform some fairly simple operations on all targets, storing results in a
single SQLite 3.33.0 database.  This database is intended to persist, and to be
amalgamated with results from others. While some basic query and fsck-like
tools are provided, LumoSQL hopes that people with skills in statistics and
data presentation will work their magic with this interesting new dataset. The
design is meant to encourage addition of new parameters and dimensions to the
benchmarking.</p>
<pre><code>make benchmark TARGETS=3.7.17+lmdb-0.9.26+datasize-10
</code></pre>
<p>Runs the same benchmarks, except that all of the operations have a zero added to them, so
25000 SELECTs becomes 250000. Other size factors can be chosen.</p>
<h1 id="results-so-far"><a class="header" href="#results-so-far">Results So Far</a></h1>
<ul>
<li>The not-forking approach works. Yes we are going to be able to &quot;try before we buy&quot; a storage API</li>
<li>Benchmarking works. It's got a long way to go, but even this much is a powerful new way of
comparing SQLite versions</li>
<li>SQLite has improved considerably in performance in the last five years</li>
<li>SQLite+LMDB performs better than SQLite as dataset sizes increase (testing is incomplete though)</li>
<li>SQLite+MDB don't perform at all well... in fact, worse than a very much older vanilla SQLite.
(Some would hesitate to use SQLite+MDB in production anyway given that MDB
is under the AGPL licence, and SQLite is a library and thus anything using it would also be
covered by the AGPL.)</li>
</ul>
<p>There are some less-technical results too, like the fact that there are
many developers around the world who have modified SQLite in interesting ways
but there has been no effective way for their work to be compared or evaluated.
Oh and, we're using Fossil, definitively so.</p>
<h1 id="where-next-for-lumosql"><a class="header" href="#where-next-for-lumosql">Where Next for LumoSQL?</a></h1>
<ul>
<li>Walk up the SQLite version tree to tip. We're all waiting to see what SQLite 3.33.0+LMDBv1.0rc1 will be like.</li>
<li>Complete our work moving LMDB to only use liblmdb as a fully-external library</li>
<li>Do some concurrency benchmarking, possibly addressing a potential concurrency problem with LMDB 
in the process. Concurrency is not SQLite's strong point, so this will be very interesting </li>
<li>Possibly increase the SQLite version supporting BDB. This is a very important use case because
the BDB API is both classic and yet also not LMDB, meaning if we get it right then we get it
right for other K-V stores</li>
<li>Produce lots more benchmarking data with our existing tools. That means lots of CPU time, and we'd
love to have volunteers to help populate this</li>
<li>First draft backend API taking into account issues relating to keys, transactions, internal
SQLite close couplings, etc.</li>
<li>Talk to the active SQLite forks ... if you're reading this, we'd love to hear from you :-)</li>
</ul>
<p>And that is LumoSQL release 0.3. We look forward to seeing those who can make it to the SQLite and Tcl 
Conference later on today at https://sqlite.org/forum/forumpost/521ebc1239 , and to producing more
releases in quick succession.</p>
<div style="break-before: page; page-break-before: always;"></div><!-- Copyright 2021 The LumoSQL Authors, see LICENSES/MIT -->
<!-- SPDX-License-Identifier: MIT -->
<!-- SPDX-FileCopyrightText: 2021 The LumoSQL Authors -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Dan Shearer, February 2021 -->
<h1 id="google-summer-of-code-ideas"><a class="header" href="#google-summer-of-code-ideas">Google Summer of Code Ideas</a></h1>
<p>This list was made for the LumoSQL project's application for 
<a href="https://summerofcode.withgoogle.com/">Google Summer of Code</a> in 2021. GSoC pays students to
contribute to free software projects during the Northern Hemiphere summer.  If
you are a student, you will be able to apply for GSoC starting March 29th 2021.</p>
<h1 id="benchmarking-tasks"><a class="header" href="#benchmarking-tasks">Benchmarking tasks</a></h1>
<p>We have design outlines and background documentation for the following.</p>
<ul>
<li>LumoSQL benchmarking creates an SQLite database. Write a tool to accept these databases via the web and consolitate them, using the facilities existing in <a href="../tool/benchmark-filter.tcl">benchmark-filter.tcl</a></li>
<li>Write a tool to drive <a href="./lumo-build-benchmark.html">benchmarking</a> runs for different <a href="https://sqlite.org/pragma.html#toc">pragmas</a> to start to address the problem that <a href="https://lumosql.org/src/lumodoc/doc/trunk/doc/lumo-benchmarking.md#all-sqlite-performance-papers-are-nonsense">all SQLite performance papers are nonsense</a>.</li>
<li>Write a tool that analyses a consolidated benchmarking database and displays summary results over the web</li>
</ul>
<h1 id="lumo-column-tasks"><a class="header" href="#lumo-column-tasks">Lumo Column tasks</a></h1>
<ul>
<li>Consider the LumoSQL &quot;Lumo Column&quot; implementation (full documentation not yet available). Look at the code for implementing per-row checksums in an implicit column that exists the same way as <a href="https://www.sqlite.org/lang_createtable.html#rowid">SQLite ROWID</a> exists. All four sha3 hash formats are supported, besides &quot;none&quot; and &quot;null&quot;. Add two new hash formats by extending the table for BLAKE2 and BLAKE3.</li>
<li>This is an advanced task: working with LumoSQL developers, design SQL row-level equivalents to Unix mtime, ctime and atime using Lumo Columns. A Lumo Column is a blob with a header; you will need to change the header used for rowsums. All of the work required for this will be within the file vdbe.c</li>
<li>Implement row-level mtime, ctime and atime using Lumo Columns</li>
</ul>
<h1 id="backend-storage-tasks"><a class="header" href="#backend-storage-tasks">Backend storage tasks</a></h1>
<ul>
<li>Document the LumoSQL-specific user interface changes needed for backends. This involves looking at the features of BDB and LMDB including the existing BDB pragmas, and designing something more generic. This needs to work across other backend stores too, so comparing BDB (which probably isn't going to be the amazing storage engine of the future!) and LMDB and native SQLite is likely to give reasonable results</li>
<li>Design a way of moving the BDB sources into a patch system like LumoSQL not-forking directory does for LMDB, rather than whole-file replacements</li>
<li>Implement a not-forking directory following the design developed above. This should mean that the BDB backend works with more recent versions of SQLite, and where it does not, that the changes will be more obvious and easier to make</li>
<li>Considering existing Not-Forking work done already, and the section &quot;List of MVCC-capable KV store-related Knowledge&quot; in the <a href="https://lumosql.org/src/lumodoc/doc/trunk/doc/lumo-relevant-knowledgebase.md">LumoSQL Knowledgebase</a>, prototype a backend for the Malbrain's btree. What might the advantages and disadvantges be of this storage engine? This is leading-edge experimentation, because there are very few new Btree storage engines, and exactly none of them under SQLite</li>
</ul>
<h1 id="tooling-tasks"><a class="header" href="#tooling-tasks">Tooling tasks</a></h1>
<ul>
<li>Add a feature to <a href="https://lumosql.org/src/not-forking">Not-Forking</a> so that it somewhat aware of licenses. For example, at the moment the LumoSQL build process gives the option of building with LMDB (<a href="https://en.wikipedia.org/wiki/MIT_License">MIT licensed</a>), native SQLite (<a href="https://sqlite.org/copyright.html">kind-of licensed under sort-of public domain</a>) and Berkeley DB (<a href="https://www.gnu.org/licenses/agpl-3.0.en.html">AGPL</a>). The LumoSQL code has a <a href="https://lumosql.org/src/lumosql/dir?ci=tip&amp;name=LICENCES">LICENCES/</a> directory pointed at by <a href="https://spdx.dev">SPDX</a> headers on all LumoSQL files. But these other codebases do not, and the interactions are different. The Berkeley DB licence subsumes all others and any resulting binary is under the AGPL. The LumoSQL/LMDB MIT subsumes the SQlite license and any resulting binary is under the MIT. At least one additional proposed backend is under the 2-clause BSD license, which is compatible with MIT and SQLite and AGPL, etc. Not-Forking needs a mechanism of signalling that it is grafting on code with a particular license - it does not need to do legal calculations about which license wins, but it does need to make it obvious which licenses apply. There are various possible designs, and the LumoSQL team would be glad to work with you on this.</li>
<li>Fix a documentation problem by writing a <a href="https://pandoc.org">Pandoc</a> filter that understands <a href="https://fossil-scm.org/home/md_rules">Fossil markdown</a>. This was considered but abandoned as <a href="https://fossil-scm.org/home/timeline?r=auto-toc">out of scope for Fossil</a>. Pandoc processing of Markdown -&gt; Markdown would give other advantages than TOCs.</li>
<li>Fix a documentation problem by writing a Pandoc filter that understands <a href="https://pikchr.org">Pikchr</a>, which would apply to all Pandoc inputs including Markdown documentation such as used by LumoSQL</li>
<li>Develop a basic buildfarm so that Fossil is regularly built and tested with (a) multiple architectures - one big endian and (b) multiple build-time options. This would use the <a href="https://www.fossil-scm.org/home/help?cmd=hook">Fossil hook</a> command</li>
</ul>
<h1 id="packaging-tasks"><a class="header" href="#packaging-tasks">Packaging tasks</a></h1>
<ul>
<li>Develop a Debian package for <a href="https://lumosql.org/src/not-forking">Not-Forking</a> for inclusion in the Not-Forking source tree. There is already an ebuild</li>
<li>Develop a Debian package for LumoSQL for distribution in the LumoSQL source tree</li>
<li>Develop an ebuild for LumoSQL for distribution in the LumoSQL source tree</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="temporary-note-of-debug-proposal"><a class="header" href="#temporary-note-of-debug-proposal">Temporary Note of Debug Proposal</a></h1>
<p>As discussed at https://sqlite.org/forum/forumpost/42e647999d , which also has demo code. 
When this is in SQLite, this file can be deleted. </p>
<h1 id="overview"><a class="header" href="#overview">Overview</a></h1>
<p>This posting and the C code following in the same thread represent an approach I am volunteering to implement in SQLite, if it is agreed to be a good approach. </p>
<p>Adding and using debugging code in SQLite <a href="1772cb4a37">has some strange problems</a> which can be easily addressed with the changes proposed in that post, which I am also volunteering to make. But there is a much bigger scope to the problem, which I think can also be safely addressed as follows.</p>
<h1 id="goal-of-this-proposal"><a class="header" href="#goal-of-this-proposal">Goal of This Proposal</a></h1>
<p>To add a minimal superset of compile-time debug options to SQLite in sqlInt.h,
with the intention of:</p>
<ul>
<li>
<p>not disturbing any existing debug code anywhere</p>
</li>
<li>
<p>allowing lightweight debugging such as printing only progress
statements, without the execution-time overhead of the current
DEBUG_SQLITE</p>
</li>
<li>
<p>allowing only certain classes of debug options to be compiled, which
addresses the previous point but also allows for selecting just some of the
heavyweight debugging options </p>
</li>
<li>
<p>allowing existing debug code to be safely and simply upgraded to the new
discriminated debugging system, with no unexpected changes in functionality to
users of existing debug facilities</p>
</li>
</ul>
<p>Note: Debugging is not logging. This proposal only discusses debugging, which
is enabled by developers at compiletime. SQLite has a logging system which is
available at runtime and controlled by SQLite users and applications.</p>
<h1 id="background"><a class="header" href="#background">Background</a></h1>
<p><a href="https://www.sqlite.org/compile.html#debugoptions">The compilation documentation</a> says:</p>
<blockquote>
<p>The SQLite source code contains literally thousands of assert() statements
used to verify internal assumptions and subroutine preconditions and
postconditions. These assert() statements are normally turned off (they
generate no code) since turning them on makes SQLite run approximately three
times slower. But for testing and analysis, it is useful to turn the assert()
statements on.</p>
</blockquote>
<p>Adding thousands of assert() statements and even more thousands of lines
of non-assert debug code is not desirable when testing just one particular aspect of SQLite, or wanting to print just one debugging line in just one function. Sometimes this debug code can interfere with SQLite behaviour in addition to just making it run slowly. While it is important to be able to do global debugging, more often a developer is only working on one thing at a time. There is no need to have an entire test suite run much slower for the sake of
observing a single print statement. On resource-constrained targets or where
SQLite is part of a much larger and more complicated codebase the addition of
debugging code can have unpredictable effects.</p>
<p>As an example of debugging code that doesn't make sense to be always-on even
when debugging:</p>
<blockquote>
<p>When compiled with SQLITE_DEBUG, SQLite includes routines that will
print out various internal parse tree structures as ASCII-art graphs.
This can be very useful in a debugging in order to understand the
variables that SQLite is working with.</p>
</blockquote>
<p>If we are not interested in these ASCII-art graphs, that's unhelpful extra code
hanging around. On the other hand, if it is a selectable debug option, it might be reasonable to enhance that feature by adding even more code, perhaps by emitting <a href="https://pikchr.org/home/doc/trunk/homepage.md">Pikchr markup</a>.</p>
<p>The goal of this proposal has already been identified as a SQLite need, as
can be seen in the <a href="https://www.sqlite.org/debugging.html">SQLIte debugging documentation</a> where of the four existing debug macros, two discriminate based on debugging function:</p>
<blockquote>
<p>The SQLITE_ENABLE_SELECTTRACE and SQLITE_ENABLE_WHERETRACE options
are not documented in compile-time options document because they are
not officially supported.</p>
</blockquote>
<h1 id="opportunity"><a class="header" href="#opportunity">Opportunity</a></h1>
<p>A common question is whether a project should implement debug levels or debug
classes. This proposal addresses both at once.</p>
<p>Given that there is otherwise no debugging discrimination, we have the
opportunity to assign comprehensive debug levels or classes and
gradually implement them consistently, and leave room for additional
classes and levels to be added in the future. Done well, this will
improve the speed and quality of debugging cycles, and also make it
easier to assist SQLite developers by asking domain-specific debugging
questions. It will encourage better quality thinking about the
debugging process, and be more compatible with the idea of SQLite as a
small, efficient embedded library.</p>
<h1 id="potential-problems"><a class="header" href="#potential-problems">Potential Problems</a></h1>
<ul>
<li>
<p>shipping debug: A better debug system might tempt some developers to ship
some degree of debugging enabled by default in production. This would break
the idea of debugging as a developer safespace, and potentially expose end
users to unexpected behaviour. New SQLite debugging features need to be
strongly documented as &quot;unsupported for production use in all contexts.&quot;</p>
</li>
<li>
<p>Booleans vs. bitmaps: This proposal uses boolean macros rather than bitmaps,
except for DEBUG_LEVEL which is a decimal integer.
Bitmaps would look like:</p>
<blockquote><tt>
      #define DEBUG_COMMANDLINE 0x00000004<br>
      #define DEBUG_PRAGMA      0x00000008<br>
</tt></blockquote>
etc.
Bitmaps have the classical advantage of being able to be specify multiple
debugging classes/levels in a single number provided at compile-time, however
that can only guarantee 31 separate numbers as any more may break on 32-bit
processors due to the sign bit. Furthermore there are four kinds of endianness
and again this might break debugging on some architectures.  
</li>
<li>
<p>Bitmaps vs. booleans: Using boolean macros means that, say 4 debug classes plus the mandatory SQLITE_SELECTIVE_DEBUG and likely DEBUG_LEVEL, and possible SQLITE_DEBUG makes for an extremely long $CC invocation line. But this is much
less likely to obscurely break the debug system than architecture/bitmap 
clashes. Even though we need lots more -D / #define statements.</p>
</li>
</ul>
<h1 id="how-to-use-the-following-code"><a class="header" href="#how-to-use-the-following-code">How to Use the Following Code</a></h1>
<pre><code>compile sqlitedebug with $CC parameters as follows, then run it.
 
    -D SQLITE_DEBUG
    -D SQLITE_SELECTIVE_DEBUG -DDEBUG_LEVEL=1
    -D SQLITE_SELECTIVE_DEBUG -DDEBUG_ALL
    -D SQLITE_SELECTIVE_DEBUG -DDEBUG_VIRTUAL_MACHINE -DDEBUG_STORAGE
    -D SQLITE_SELECTIVE_DEBUG -DDEBUG_LEVEL=2 -DDEBUG_VIRTUAL_MACHINE

some combinations will halt compilation with an error, eg

    -D DEBUG_LEVEL=1                     
                        (no SQLITE_SELECTIVE_DEBUG)
    -D SQLITE_SELECTIVE_DEBUG -D DEBUG_LEVEL=4
                        (debug level out of range)
    -D DEBUG_ALL
                        (no SQLITE_SELECTIVE_DEBUG)

</code></pre>
<p>Implementation of the debug helper functions include function name and line
number.</p>
<h1 id="todo"><a class="header" href="#todo">Todo</a></h1>
<p>Agree on a larger initial/better category list</p>
<div style="break-before: page; page-break-before: always;"></div><!-- Copyright 2022 The LumoSQL Authors, see LICENSES/MIT -->
<!-- SPDX-License-Identifier: MIT -->
<!-- SPDX-FileCopyrightText: 2022 The LumoSQL Authors -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Dan Shearer, December 2019 -->
<h1 id="contributing-to-the-lumosql-project"><a class="header" href="#contributing-to-the-lumosql-project">Contributing to the LumoSQL Project</a></h1>
<p>Welcome! We'd love to have your contributions to <a href="https://lumosql.org/src/lumosql">the LumoSQL
project’s files</a>. You don't need to be a coder to contribute.
We need help from documenters, cryptographers, mathematicians,
software packagers and graphic designers as well as coders who are
familiar with C, Tcl and other languages. If you know about database
theory and SQL then we definitely want to hear from you :-)</p>
<p>Please do:</p>
<ul>
<li>Drop in to #lumosql on <a href="https://libera.chat/">Libera irc chat</a> , or email <a href="mailto://authors@lumosql.org">authors@lumosql.org</a>.</li>
<li>Tell us about any problems you have following the <a href="doc/lumo-build-benchmark.html">LumoSQL Build, Test and Benchmarking</a> documentation.</li>
<li>Help us improve the <a href="doc/rfc/README.html">Lumion design and RFC</a>.</li>
</ul>
<p>Things to note:</p>
<ul>
<li>The LumoSQL GitHub mirror is one-way. Development happens with <a href="https://fossil-scm.org/">Fossil</a>, which has good documentation and a very <a href="https://fossil-scm.org/forum/">helpful forum</a>.</li>
</ul>
<h1 id="accessibility"><a class="header" href="#accessibility">Accessibility</a></h1>
<p>Accessibility means that we make it as easy as possible for
developers to fully participate in LumoSQL development. This starts
with the tools we choose. We use and recommend:</p>
<ul>
<li><a href="https://fossil-scm.org/">Fossil</a>, whose features are available via the commandline as well
as Javascript-free web pages. (The one exception is the Fossil chat
facility implemented entirely in Javascript, which while interesting
and useful is not something LumoSQL uses.)</li>
<li><a href="https://libera.chat/">irc chat</a>, for which there are many accessible clients</li>
<li><a href="http://paste.c-net.org/">paste-c</a> , which is an accessible and
efficient Pastebin</li>
<li><a href="https://www.r-project.org/">R</a>, a data analysis toolkit</li>
<li>Unix-like operating systems and commandline toolchains, to nobody's
surprise :-)</li>
</ul>
<p>We know these tools do work for many people, including all those
involved with LumoSQL at present.`</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="automated-meeting-notes-with-lumosql-meetbot"><a class="header" href="#automated-meeting-notes-with-lumosql-meetbot">Automated Meeting Notes With lumosql-meetbot</a></h1>
<p>We use <a href="https://hcoop-meetbot.readthedocs.io">HcoopMeetbot</a> to make irc meetings simpler and easier.</p>
<p>Normal irc chat is ignored by the bot, but chat that tagged as part of meeting
goes in the notes. Any user on the #lumosql channel can participate in a
meeting, or call one.</p>
<p>Meetings notes automatically appear in the <a href="https://lumosql.org/meetings">Meetbot log directory</a> as soon as 
the meeting is finished.</p>
<p>This Meetbot helps us remember the important information immediately, and the action items.</p>
<h1 id="how-to-use-the-meetbot"><a class="header" href="#how-to-use-the-meetbot">How to Use the Meetbot</a></h1>
<p>In the #lumosql chat room on the [libera chat network](https://libera.chat Libera), you should see a logged-in user
called &quot;lumosql-meetbot&quot;. This is a bot, and its purpose is to hang around waiting until someone
says &quot;#startmeeting&quot; in the chat. From then on, it listens for more instructions preceded with &quot;#&quot;.</p>
<p>You can read all the details in the help page above. These are the commands we need for LumoSQL meetings:</p>
<ul>
<li><strong>#startmeeting</strong>            Anyone can start a meeting, and is then the chair until <strong>#endmeeting</strong> is issued.</li>
<li><strong>#meetingname &lt;descriptive name&gt;</strong>             Important! The chair should specify this, because it gets extracted as the comment in the <a href="https://lumosql.org/meetings">table of meetings</a>.</li>
<li><strong>#here</strong>                    List yourself as an attendee. Everyone should do this at the beginning because it looks neater in the notes.</li>
<li><strong>#topic</strong>                   Start a new topic, that is a section heading in the meeting notes.
<ul>
<li><strong>#info &lt;text&gt;</strong>           Add a bullet item under the current topic heading.</li>
<li><strong>#link &lt;link&gt; &lt;text&gt;</strong>    The supplied URL gets added under the current topic heading as a clickable HREF link.</li>
<li><strong>#action &lt;nick&gt; &lt;text&gt;</strong>  Assign an action item to user &lt;nick&gt; eg <em>#action bk712 to make the coffee</em>. Always use the irc nickame instead of a human-like name such as &quot;Björn&quot;, because otherwise the Meetbot won't assign the actions properly at the end.</li>
<li><strong>#accepted &lt;text&gt;</strong>       Log an agreement we have made, eg <em>#accepted we are all going home now</em>.</li>
<li><strong>#motion &lt;text&gt;</strong>         The chair can propose any motion for voting, eg <em>#motion Vanilla icecream is best</em>.</li>
<li><strong>#vote +1 / -1</strong>          Anyone can vote +1 or -1. The meetbot will allow people to vote multiple times, to correct a mistaken vote.</li>
<li>**#help &lt;text&gt;           Add a request for people to volunteer for a task in the notes. If you are looking for Meetbot help, then see <strong>#commands</strong> below.</li>
<li><strong>#close</strong>                 The chair closes the vote, and the meetbot summarises the results.</li>
</ul>
</li>
<li><strong>#endmeeting</strong>              End the meeting. The formatted minutes and raw log magically appear in a few seconds.</li>
<li><strong>#commands</strong>          get a list of all the valid commands, and be reminded of the URL of the help page.</li>
</ul>
<p>It's a great tool, thanks to <a href="https://github.com/pronovic">Kenneth J. Pronovici</a> and others.</p>
<p>You can address the bot directly and chat with it, including by the shortcut &quot;@ &lt;text&gt;&quot;. You'll find out about that in the online help.</p>
<blockquote>
<p><font size="6"> ☝🏾 </font> The meeting logs are just HTML files, so if something <em>really</em> incorrect gets into the notes by accident we can edit them manually. But this should be very rare.</p>
</blockquote>
<blockquote>
<p>Obviously, chat in #lumosql is covered by the <a href="../CODE-OF-CONDUCT.html">LumoSQL Code of Conduct</a>, which says &quot;be a decent person&quot;.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><!-- Copyright 2022 The LumoSQL Authors, see LICENSES/MIT -->
<!-- SPDX-License-Identifier: CC-BY-SA-4.0 -->
<!-- SPDX-FileCopyrightText: 2022 The LumoSQL Authors -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Dan Shearer, December 2019 -->
<h1 id="lumosql-code-of-conduct"><a class="header" href="#lumosql-code-of-conduct">LumoSQL Code of Conduct</a></h1>
<p>Version 1.5 – Updated 18th February, 2022</p>
<p><strong>This file exists because the LumoSQL Project needed it, less than one year
after starting in 2019. We take it seriously. We take the view that this file is
about mandating kindness. We are in a global pandemic when kindness is needed more
than ever.</strong></p>
<p>The LumoSQL Project welcomes contributions from anyone who shares
<a href="https://lumosql.org/src/lumodoc/doc/trunk/doc/lumo-project-aims.md">the LumoSQL Project Goals</a>
This document outlines both expected and prohibited behaviour.</p>
<h1 id="short-summary"><a class="header" href="#short-summary">Short Summary</a></h1>
<p>The rest of this document is detail to support these three points:</p>
<ul>
<li>LumoSQL participants are to be respectful and direct with each other.</li>
<li>We will not tolerate bullying, racism, sexism or constant domineering behaviour.</li>
<li>No personal attacks, and generally stay focussed on what we are trying to achieve. </li>
</ul>
<h1 id="who-should-feel-safe"><a class="header" href="#who-should-feel-safe">Who Should Feel Safe?</a></h1>
<p>Everyone, regardless of diversity dimensions including:</p>
<ul>
<li>Gender, identity or expression</li>
<li>Age</li>
<li>Socioeconomic status</li>
<li>Sex or sexual orientation</li>
<li>Family status</li>
<li>Race and/or caste and/or ethnicity</li>
<li>National origin</li>
<li>Religion</li>
<li>Native or other languages</li>
</ul>
<h1 id="when-should-they-feel-safe"><a class="header" href="#when-should-they-feel-safe">When Should They Feel Safe?</a></h1>
<ul>
<li>Working with other LumoSQL community participants virtually or co-located</li>
<li>Representing LumoSQL at public events</li>
<li>Representing LumoSQL in social media</li>
</ul>
<h1 id="what-is-expected"><a class="header" href="#what-is-expected">What is Expected?</a></h1>
<p>The following behaviours are expected of all LumoSQL community participants:</p>
<h2 id="be-respectful"><a class="header" href="#be-respectful">Be Respectful</a></h2>
<p>Value each other’s ideas, styles and viewpoints. Disagreement is no excuse for
bad manners. Be open to different possibilities and to being wrong. Take
responsibility, so if someone says they have been harmed through your words or
actions, listen carefully, apologise sincerely, and correct the behaviour.</p>
<h2 id="be-direct-but-professional"><a class="header" href="#be-direct-but-professional">Be Direct but Professional</a></h2>
<p>We must be able to speak directly when we disagree and when we think we need to
improve. We cannot withhold hard truths.  Doing so respectfully is hard, doing
so when others don’t seem to be listening is harder, and hearing such comments
when can be harder still.</p>
<h2 id="be-inclusive"><a class="header" href="#be-inclusive">Be Inclusive</a></h2>
<p>Seek diverse perspectives. Diversity of views and of people gives better
results.  Encourage all voices. Help new perspectives be heard and listen
actively. If you find yourself dominating a discussion, step back and give
other people a chance.  Observe how much time is taken up by dominant members
of the group.</p>
<h2 id="appreciate-and-accommodate-our-similarities-and-differences"><a class="header" href="#appreciate-and-accommodate-our-similarities-and-differences">Appreciate and Accommodate Our Similarities and Differences</a></h2>
<p>Be respectful of people with different cultural practices, attitudes and
beliefs. Work to eliminate your own biases, prejudices and discriminatory
practices. Think of others’ needs from their point of view. Use preferred
titles (including pronouns). Respect people’s right to privacy and
confidentiality. Be open to learning from and educating others as well as
educating yourself.</p>
<h1 id="behaviour-that-will-not-be-tolerated"><a class="header" href="#behaviour-that-will-not-be-tolerated">Behaviour That Will Not Be Tolerated</a></h1>
<p>The following behaviours are unacceptable, as should be obvious in any case:</p>
<h2 id="violence-and-threats-of-violence-are-not-acceptable"><a class="header" href="#violence-and-threats-of-violence-are-not-acceptable">Violence and Threats of Violence Are Not Acceptable</a></h2>
<p>Offline or online, including incitement of violence or encouraging a person to
commit self-harm. This also includes posting or threatening to post other
people’s personal data (“doxxing”) online.</p>
<h2 id="derogatory-language-is-not-acceptable"><a class="header" href="#derogatory-language-is-not-acceptable">Derogatory Language Is Not Acceptable</a></h2>
<p>Hurtful or harmful language related to any dimension of diversity is not
acceptable.</p>
<p>This includes deliberately referring to someone by a gender that they do not
identify with, or questioning an individual’s gender identity. If you’re unsure
if a word is derogatory, don’t use it.  When asked to stop, stop the behaviour.</p>
<h2 id="unwelcome-sexual-attention-or-physical-contact-is-not-acceptable"><a class="header" href="#unwelcome-sexual-attention-or-physical-contact-is-not-acceptable">Unwelcome Sexual Attention or Physical Contact Is Not Acceptable</a></h2>
<p>This section is here because it has been proven to be needed where LumoSQL has
been present. It is not some formality.  If you don't think it is needed, you have
not been paying attention.</p>
<p>Unwelcome sexual attention online or offline, or unwelcome physical contact is
not acceptable. This includes sexualised comments, jokes or imagery as well as
inappropriate touching, groping, or sexual advances.  This also includes
physically blocking or intimidating another person. Physical contact or
simulated physical contact (potentially including emojis) without affirmative
consent is not acceptable.</p>
<h1 id="consequences-of-unacceptable-behaviour"><a class="header" href="#consequences-of-unacceptable-behaviour">Consequences of Unacceptable Behaviour</a></h1>
<p>Bad behaviour from any LumoSQL community participant can't be tolerated.
Intentional efforts to exclude people (except as part of a consequence of these
guidelines) from LumoSQL activities are not acceptable.</p>
<p>Reports of harassment/discrimination will be promptly and thoroughly
investigated by the people responsible for the safety of the space, event or
activity, with a view to taking action.</p>
<p>Anyone asked to stop unacceptable behaviour is expected to stop immediately.
Violation of these guidelines can result in you being ask to leave an event or
online space, either temporarily or for the duration of the event, or being
banned from participation in spaces, or future events and activities.</p>
<p>Participants who abuse the reporting process will be considered to be in
violation. False reporting, especially to retaliate or exclude, will not be
accepted or tolerated.</p>
<h1 id="reporting"><a class="header" href="#reporting">Reporting</a></h1>
<p>If you believe you’re experiencing unacceptable behaviour 
as outlined above please contact one of the 
<a href="https://lumosql.org/src/lumosql/file?name=AUTHORS">current authors</a>.</p>
<p>After receiving a concise description of your situation, they will review and
determine next steps. </p>
<p>Please also report to us if you observe someone else in distress or violations of
these guidelines.</p>
<p>If you feel you have been unfairly accused of violating these guidelines,
please follow the same reporting process.</p>
<hr>
<p>This document is (c) 2022 The LumoSQL Authors, under the <a href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International</a> license.</p>
<p><em>Heavily Adapted and Compressed from the surprisingly large version 3.1 of the 
<a href="https://www.mozilla.org/en-US/about/governance/policies/participation/">Mozilla Participation Guidelines</a>
which Mozilla released under the Creative Commons Attribution-ShareAlike 4.0 International. We thank Mozilla for their work.</em></p>
<div style="break-before: page; page-break-before: always;"></div><!-- SPDX-License-Identifier: CC-BY-SA-4.0 -->
<!-- SPDX-FileCopyrightText: 2020 The LumoSQL Authors -->
<!-- SPDX-ArtifactOfProjectName: LumoSQL -->
<!-- SPDX-FileType: Documentation -->
<!-- SPDX-FileComment: Original by Dan Shearer, 2020 -->
<p><img src="./images/lumo-legal-aspects-intro.png" alt="" title="XXXXXXXX" /></p>
<h1 id="table-of-contents-14"><a class="header" href="#table-of-contents-14">Table of Contents</a></h1>
<ul>
<li><a href="3.2-legal-aspects.html#lumosql-licensing">LumoSQL Licensing</a></li>
<li><a href="3.2-legal-aspects.html#why-mit-why-not-mit">Why MIT? Why Not MIT?</a></li>
<li><a href="3.2-legal-aspects.html#in-detail-patents-mit-and-apache-20">In Detail: Patents, MIT and Apache 2.0</a></li>
<li><a href="3.2-legal-aspects.html#in-detail-the-sqlite-public-domain-licensing-problem">In Detail: the SQLite Public Domain Licensing Problem</a></li>
<li><a href="3.2-legal-aspects.html#history-and-rationale">History and Rationale</a></li>
<li><a href="3.2-legal-aspects.html#encryption-legal-issues">Encryption Legal Issues</a></li>
<li><a href="3.2-legal-aspects.html#lumosql-requirements-and-decisions">LumoSQL Requirements and Decisions</a></li>
</ul>
<h1 id="lumosql-licensing"><a class="header" href="#lumosql-licensing">LumoSQL Licensing</a></h1>
<p>SQLite is released as <a href="https://www.sqlite.org/copyright.html">Public Domain</a>.
In order to both respect and improve on this, the <a href="lumo-projet-aims.html">LumoSQL Project Aims</a> make this promise to SQLite users:</p>
<blockquote>
<p>LumoSQL will not come with legal terms less favourable than SQLite. LumoSQL
will try to improve the legal standing and safety worldwide as compared to
SQLite.</p>
</blockquote>
<p>To achieve this LumoSQL has made these policy decisions:</p>
<ul>
<li>New LumoSQL code is licensed under the <a href="https://opensource.org/licenses/MIT">MIT License</a>, as used by many large corporations worldwide</li>
<li>LumoSQL documentation is licensed under the <a href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons</a></li>
<li>Existing and future SQLite code is relicenced by the act of being distributed under the terms of the MIT license</li>
<li>Open Source code from elsewhere, such as backend data stores, remain under the terms of the original license except where distribution under MIT effectively relicenses it</li>
<li>Open Content documentation from elsewhere remains under the terms of the original license. No documentation is used in LumoSQL unless it can be freely mixed with any other documentation. </li>
</ul>
<p>The effect of these policy decisions are:</p>
<ul>
<li>
<p>LumoSQL users gain certainty as compared with SQLite users because they have a
license that is recognised in jurisdictions worldwide. </p>
</li>
<li>
<p>LumoSQL users do not lose any rights. For example, the MIT license permits use
with fully proprietary software, by anyone. Whatever users do today with
SQLite they can continue to do with LumoSQL. </p>
</li>
<li>
<p>While MIT does require users to include a copy of the license and the
copyright notice, the MIT license also permits the user to remove the
sentence requiring this from the license (thus re-licensing LumoSQL.) </p>
</li>
</ul>
<h1 id="why-mit-why-not-mit"><a class="header" href="#why-mit-why-not-mit">Why MIT? Why Not MIT?</a></h1>
<p>Github's <a href="https://choosealicense.com/licenses/mit/">License Chooser for MIT</a> describes the MIT as:</p>
<blockquote>
<p>A short and simple permissive license with conditions only requiring
preservation of copyright and license notices. Licensed works, modifications,
and larger works may be distributed under different terms and without source
code. </p>
</blockquote>
<p>The MIT license aims to get out of the way of software developers, and despite
some flaws it appears to do so reliably.</p>
<p>In addition, MIT is popular. As documented <a href="https://en.wikipedia.org/wiki/MIT_License">on Wikipedia</a> MIT appears to be the most-used open source licenses. Popularity matters, because all licenses are in part a matter of community belief and momentum.  Microsoft releasedi
<a href="https://en.wikipedia.org/wiki/.NET_Core">.NET Core</a> and Facebook released
<a href="https://en.wikipedia.org/wiki/React_(web_framework)">React</a> under the MIT, and
these companies are very cautious about the validity of the licenses they use.</p>
<p>In a forensic article analysing <a href="https://writing.kemitchell.com/2016/09/21/MIT-License-Line-by-Line.html">the 171 words of the MIT license</a> as they apply in the US, lawyer Kyle E. Mitchell writes in his conclusion:</p>
<blockquote>
<p>The MIT License is a legal classic. The MIT License works. It is by no means
a panacea for all software IP ills, in particular the software patent
scourge, which it predates by decades. But MIT-style licenses have served
admirably... We’ve seen that despite some crusty verbiage and lawyerly
affectation, one hundred and seventy one little words can get a hell of a lot
of legal work done, clearing a path for open-source software through a dense
underbrush of intellectual property and contract.</p>
</blockquote>
<p>Overall, in LumoSQL we have concluded that the MIT license is solid and it is
better than any other mainstream license for existing SQLite users. It is
certainly better than the SQLite Public Domain terms.</p>
<h1 id="in-detail-patents-mit-and-apache-20"><a class="header" href="#in-detail-patents-mit-and-apache-20">In Detail: Patents, MIT and Apache 2.0</a></h1>
<p>LumoSQL has a narrower range of possible licenses because of its nature as an
embedded library, where it is tightly combined with users' code. This means
that the terms and conditions for using LumoSQL have to be as open as possible
to accommodate all the different legal statuses of software that users combine
with LumoSQL. And the status that worries corporate lawyers the most is
&quot;unknown&quot;. What if you aren't completely sure of the patent status of the
software, or the intentions of your company? And where there is uncertainty,
users are wise not to commit.</p>
<p>LumoSQL has tried hard to bring more certainty, not less, and this is tricky when it comes to patents.</p>
<p>Software patents are an issue in many jurisdictions. The MIT license includes a
grant of patents to its users, as <a href="https://opensource.com/article/18/3/patent-grant-mit-license">explained by the Open Source Initiative</a>,
including in the grant &quot;... to deal in the software without restriction.&quot; While the
Apache 2.0 license specifically grants patent rights (as do the GPL and MPL), they are not more generous than the MIT license. There is some debate that varies by jurisdiction about exactly how clear the patent grant is, as documented in <a href="https://en.wikipedia.org/wiki/MIT_License#Relation_to_patents">the patent section on Wikipedia</a>.</p>
<p>The difficulty is that the Apache 2.0 (similar to the GPL and MPL) license also
includes a <em>patent retaliation</em> clause:</p>
<blockquote>
<p>If You institute patent litigation against any entity (including a
cross-claim or counterclaim in a lawsuit) alleging that the Work or a
Contribution incorporated within the Work constitutes direct or contributory
patent infringement, then any patent licenses granted to You under this
License for that Work shall terminate as of the date such litigation is
filed.</p>
</blockquote>
<p>The intention is progressive and seemingly a Good Thing - after all, unless you
are a patent troll who wants more pointless patent litigation? However the
effect is that the Apache 2.0 license brings with it the requirement to check
for patent issues in any code it is connected to. It also is possible that the
company using LumoSQL actually does want the liberty to take software patent
action in court. So whether by the risk or the constraint, Apache 2.0 brings with it
significant change compared to SQLite's license terms in countries that recognise them. </p>
<p>MIT has only a patent grant, not retaliation. That is why LumoSQL does not use the Apache 2.0 license.</p>
<h1 id="in-detail-the-sqlite-public-domain-licensing-problem"><a class="header" href="#in-detail-the-sqlite-public-domain-licensing-problem">In Detail: the SQLite Public Domain Licensing Problem</a></h1>
<p>There are numerous reasons other than licensing why SQLite is less open source
than it appears, and these are covered in the <a href="./lumo-landscape.html">LumoSQL Landscape</a>. As to licensing, SQLite is distributed as
Public Domain software, and this is mentioned by D Richard Hipp in his <a href="https://changelog.com/podcast/201">2016 Changelog Podcast Interview</a>. Although he is aware of the problems, Hipp has decided not to introduce changes.</p>
<p>The <a href="https://opensource.org/node/878">Open Source Initiative</a> explains the Public Domain problem like this:</p>
<blockquote>
<p>“Public Domain” means software (or indeed anything else that could be
copyrighted) that is not restricted by copyright. It may be this way because
the copyright has expired, or because the person entitled to control the
copyright has disclaimed that right. Disclaiming copyright is only possible
in some countries, and copyright expiration happens at different times in
different jurisdictions (and usually after such a long time as to be
irrelevant for software). As a consequence, it’s impossible to make a
globally applicable statement that a certain piece of software is in the
public domain.</p>
</blockquote>
<p>Germany and Australia are examples of countries in which Public Domain is not
normally recognised which means that legal certainty is not possible for users
in these countries who need it or want it. This is why the Open Source
Initiative does not recommend it and nor does it appear on the <a href="https://spdx.org/licenses/">SPDX License List</a>.</p>
<p>The SPDX License List is a tool used by many organisations to understand where they stand legally with the millions of lines of code they are using. David A Wheeler has produced a helpful <a href="https://github.com/david-a-wheeler/spdx-tutorial">SPDX Tutorial</a> . All code and documentation developed by the LumoSQL project has a SPDX identifier.</p>
<h1 id="history-and-rationale"><a class="header" href="#history-and-rationale">History and Rationale</a></h1>
<p>SQLite Version 1 used the gdbm key-value store. This was under the GPL and
therefore so was SQLite. gdbm is limited, and is not a binary tree. When
Richard Hipp replaced it for SQLite version 2, he also dropped the GPL. SQLite
has been released as &quot;Public Domain&quot;</p>
<h1 id="encryption-legal-issues"><a class="header" href="#encryption-legal-issues">Encryption Legal Issues</a></h1>
<p>SQLite is not available with encryption. There are two common ways of adding encryption to SQLite, both of which have legal implications: </p>
<ol>
<li>Purchasing the <a href="https://www.hwaci.com/sw/sqlite/see.html">SQLite Encryption Extension</a>(SEE) from Richard Hipp's company Hwaci. The SEE is proprietary software, and cannot be used with open source applications.</li>
<li><a href="https://www.zetetic.net/sqlcipher/">SQLcipher</a> which has a open core model. The BSD-licensed open source version requires users to publish copyright notices, and the more capable commercial editions are available on similar terms to SEE, and therefore cannot be used with open source applications. </li>
</ol>
<p>There are many other ways of adding encryption to SQLite, some of which are listed in the <a href="./lumo-relevant-knowledgebase.html">Knowledgebase Relevant to LumoSQL</a>.</p>
<p>The legal issues addressed in LumoSQL encryption include:</p>
<ul>
<li>Usability. Encryption should be available with LumoSQL in the core source code without having to consider any additional legal considerations.</li>
<li>Unencumbered. No encryption code is used that may reasonably be subject to action by companies (eg copyright claims) or governments (eg export regulations). Crypto code will be reused from known-safe sources.</li>
<li>Compliant with minimum requirements in various jurisdictions. With encryption being legally mandated or strongly recommended in many jurisdictions for particular use cases (banking, handling personal data, government data, etc) there are also minimum requirements. LumoSQL will not ship crypto code that fails minimum crypto requirements.</li>
<li>Conspicuously <em>non-compliant</em> with maximum requirements in any jurisdiction. LumoSQL will not limit its encryption mechanisms or strength to comply with any legal restrictions, in common with other critical open source infrastructure. LumoSQL crypto tries to be as hard to break as possible regardless of the use case or jurisdiction.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="licensing"><a class="header" href="#licensing">Licensing</a></h1>
<p>This directory contains the licence which applies to all material in the LumoSQL project.</p>
<h1 id="mit-licence"><a class="header" href="#mit-licence">MIT Licence</a></h1>
<p>The <a href="https://en.wikipedia.org/wiki/MIT_License">MIT License</a> applies to all
files in this project. The MIT licence is the most-widely used Open Source
licence, including by some of the world's largest companies. The MIT licence
grants everyone the right to use the code for any purpose under copyright law.
Every file should have a copyright statement at the top and also a 
<a href="https://spdx.dev">Software Package Data Exchange</a> machine-readable header, but even if
it doesn't, the MIT licence still applies. It also applies to any file which is 
not clearly code, such as configuration or input data.</p>
<p>The <a href="https://lumosql.org/src/lumodoc/doc/tip/doc/README.md">LumoSQL Documentation</a> 
is also generally covered by this MIT License. There are documentation-specific
licenses and we will adopt one of them if the MIT License ever becomes a problem.</p>
<p>To even further avoid potential misunderstanding, we maintain the site
<a href="https://license.lumosql.org">license.lumosql.org</a> with the full text of the MIT License.</p>
<div style="break-before: page; page-break-before: always;"></div><p>Copyright 2019-2020, The LumoSQL Authors</p>
<p>Permission is hereby granted, free of charge, to any person obtaining a
copy of this software and associated documentation files (the
&quot;Software&quot;), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:</p>
<p>The above copyright notice and this permission notice shall be included
in all copies or substantial portions of the Software.</p>
<p>THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS
OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                        
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                            </nav>

        </div>

        
        
        
                <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        
        
                <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        
                        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
                
    </body>
</html>
